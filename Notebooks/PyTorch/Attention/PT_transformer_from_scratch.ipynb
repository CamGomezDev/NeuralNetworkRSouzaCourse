{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Transformer from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll be implementing the famous Transformer architecture from scratch.\n",
    "\n",
    "The code is based off of the following repos/blog posts:\n",
    "\n",
    "- [attention-is-all-you-need-pytorch](https://github.com/jadore801120/attention-is-all-you-need-pytorch)\n",
    "- [pytorch-pretrained-BERT](https://github.com/huggingface/pytorch-pretrained-BERT)\n",
    "- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html) \n",
    "\n",
    "Thanks so much to their authors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the keys to understanding how any model works is understanding how the shapes of the tensors change during the processing of each part. We'll be using the logging module to output debugging information to help our understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(\"tensor_shapes\")\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\n",
    "        '%(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "# if you want the model to continuously print tensor shapes, set to DEBUG!\n",
    "logger.setLevel(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "def getclass():\n",
    "    stack = inspect.stack()\n",
    "    return stack[3][0].f_locals[\"self\"].__class__\n",
    "\n",
    "# A helper function to check how tensor sizes change\n",
    "def log_size(tsr: torch.Tensor, name: str):\n",
    "    cls = getclass()\n",
    "    logger.log(level=cls.level, msg=f\"[{cls.__name__}] {name} size={tsr.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use logging levels to control the modules we receive output from. The lower the logging level, the more tensor information you'll get. Feel free to play around!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "# Control how much debugging output we want\n",
    "class TensorLoggingLevels(IntEnum):\n",
    "    attention = 1\n",
    "    attention_head = 2\n",
    "    multihead_attention_block = 3\n",
    "    enc_dec_block = 4\n",
    "    enc_dec = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using an enum to refer to dimensions whenever possible to improve readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dim(IntEnum):\n",
    "    batch = 0\n",
    "    seq = 1\n",
    "    feature = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled dot product attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Transformer is an attention-based architecture. The attention used in the Transformer is the scaled dot product attention, represented by the following formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\textrm{Attention}(Q, K, V) = \\textrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://i2.wp.com/mlexplained.com/wp-content/uploads/2017/12/scaled_dot_product_attention.png?zoom=2&w=750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    level = TensorLoggingLevels.attention # Logging level: \n",
    "    def __init__(self, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        d_k = k.size(-1) # get the size of the key\n",
    "        assert q.size(-1) == d_k\n",
    "\n",
    "        # compute the dot product between queries and keys for\n",
    "        # each batch and position in the sequence\n",
    "        attn = torch.bmm(q, k.transpose(Dim.seq, Dim.feature)) # (Batch, Seq, Seq)\n",
    "        # we get an attention score between each position in the sequence\n",
    "        # for each batch\n",
    "\n",
    "        # scale the dot products by the dimensionality (see the paper for why we do this!)\n",
    "        attn = attn / math.sqrt(d_k)\n",
    "        # normalize the weights across the sequence dimension\n",
    "        # (Note that since we transposed, the sequence and feature dimensions are switched)\n",
    "        attn = torch.exp(attn)\n",
    "        log_size(attn, \"attention weight\") # (Batch, Seq, Seq)\n",
    "        \n",
    "        # fill attention weights with 0s where padded\n",
    "        if mask is not None: attn = attn.masked_fill(mask, 0)\n",
    "        attn = attn / attn.sum(dim=-1, keepdim=True)\n",
    "        attn = self.dropout(attn)\n",
    "        output = torch.bmm(attn, v) # (Batch, Seq, Feature)\n",
    "        log_size(output, \"attention output size\") # (Batch, Seq, Seq)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = ScaledDotProductAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.rand(5, 10, 20)\n",
    "k = torch.rand(5, 10, 20)\n",
    "v = torch.rand(5, 10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4857, 0.3804, 0.4538, 0.6123, 0.4129, 0.5331, 0.6630, 0.5870,\n",
       "          0.6286, 0.5360, 0.5223, 0.4314, 0.5797, 0.5173, 0.4921, 0.4940,\n",
       "          0.4200, 0.5500, 0.6776, 0.2807],\n",
       "         [0.4994, 0.4496, 0.4019, 0.5565, 0.4321, 0.5182, 0.6957, 0.5612,\n",
       "          0.6322, 0.4660, 0.4624, 0.4465, 0.5715, 0.4583, 0.5564, 0.4500,\n",
       "          0.4112, 0.4479, 0.6002, 0.3139],\n",
       "         [0.5427, 0.3497, 0.4569, 0.6068, 0.4407, 0.5626, 0.6485, 0.5449,\n",
       "          0.5605, 0.4981, 0.5649, 0.3616, 0.5621, 0.5028, 0.4819, 0.4704,\n",
       "          0.4494, 0.5098, 0.5724, 0.3464],\n",
       "         [0.5319, 0.4777, 0.4630, 0.6271, 0.4476, 0.6110, 0.7463, 0.6055,\n",
       "          0.7028, 0.5336, 0.5724, 0.5006, 0.6333, 0.5503, 0.5727, 0.5186,\n",
       "          0.4917, 0.5291, 0.6765, 0.3580],\n",
       "         [0.5602, 0.4699, 0.4619, 0.6399, 0.4525, 0.6335, 0.7449, 0.6070,\n",
       "          0.6830, 0.5381, 0.5995, 0.4956, 0.6323, 0.5505, 0.5741, 0.5287,\n",
       "          0.5057, 0.5323, 0.6766, 0.3656],\n",
       "         [0.5513, 0.4791, 0.4524, 0.6368, 0.4540, 0.6365, 0.7454, 0.6136,\n",
       "          0.6866, 0.5322, 0.5972, 0.4950, 0.6254, 0.5475, 0.5753, 0.5258,\n",
       "          0.5007, 0.5338, 0.6694, 0.3635],\n",
       "         [0.5626, 0.4577, 0.4636, 0.6524, 0.4597, 0.6233, 0.7393, 0.6224,\n",
       "          0.6803, 0.5455, 0.5871, 0.4823, 0.6338, 0.5471, 0.5714, 0.5291,\n",
       "          0.4937, 0.5469, 0.6898, 0.3507],\n",
       "         [0.4745, 0.3810, 0.4235, 0.5677, 0.4405, 0.5521, 0.7038, 0.5437,\n",
       "          0.6613, 0.5148, 0.5754, 0.4375, 0.6100, 0.5510, 0.4886, 0.5199,\n",
       "          0.4203, 0.4787, 0.5954, 0.3526],\n",
       "         [0.4984, 0.3627, 0.4029, 0.5743, 0.4158, 0.5021, 0.6649, 0.5796,\n",
       "          0.5857, 0.4866, 0.4797, 0.4848, 0.5946, 0.4317, 0.4544, 0.4652,\n",
       "          0.4443, 0.4566, 0.5719, 0.3356],\n",
       "         [0.5354, 0.4528, 0.3709, 0.5966, 0.4108, 0.6160, 0.6558, 0.5918,\n",
       "          0.6045, 0.4500, 0.5928, 0.4757, 0.5568, 0.4662, 0.5663, 0.4769,\n",
       "          0.4613, 0.4550, 0.5966, 0.3030]],\n",
       "\n",
       "        [[0.5886, 0.3219, 0.4014, 0.5372, 0.5164, 0.5128, 0.2619, 0.4616,\n",
       "          0.5057, 0.5133, 0.4501, 0.3495, 0.5097, 0.3549, 0.6372, 0.4170,\n",
       "          0.3772, 0.7026, 0.4155, 0.4442],\n",
       "         [0.6534, 0.3446, 0.4310, 0.4859, 0.5814, 0.5608, 0.3687, 0.5648,\n",
       "          0.5179, 0.5143, 0.5088, 0.3236, 0.5524, 0.4737, 0.6282, 0.4553,\n",
       "          0.5071, 0.6535, 0.5340, 0.5089],\n",
       "         [0.5616, 0.2985, 0.3702, 0.4013, 0.4841, 0.4547, 0.2801, 0.4536,\n",
       "          0.4370, 0.3808, 0.4570, 0.2516, 0.4283, 0.3858, 0.5215, 0.3786,\n",
       "          0.4855, 0.5322, 0.3545, 0.3756],\n",
       "         [0.7153, 0.4399, 0.4988, 0.6048, 0.6211, 0.6084, 0.3889, 0.6498,\n",
       "          0.6050, 0.6450, 0.5685, 0.3856, 0.6779, 0.4812, 0.7407, 0.4826,\n",
       "          0.5035, 0.7734, 0.5856, 0.5437],\n",
       "         [0.6028, 0.3997, 0.4200, 0.5870, 0.4981, 0.4878, 0.2813, 0.5432,\n",
       "          0.5392, 0.5308, 0.4772, 0.3477, 0.5737, 0.4218, 0.6327, 0.3947,\n",
       "          0.4330, 0.6522, 0.5393, 0.4408],\n",
       "         [0.7252, 0.4278, 0.4995, 0.6073, 0.6320, 0.6176, 0.3954, 0.6402,\n",
       "          0.5949, 0.6426, 0.5657, 0.3809, 0.6768, 0.4786, 0.7392, 0.4845,\n",
       "          0.4953, 0.7854, 0.5880, 0.5629],\n",
       "         [0.4297, 0.2845, 0.3187, 0.3978, 0.3456, 0.4309, 0.2325, 0.3830,\n",
       "          0.4366, 0.3970, 0.3519, 0.3072, 0.4038, 0.3323, 0.5105, 0.3397,\n",
       "          0.2905, 0.5939, 0.3526, 0.3737],\n",
       "         [0.5508, 0.3806, 0.4151, 0.4538, 0.4515, 0.5124, 0.3648, 0.5462,\n",
       "          0.5344, 0.5334, 0.4414, 0.3243, 0.5530, 0.4363, 0.6024, 0.4015,\n",
       "          0.3910, 0.6554, 0.5104, 0.4773],\n",
       "         [0.6353, 0.3822, 0.3965, 0.5089, 0.6045, 0.5771, 0.3674, 0.6175,\n",
       "          0.4966, 0.6195, 0.5380, 0.3707, 0.5971, 0.4353, 0.6472, 0.4201,\n",
       "          0.4343, 0.6651, 0.5346, 0.5283],\n",
       "         [0.7340, 0.4280, 0.5145, 0.6148, 0.6226, 0.6036, 0.3924, 0.6283,\n",
       "          0.6019, 0.6165, 0.5598, 0.3616, 0.6730, 0.4837, 0.7332, 0.4849,\n",
       "          0.5149, 0.7788, 0.5884, 0.5519]],\n",
       "\n",
       "        [[0.5952, 0.5025, 0.3564, 0.5766, 0.3469, 0.4956, 0.4043, 0.5203,\n",
       "          0.5865, 0.5267, 0.5911, 0.5940, 0.6525, 0.5467, 0.5354, 0.3299,\n",
       "          0.5908, 0.4811, 0.5240, 0.5885],\n",
       "         [0.6797, 0.5782, 0.4106, 0.5932, 0.4283, 0.5616, 0.4208, 0.5171,\n",
       "          0.6614, 0.6145, 0.6647, 0.6218, 0.7240, 0.6259, 0.5979, 0.3718,\n",
       "          0.6108, 0.5593, 0.5422, 0.5955],\n",
       "         [0.5835, 0.4904, 0.3872, 0.4895, 0.3650, 0.5111, 0.4147, 0.4287,\n",
       "          0.5591, 0.5848, 0.5668, 0.5364, 0.6708, 0.5975, 0.4903, 0.3077,\n",
       "          0.5313, 0.4583, 0.4739, 0.5153],\n",
       "         [0.6370, 0.4709, 0.3802, 0.5388, 0.4061, 0.5141, 0.3662, 0.5070,\n",
       "          0.5759, 0.5530, 0.6166, 0.5300, 0.6427, 0.5417, 0.5776, 0.3091,\n",
       "          0.5308, 0.5294, 0.4377, 0.4980],\n",
       "         [0.4528, 0.4687, 0.3310, 0.5094, 0.2947, 0.4455, 0.3455, 0.4222,\n",
       "          0.4766, 0.4295, 0.4496, 0.5333, 0.5222, 0.4137, 0.5047, 0.3035,\n",
       "          0.5841, 0.4668, 0.4607, 0.4687],\n",
       "         [0.5860, 0.3502, 0.2220, 0.4095, 0.2903, 0.2785, 0.3050, 0.3222,\n",
       "          0.5585, 0.4104, 0.4244, 0.4420, 0.5104, 0.3574, 0.3234, 0.2920,\n",
       "          0.2480, 0.3496, 0.2627, 0.4352],\n",
       "         [0.6987, 0.5737, 0.4085, 0.5948, 0.4328, 0.5562, 0.4300, 0.5176,\n",
       "          0.6799, 0.6194, 0.6585, 0.6313, 0.7131, 0.6119, 0.5902, 0.3851,\n",
       "          0.5907, 0.5642, 0.5229, 0.5885],\n",
       "         [0.6736, 0.5695, 0.4145, 0.5829, 0.4351, 0.5665, 0.4273, 0.5186,\n",
       "          0.6541, 0.6323, 0.6596, 0.6173, 0.7172, 0.6400, 0.5869, 0.3681,\n",
       "          0.6132, 0.5514, 0.5386, 0.5779],\n",
       "         [0.6804, 0.5826, 0.4201, 0.5926, 0.4294, 0.5694, 0.4243, 0.5095,\n",
       "          0.6662, 0.6171, 0.6632, 0.6275, 0.7115, 0.6234, 0.6039, 0.3782,\n",
       "          0.6127, 0.5705, 0.5345, 0.5840],\n",
       "         [0.6992, 0.5790, 0.4135, 0.5954, 0.4267, 0.5462, 0.4222, 0.5078,\n",
       "          0.6808, 0.6161, 0.6647, 0.6327, 0.7302, 0.6197, 0.5903, 0.3766,\n",
       "          0.6026, 0.5603, 0.5357, 0.6054]],\n",
       "\n",
       "        [[0.4020, 0.4967, 0.6070, 0.5896, 0.6246, 0.5623, 0.5024, 0.8478,\n",
       "          0.6160, 0.4858, 0.5739, 0.6380, 0.6591, 0.6014, 0.6311, 0.3521,\n",
       "          0.5327, 0.6606, 0.6293, 0.5906],\n",
       "         [0.3698, 0.4609, 0.4542, 0.4782, 0.4944, 0.5303, 0.4802, 0.7116,\n",
       "          0.6134, 0.4149, 0.4976, 0.5299, 0.5374, 0.5624, 0.5941, 0.3506,\n",
       "          0.3954, 0.5993, 0.6090, 0.4447],\n",
       "         [0.4145, 0.4104, 0.4509, 0.4716, 0.5864, 0.4769, 0.4264, 0.6849,\n",
       "          0.5152, 0.3235, 0.5855, 0.5361, 0.5322, 0.5435, 0.5668, 0.2147,\n",
       "          0.3916, 0.4542, 0.4747, 0.4052],\n",
       "         [0.4058, 0.3487, 0.4918, 0.4685, 0.5644, 0.4552, 0.3792, 0.7453,\n",
       "          0.5370, 0.4272, 0.4954, 0.5139, 0.6267, 0.5067, 0.5071, 0.3156,\n",
       "          0.4254, 0.5454, 0.4612, 0.5278],\n",
       "         [0.4140, 0.5005, 0.5956, 0.5955, 0.6248, 0.5664, 0.4968, 0.8492,\n",
       "          0.6106, 0.5002, 0.5669, 0.6092, 0.6514, 0.6097, 0.6418, 0.3604,\n",
       "          0.5139, 0.6553, 0.6238, 0.5875],\n",
       "         [0.3061, 0.4279, 0.5655, 0.5027, 0.6106, 0.4492, 0.3822, 0.7337,\n",
       "          0.5074, 0.4812, 0.4844, 0.5226, 0.6236, 0.4838, 0.5925, 0.3298,\n",
       "          0.4904, 0.5580, 0.6113, 0.5681],\n",
       "         [0.3124, 0.4329, 0.3887, 0.5096, 0.5129, 0.4175, 0.4152, 0.6547,\n",
       "          0.4008, 0.3306, 0.4746, 0.4833, 0.4496, 0.5693, 0.5483, 0.2161,\n",
       "          0.4372, 0.5121, 0.4730, 0.4515],\n",
       "         [0.4034, 0.4885, 0.4904, 0.5374, 0.6154, 0.5223, 0.4613, 0.7537,\n",
       "          0.5218, 0.3884, 0.5878, 0.5562, 0.5375, 0.5903, 0.6442, 0.2567,\n",
       "          0.4544, 0.5372, 0.5642, 0.4643],\n",
       "         [0.3792, 0.4271, 0.4955, 0.5414, 0.5218, 0.4887, 0.4317, 0.7712,\n",
       "          0.5391, 0.4411, 0.4571, 0.4994, 0.5547, 0.4930, 0.5380, 0.2772,\n",
       "          0.4728, 0.6005, 0.4756, 0.5132],\n",
       "         [0.3994, 0.4764, 0.4890, 0.5194, 0.6083, 0.5152, 0.4464, 0.7392,\n",
       "          0.5114, 0.3868, 0.5742, 0.5425, 0.5287, 0.5819, 0.6382, 0.2684,\n",
       "          0.4314, 0.5202, 0.5661, 0.4548]],\n",
       "\n",
       "        [[0.5024, 0.3658, 0.7032, 0.5167, 0.4236, 0.4633, 0.6149, 0.6278,\n",
       "          0.5722, 0.4415, 0.6502, 0.6575, 0.3699, 0.5891, 0.5293, 0.4722,\n",
       "          0.3494, 0.4929, 0.6670, 0.7239],\n",
       "         [0.4375, 0.3029, 0.6030, 0.4670, 0.3502, 0.3864, 0.5377, 0.5714,\n",
       "          0.4955, 0.3976, 0.5775, 0.5916, 0.3228, 0.4880, 0.4424, 0.4300,\n",
       "          0.3373, 0.4169, 0.6164, 0.6565],\n",
       "         [0.4613, 0.3489, 0.6108, 0.4342, 0.3924, 0.4525, 0.5162, 0.5628,\n",
       "          0.5217, 0.4233, 0.5654, 0.6119, 0.3067, 0.5763, 0.4519, 0.3859,\n",
       "          0.3308, 0.4176, 0.6323, 0.6497],\n",
       "         [0.4880, 0.3326, 0.6338, 0.4503, 0.3698, 0.4480, 0.5244, 0.5727,\n",
       "          0.5705, 0.3559, 0.5790, 0.5774, 0.2479, 0.5365, 0.4087, 0.3762,\n",
       "          0.3431, 0.4180, 0.6096, 0.6192],\n",
       "         [0.3214, 0.3003, 0.5772, 0.4487, 0.3393, 0.3908, 0.5814, 0.4500,\n",
       "          0.4152, 0.2964, 0.5061, 0.4691, 0.3541, 0.4474, 0.4341, 0.3754,\n",
       "          0.2651, 0.4408, 0.4749, 0.5673],\n",
       "         [0.4489, 0.3503, 0.6177, 0.4263, 0.4129, 0.4654, 0.5345, 0.5716,\n",
       "          0.5512, 0.4549, 0.5820, 0.6110, 0.3219, 0.5777, 0.4645, 0.4188,\n",
       "          0.3112, 0.4459, 0.6354, 0.6670],\n",
       "         [0.4877, 0.3323, 0.6402, 0.3968, 0.4021, 0.3989, 0.5460, 0.6334,\n",
       "          0.5604, 0.3876, 0.6672, 0.5958, 0.2949, 0.5493, 0.4405, 0.4675,\n",
       "          0.3146, 0.4955, 0.5650, 0.6149],\n",
       "         [0.4417, 0.3345, 0.6659, 0.4776, 0.3812, 0.3941, 0.5779, 0.5769,\n",
       "          0.5060, 0.3692, 0.6129, 0.5321, 0.3315, 0.4902, 0.4796, 0.3968,\n",
       "          0.2998, 0.4796, 0.6035, 0.6531],\n",
       "         [0.4292, 0.2657, 0.6376, 0.4609, 0.3932, 0.3820, 0.5166, 0.5695,\n",
       "          0.5942, 0.4523, 0.5902, 0.5296, 0.3146, 0.4422, 0.4488, 0.4347,\n",
       "          0.2779, 0.4492, 0.6417, 0.6659],\n",
       "         [0.4491, 0.3377, 0.6765, 0.4857, 0.3928, 0.3885, 0.5957, 0.5847,\n",
       "          0.4953, 0.3808, 0.6293, 0.5496, 0.3557, 0.5015, 0.5071, 0.4214,\n",
       "          0.2977, 0.4967, 0.5991, 0.6653]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we turn to the core component in the Transformer architecture: the multi-head attention block. This block applies linear transformations to the input, then applies scaled dot product attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://i2.wp.com/mlexplained.com/wp-content/uploads/2017/12/multi_head_attention.png?zoom=2&resize=224%2C293)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    level = TensorLoggingLevels.attention_head\n",
    "    def __init__(self, d_model, d_feature, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # We will assume the queries, keys, and values all have the same feature size\n",
    "        self.attn = ScaledDotProductAttention(dropout)\n",
    "        self.query_tfm = nn.Linear(d_model, d_feature)\n",
    "        self.key_tfm = nn.Linear(d_model, d_feature)\n",
    "        self.value_tfm = nn.Linear(d_model, d_feature)\n",
    "\n",
    "    def forward(self, queries, keys, values, mask=None):\n",
    "        Q = self.query_tfm(queries) # (Batch, Seq, Feature)\n",
    "        K = self.key_tfm(keys) # (Batch, Seq, Feature)\n",
    "        V = self.value_tfm(values) # (Batch, Seq, Feature)\n",
    "        log_size(Q, \"queries, keys, vals\")\n",
    "        # compute multiple attention weighted sums\n",
    "        x = self.attn(Q, K, V)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1168, -0.3617,  0.5936, -0.5844, -0.1107,  0.0041, -0.2305,\n",
       "           0.1480, -0.1898, -0.7099, -0.4407,  0.4266, -0.0891,  0.4478,\n",
       "          -0.0348, -0.2356, -0.5981,  0.0698,  0.3248,  0.2721],\n",
       "         [-0.1120, -0.3312,  0.4993, -0.4546, -0.1087, -0.0078, -0.2317,\n",
       "           0.1108, -0.1533, -0.6003, -0.3819,  0.4152, -0.0756,  0.4097,\n",
       "           0.0410, -0.2562, -0.5285,  0.0390,  0.2496,  0.1992],\n",
       "         [-0.1314, -0.3839,  0.5574, -0.5464, -0.1182,  0.0010, -0.2491,\n",
       "           0.1516, -0.1735, -0.7196, -0.4778,  0.4343, -0.1131,  0.4082,\n",
       "           0.0569, -0.2585, -0.6125,  0.0342,  0.2955,  0.2721],\n",
       "         [-0.1523, -0.3917,  0.5769, -0.5523, -0.1125,  0.0246, -0.2222,\n",
       "           0.1460, -0.1641, -0.7126, -0.4480,  0.4509, -0.0647,  0.4409,\n",
       "           0.0014, -0.2791, -0.5824,  0.0839,  0.2986,  0.2416],\n",
       "         [-0.1141, -0.2396,  0.3893, -0.3451, -0.1042,  0.0187, -0.1365,\n",
       "           0.1195, -0.0713, -0.5247, -0.3133,  0.3209, -0.0427,  0.2756,\n",
       "           0.0247, -0.1812, -0.3930,  0.0315,  0.1490,  0.1254],\n",
       "         [-0.1493, -0.4173,  0.6436, -0.6234, -0.1320,  0.0084, -0.2662,\n",
       "           0.1707, -0.2063, -0.7978, -0.5098,  0.4748, -0.1047,  0.4960,\n",
       "          -0.0117, -0.2716, -0.6827,  0.0733,  0.3464,  0.2880],\n",
       "         [-0.1494, -0.4167,  0.6411, -0.6226, -0.1293,  0.0055, -0.2643,\n",
       "           0.1715, -0.2082, -0.7976, -0.5099,  0.4756, -0.1042,  0.4975,\n",
       "          -0.0113, -0.2722, -0.6836,  0.0736,  0.3476,  0.2868],\n",
       "         [-0.1673, -0.3664,  0.5715, -0.5435, -0.1118, -0.0041, -0.2084,\n",
       "           0.1611, -0.1685, -0.7502, -0.4739,  0.4359, -0.0786,  0.4367,\n",
       "           0.0102, -0.2425, -0.6010,  0.0779,  0.2977,  0.2392],\n",
       "         [-0.1154, -0.3700,  0.6067, -0.5745, -0.0946, -0.0100, -0.2597,\n",
       "           0.1494, -0.2140, -0.6906, -0.4634,  0.3912, -0.1224,  0.4826,\n",
       "          -0.0481, -0.2276, -0.6286,  0.0824,  0.3395,  0.2765],\n",
       "         [-0.1688, -0.3681,  0.5754, -0.5482, -0.1131, -0.0023, -0.2081,\n",
       "           0.1621, -0.1683, -0.7555, -0.4765,  0.4370, -0.0783,  0.4364,\n",
       "           0.0088, -0.2421, -0.6024,  0.0792,  0.2995,  0.2421]],\n",
       "\n",
       "        [[-0.1148, -0.2817,  0.4442, -0.4612, -0.2041,  0.0688, -0.2325,\n",
       "           0.0965, -0.1785, -0.5826, -0.3563,  0.3525, -0.0984,  0.4581,\n",
       "           0.0423, -0.2267, -0.6814,  0.1382,  0.3536,  0.1509],\n",
       "         [-0.1711, -0.3162,  0.4610, -0.5449, -0.2821,  0.1547, -0.2425,\n",
       "           0.1133, -0.1517, -0.6492, -0.3559,  0.3309, -0.0851,  0.4968,\n",
       "          -0.0034, -0.2340, -0.6661,  0.1602,  0.3463,  0.1589],\n",
       "         [-0.1420, -0.3415,  0.5179, -0.5543, -0.2528,  0.0963, -0.2874,\n",
       "           0.1206, -0.1890, -0.6695, -0.3805,  0.3605, -0.1235,  0.5515,\n",
       "           0.0220, -0.2634, -0.7628,  0.1473,  0.4055,  0.1906],\n",
       "         [-0.1176, -0.3019,  0.4026, -0.4034, -0.2163,  0.0844, -0.2542,\n",
       "           0.0851, -0.1314, -0.4950, -0.2621,  0.2445, -0.0898,  0.4038,\n",
       "           0.0080, -0.2022, -0.6463,  0.1151,  0.3380,  0.2006],\n",
       "         [-0.1783, -0.3930,  0.5859, -0.6346, -0.3327,  0.1538, -0.3337,\n",
       "           0.1340, -0.2092, -0.7506, -0.4320,  0.3952, -0.1336,  0.6131,\n",
       "           0.0149, -0.2938, -0.8549,  0.1749,  0.4300,  0.2263],\n",
       "         [-0.1800, -0.3983,  0.5874, -0.6353, -0.3313,  0.1574, -0.3364,\n",
       "           0.1340, -0.2084, -0.7532, -0.4316,  0.3961, -0.1306,  0.6093,\n",
       "           0.0139, -0.2951, -0.8593,  0.1745,  0.4340,  0.2289],\n",
       "         [-0.1426, -0.3445,  0.5134, -0.5638, -0.3062,  0.1328, -0.2836,\n",
       "           0.1447, -0.1811, -0.6701, -0.3931,  0.3426, -0.1253,  0.5469,\n",
       "           0.0157, -0.2455, -0.7327,  0.1370,  0.3505,  0.2016],\n",
       "         [-0.1600, -0.3636,  0.5391, -0.5747, -0.3091,  0.1367, -0.3134,\n",
       "           0.1280, -0.1777, -0.6566, -0.3864,  0.3143, -0.1377,  0.5719,\n",
       "          -0.0012, -0.2544, -0.7780,  0.1759,  0.3859,  0.2286],\n",
       "         [-0.1791, -0.3927,  0.5890, -0.6351, -0.3295,  0.1531, -0.3366,\n",
       "           0.1326, -0.2121, -0.7501, -0.4320,  0.3989, -0.1334,  0.6121,\n",
       "           0.0154, -0.2966, -0.8563,  0.1749,  0.4323,  0.2235],\n",
       "         [-0.1667, -0.3542,  0.4156, -0.4698, -0.2502,  0.1521, -0.2700,\n",
       "           0.0700, -0.1465, -0.5934, -0.3022,  0.2896, -0.0771,  0.3812,\n",
       "           0.0365, -0.2462, -0.6651,  0.0995,  0.3543,  0.2302]],\n",
       "\n",
       "        [[-0.1461, -0.3516,  0.5729, -0.6580, -0.2145,  0.0597, -0.2569,\n",
       "           0.1314, -0.1742, -0.8148, -0.4322,  0.5095, -0.1356,  0.5006,\n",
       "           0.0752, -0.3514, -0.7533,  0.1345,  0.3893,  0.1590],\n",
       "         [-0.1074, -0.3015,  0.5222, -0.5852, -0.1866,  0.0533, -0.2641,\n",
       "           0.1108, -0.1603, -0.6892, -0.3948,  0.4612, -0.1389,  0.4364,\n",
       "           0.0697, -0.3020, -0.6569,  0.1049,  0.3284,  0.1276],\n",
       "         [-0.1234, -0.3171,  0.5081, -0.5752, -0.2131,  0.0552, -0.2413,\n",
       "           0.1200, -0.1363, -0.7114, -0.4020,  0.4498, -0.1328,  0.4654,\n",
       "           0.0779, -0.3075, -0.6634,  0.1138,  0.3263,  0.1427],\n",
       "         [-0.1081, -0.3008,  0.5224, -0.5886, -0.1854,  0.0542, -0.2638,\n",
       "           0.1109, -0.1641, -0.6908, -0.3983,  0.4617, -0.1389,  0.4387,\n",
       "           0.0670, -0.2996, -0.6571,  0.1076,  0.3296,  0.1262],\n",
       "         [-0.0791, -0.2381,  0.3976, -0.4680, -0.1580,  0.0782, -0.1721,\n",
       "           0.0915, -0.1296, -0.5304, -0.2526,  0.3548, -0.1032,  0.3402,\n",
       "           0.0387, -0.2903, -0.5401,  0.0917,  0.2591,  0.1030],\n",
       "         [-0.1235, -0.2739,  0.4370, -0.5044, -0.2046,  0.0610, -0.2207,\n",
       "           0.0787, -0.1409, -0.6195, -0.3833,  0.3913, -0.1143,  0.4298,\n",
       "           0.0629, -0.2536, -0.5979,  0.1339,  0.2829,  0.1262],\n",
       "         [-0.1458, -0.3518,  0.5759, -0.6577, -0.2119,  0.0606, -0.2567,\n",
       "           0.1346, -0.1723, -0.8171, -0.4316,  0.5091, -0.1360,  0.4978,\n",
       "           0.0772, -0.3537, -0.7555,  0.1328,  0.3898,  0.1581],\n",
       "         [-0.1459, -0.3543,  0.5771, -0.6543, -0.2133,  0.0579, -0.2570,\n",
       "           0.1364, -0.1676, -0.8194, -0.4321,  0.5111, -0.1346,  0.4977,\n",
       "           0.0803, -0.3551, -0.7566,  0.1297,  0.3897,  0.1594],\n",
       "         [-0.1436, -0.3541,  0.5770, -0.6551, -0.2145,  0.0587, -0.2586,\n",
       "           0.1344, -0.1676, -0.8181, -0.4350,  0.5107, -0.1375,  0.4997,\n",
       "           0.0807, -0.3549, -0.7559,  0.1299,  0.3863,  0.1606],\n",
       "         [-0.1337, -0.3159,  0.4923, -0.6110, -0.1988,  0.0391, -0.2084,\n",
       "           0.1032, -0.1786, -0.7396, -0.3623,  0.4714, -0.1077,  0.4673,\n",
       "           0.0414, -0.3155, -0.6709,  0.1326,  0.3744,  0.1502]],\n",
       "\n",
       "        [[-0.1017, -0.4622,  0.6084, -0.6031, -0.0918,  0.1269, -0.2662,\n",
       "           0.2571, -0.0907, -0.6933, -0.4790,  0.4326, -0.0552,  0.5145,\n",
       "          -0.0140, -0.2986, -0.6898,  0.1863,  0.3438,  0.1658],\n",
       "         [-0.0932, -0.4073,  0.4544, -0.4745, -0.0695,  0.0971, -0.2131,\n",
       "           0.1900, -0.0897, -0.5863, -0.4215,  0.3841, -0.0159,  0.3727,\n",
       "          -0.0183, -0.2353, -0.5605,  0.1347,  0.2907,  0.1469],\n",
       "         [-0.1385, -0.5085,  0.6404, -0.6664, -0.1120,  0.1222, -0.2861,\n",
       "           0.2440, -0.1489, -0.7775, -0.5199,  0.4990, -0.0459,  0.5680,\n",
       "          -0.0074, -0.3363, -0.7679,  0.2153,  0.4014,  0.1899],\n",
       "         [-0.0627, -0.3223,  0.4313, -0.4288, -0.1066,  0.0859, -0.2036,\n",
       "           0.1465, -0.0798, -0.5004, -0.3500,  0.3672,  0.0034,  0.3948,\n",
       "          -0.0233, -0.1759, -0.5100,  0.1709,  0.3042,  0.0834],\n",
       "         [-0.1369, -0.5088,  0.6411, -0.6668, -0.1119,  0.1232, -0.2863,\n",
       "           0.2464, -0.1486, -0.7764, -0.5186,  0.4980, -0.0469,  0.5669,\n",
       "          -0.0079, -0.3364, -0.7687,  0.2144,  0.4017,  0.1901],\n",
       "         [-0.1183, -0.4840,  0.5748, -0.5974, -0.0828,  0.1124, -0.2420,\n",
       "           0.2422, -0.1330, -0.6990, -0.4753,  0.4515, -0.0418,  0.5100,\n",
       "           0.0021, -0.3300, -0.6936,  0.1905,  0.3321,  0.1893],\n",
       "         [-0.1342, -0.4482,  0.5737, -0.6177, -0.1288,  0.1244, -0.2655,\n",
       "           0.2182, -0.1633, -0.7027, -0.4514,  0.4508, -0.0413,  0.5449,\n",
       "          -0.0234, -0.3082, -0.7218,  0.2162,  0.3902,  0.1456],\n",
       "         [-0.1363, -0.5081,  0.6411, -0.6657, -0.1108,  0.1230, -0.2846,\n",
       "           0.2458, -0.1458, -0.7763, -0.5199,  0.4986, -0.0455,  0.5668,\n",
       "          -0.0078, -0.3357, -0.7668,  0.2155,  0.3990,  0.1898],\n",
       "         [-0.1332, -0.4493,  0.5727, -0.6152, -0.1276,  0.1235, -0.2663,\n",
       "           0.2187, -0.1632, -0.6998, -0.4512,  0.4492, -0.0421,  0.5452,\n",
       "          -0.0232, -0.3094, -0.7200,  0.2136,  0.3883,  0.1463],\n",
       "         [-0.1369, -0.5077,  0.6401, -0.6663, -0.1118,  0.1235, -0.2845,\n",
       "           0.2458, -0.1476, -0.7764, -0.5183,  0.4984, -0.0457,  0.5658,\n",
       "          -0.0076, -0.3354, -0.7677,  0.2159,  0.4005,  0.1895]],\n",
       "\n",
       "        [[-0.1048, -0.5089,  0.6854, -0.5506, -0.1035,  0.1617, -0.2369,\n",
       "           0.2743, -0.0226, -0.7328, -0.4633,  0.3211, -0.0728,  0.2963,\n",
       "           0.0358, -0.2847, -0.6384,  0.0723,  0.3195,  0.3629],\n",
       "         [-0.1106, -0.5383,  0.7411, -0.6473, -0.1105,  0.1690, -0.2466,\n",
       "           0.3244, -0.0421, -0.8221, -0.4985,  0.3433, -0.1130,  0.3716,\n",
       "           0.0401, -0.3284, -0.6916,  0.0741,  0.3523,  0.3698],\n",
       "         [-0.1296, -0.5172,  0.6948, -0.6003, -0.1295,  0.1991, -0.2448,\n",
       "           0.2867, -0.0213, -0.7676, -0.4586,  0.2875, -0.0895,  0.3123,\n",
       "           0.0185, -0.2861, -0.6344,  0.0671,  0.3378,  0.3724],\n",
       "         [-0.1134, -0.4560,  0.6023, -0.5499, -0.1095,  0.1777, -0.2333,\n",
       "           0.2598, -0.0235, -0.6746, -0.3990,  0.2516, -0.0969,  0.2918,\n",
       "           0.0257, -0.2608, -0.5539,  0.0532,  0.2966,  0.3057],\n",
       "         [-0.1119, -0.5405,  0.7427, -0.6440, -0.1109,  0.1695, -0.2449,\n",
       "           0.3237, -0.0410, -0.8238, -0.5006,  0.3438, -0.1105,  0.3671,\n",
       "           0.0410, -0.3281, -0.6932,  0.0743,  0.3523,  0.3742],\n",
       "         [-0.0846, -0.4147,  0.6387, -0.5297, -0.0811,  0.1387, -0.2056,\n",
       "           0.2608, -0.0365, -0.6675, -0.3812,  0.2688, -0.0979,  0.3425,\n",
       "           0.0250, -0.2833, -0.5985,  0.0742,  0.3295,  0.2851],\n",
       "         [-0.1093, -0.5387,  0.7409, -0.6445, -0.1115,  0.1687, -0.2465,\n",
       "           0.3235, -0.0406, -0.8204, -0.4993,  0.3447, -0.1107,  0.3697,\n",
       "           0.0386, -0.3262, -0.6908,  0.0740,  0.3506,  0.3707],\n",
       "         [-0.1033, -0.5010,  0.6113, -0.5650, -0.0726,  0.1207, -0.1987,\n",
       "           0.3113, -0.0493, -0.7540, -0.4649,  0.3105, -0.1202,  0.3035,\n",
       "           0.0982, -0.3128, -0.6054,  0.0219,  0.2777,  0.3504],\n",
       "         [-0.1116, -0.5394,  0.7439, -0.6474, -0.1130,  0.1717, -0.2478,\n",
       "           0.3228, -0.0407, -0.8221, -0.4988,  0.3427, -0.1110,  0.3705,\n",
       "           0.0370, -0.3266, -0.6921,  0.0758,  0.3535,  0.3721],\n",
       "         [-0.0730, -0.3850,  0.6171, -0.5035, -0.0683,  0.1109, -0.1736,\n",
       "           0.2781, -0.0337, -0.6556, -0.3739,  0.3036, -0.0863,  0.3139,\n",
       "           0.0283, -0.2811, -0.5575,  0.0738,  0.3059,  0.2178]]],\n",
       "       grad_fn=<BmmBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_head = AttentionHead(20, 20)\n",
    "attn_head(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multi-head attention block simply applies multiple attention heads, then concatenates the outputs and applies a single linear projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll supress logging from the scaled dot product attention now\n",
    "logger.setLevel(TensorLoggingLevels.attention_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    level = TensorLoggingLevels.multihead_attention_block\n",
    "    def __init__(self, d_model, d_feature, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_feature = d_feature\n",
    "        self.n_heads = n_heads\n",
    "        # in practice, d_model == d_feature * n_heads\n",
    "        assert d_model == d_feature * n_heads\n",
    "\n",
    "        # Note that this is very inefficient:\n",
    "        # I am merely implementing the heads separately because it is \n",
    "        # easier to understand this way\n",
    "        self.attn_heads = nn.ModuleList([\n",
    "            AttentionHead(d_model, d_feature, dropout) for _ in range(n_heads)\n",
    "        ])\n",
    "        self.projection = nn.Linear(d_feature * n_heads, d_model) \n",
    "    \n",
    "    def forward(self, queries, keys, values, mask=None):\n",
    "        log_size(queries, \"Input queries\")\n",
    "        x = [attn(queries, keys, values, mask=mask) # (Batch, Seq, Feature)\n",
    "             for i, attn in enumerate(self.attn_heads)]\n",
    "        log_size(x[0], \"output of single head\")\n",
    "        \n",
    "        # reconcatenate\n",
    "        x = torch.cat(x, dim=Dim.feature) # (Batch, Seq, D_Feature * n_heads)\n",
    "        log_size(x, \"concatenated output\")\n",
    "        x = self.projection(x) # (Batch, Seq, D_Model)\n",
    "        log_size(x, \"projected output\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MultiHeadAttention] Input queries size=torch.Size([5, 10, 160])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[MultiHeadAttention] output of single head size=torch.Size([5, 10, 20])\n",
      "[MultiHeadAttention] concatenated output size=torch.Size([5, 10, 160])\n",
      "[MultiHeadAttention] projected output size=torch.Size([5, 10, 160])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2726,  0.0613,  0.0665,  ...,  0.1950, -0.0730, -0.2064],\n",
       "         [-0.2451,  0.0897,  0.0726,  ...,  0.2008, -0.0787, -0.2203],\n",
       "         [-0.2224,  0.0783,  0.0927,  ...,  0.2146, -0.0632, -0.2278],\n",
       "         ...,\n",
       "         [-0.2027,  0.0793,  0.0466,  ...,  0.2121, -0.0786, -0.2020],\n",
       "         [-0.2109,  0.0995,  0.0565,  ...,  0.1801, -0.0490, -0.1820],\n",
       "         [-0.2407,  0.0730,  0.0738,  ...,  0.1710, -0.0650, -0.2465]],\n",
       "\n",
       "        [[-0.3743,  0.0265,  0.0836,  ...,  0.1776, -0.1079, -0.2764],\n",
       "         [-0.2933,  0.1091,  0.0896,  ...,  0.1938, -0.0715, -0.2439],\n",
       "         [-0.2999,  0.0532,  0.0834,  ...,  0.1677, -0.0811, -0.2427],\n",
       "         ...,\n",
       "         [-0.2830,  0.0988,  0.0813,  ...,  0.1985, -0.0696, -0.2285],\n",
       "         [-0.2947,  0.0946,  0.0936,  ...,  0.1996, -0.0590, -0.2627],\n",
       "         [-0.2478,  0.1118,  0.0684,  ...,  0.1838, -0.0690, -0.2064]],\n",
       "\n",
       "        [[-0.2833,  0.0613,  0.0448,  ...,  0.1163, -0.0277, -0.2150],\n",
       "         [-0.2705,  0.0666,  0.0359,  ...,  0.1450, -0.0455, -0.2378],\n",
       "         [-0.2670,  0.0697,  0.0669,  ...,  0.1213, -0.0248, -0.2549],\n",
       "         ...,\n",
       "         [-0.2741,  0.0642,  0.0452,  ...,  0.1143, -0.0195, -0.2237],\n",
       "         [-0.2274,  0.0827,  0.0432,  ...,  0.1245, -0.0226, -0.2182],\n",
       "         [-0.2633,  0.0708,  0.0506,  ...,  0.1289,  0.0034, -0.2191]],\n",
       "\n",
       "        [[-0.3273,  0.0177,  0.0675,  ...,  0.2060, -0.1079, -0.2284],\n",
       "         [-0.2889,  0.0323,  0.0723,  ...,  0.2014, -0.0769, -0.2328],\n",
       "         [-0.3197,  0.0599,  0.0681,  ...,  0.2075, -0.0975, -0.1975],\n",
       "         ...,\n",
       "         [-0.3424,  0.0297,  0.0715,  ...,  0.1912, -0.1145, -0.2349],\n",
       "         [-0.2871,  0.0651,  0.0601,  ...,  0.2042, -0.0877, -0.2104],\n",
       "         [-0.2910,  0.0443,  0.0719,  ...,  0.1981, -0.0825, -0.2036]],\n",
       "\n",
       "        [[-0.2927,  0.0797,  0.0591,  ...,  0.2331, -0.1507, -0.1726],\n",
       "         [-0.2890,  0.0425,  0.0490,  ...,  0.1957, -0.1688, -0.1846],\n",
       "         [-0.3240,  0.0544,  0.0539,  ...,  0.2425, -0.1633, -0.1945],\n",
       "         ...,\n",
       "         [-0.2754,  0.0668,  0.0307,  ...,  0.1713, -0.1461, -0.1175],\n",
       "         [-0.2994,  0.0721,  0.0577,  ...,  0.2158, -0.1461, -0.2031],\n",
       "         [-0.3113,  0.0771,  0.0415,  ...,  0.2246, -0.1561, -0.1458]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heads = MultiHeadAttention(20 * 8, 20, 8)\n",
    "heads(q.repeat(1, 1, 8), \n",
    "      k.repeat(1, 1, 8), \n",
    "      v.repeat(1, 1, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these core components in place, implementing the encoder is pretty easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://i2.wp.com/mlexplained.com/wp-content/uploads/2017/12/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88-2017-12-29-19.14.41.png?w=273)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder consists of the following components:\n",
    "- A multi-head attention block\n",
    "- A simple feedforward neural network\n",
    "\n",
    "These components are connected using residual connections and layer normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll supress logging from the individual attention heads\n",
    "logger.setLevel(TensorLoggingLevels.multihead_attention_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer normalization is similar to batch normalization, but normalizes across the feature dimension instead of the batch dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://i1.wp.com/mlexplained.com/wp-content/uploads/2018/01/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88-2018-01-11-11.48.12.png?w=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-8):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder just stacks these together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    level = TensorLoggingLevels.enc_dec_block\n",
    "    def __init__(self, d_model=512, d_feature=64,\n",
    "                 d_ff=2048, n_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn_head = MultiHeadAttention(d_model, d_feature, n_heads, dropout)\n",
    "        self.layer_norm1 = LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.position_wise_feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "        self.layer_norm2 = LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        log_size(x, \"Encoder block input\")\n",
    "        att = self.attn_head(x, x, x, mask=mask)\n",
    "        log_size(x, \"Attention output\")\n",
    "        # Apply normalization and residual connection\n",
    "        x = x + self.dropout(self.layer_norm1(att))\n",
    "        # Apply position-wise feedforward network\n",
    "        pos = self.position_wise_feed_forward(x)\n",
    "        log_size(x, \"Feedforward output\")\n",
    "        # Apply normalization and residual connection\n",
    "        x = x + self.dropout(self.layer_norm2(pos))\n",
    "        log_size(x, \"Encoder size output\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = EncoderBlock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EncoderBlock] Encoder block input size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] Input queries size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] output of single head size=torch.Size([5, 10, 64])\n",
      "[MultiHeadAttention] concatenated output size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] projected output size=torch.Size([5, 10, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 10, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 10, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 10, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.8346e+00, -7.4305e-01, -1.5199e-01,  ...,  6.0437e-01,\n",
       "           3.0892e-01, -2.6203e-01],\n",
       "         [ 4.3891e+00, -3.2669e-03,  2.4501e-01,  ...,  4.7756e-01,\n",
       "          -7.6516e-01,  1.8474e-01],\n",
       "         [ 4.2455e+00,  5.2228e-01,  1.9544e-01,  ...,  3.9973e-01,\n",
       "          -1.2112e+00,  3.9558e-02],\n",
       "         ...,\n",
       "         [ 3.4253e+00, -8.0995e-01, -1.8052e-01,  ...,  1.7636e-01,\n",
       "          -3.0822e-01, -6.1162e-01],\n",
       "         [ 3.9821e+00, -5.2248e-01,  2.2056e-01,  ...,  5.7183e-01,\n",
       "           7.4778e-02, -1.0841e+00],\n",
       "         [ 4.0259e+00, -1.8689e-01, -2.2434e-01,  ..., -1.6212e-01,\n",
       "           2.4365e-01,  2.6721e-01]],\n",
       "\n",
       "        [[ 4.4928e+00, -2.9093e-01, -3.3372e-01,  ..., -4.9624e-01,\n",
       "          -5.8743e-01, -4.5782e-01],\n",
       "         [ 4.2599e+00,  3.6836e-01, -8.1547e-03,  ..., -7.4190e-01,\n",
       "          -2.3468e-01, -4.7133e-01],\n",
       "         [ 3.4088e+00, -6.7641e-01, -2.2919e-01,  ..., -3.5993e-01,\n",
       "          -1.1139e+00, -1.2430e+00],\n",
       "         ...,\n",
       "         [ 4.3702e+00,  1.1254e+00, -9.8362e-03,  ...,  9.8369e-02,\n",
       "          -1.0758e+00,  3.8453e-01],\n",
       "         [ 4.1806e+00, -1.3276e-01,  1.4710e-01,  ..., -8.4542e-02,\n",
       "           6.5968e-01, -5.8224e-02],\n",
       "         [ 2.7910e+00,  8.8287e-01,  1.9085e-02,  ..., -8.5505e-01,\n",
       "          -4.6704e-01, -9.6286e-01]],\n",
       "\n",
       "        [[ 1.7411e+00,  1.7561e-01, -9.1777e-02,  ..., -9.4938e-02,\n",
       "          -7.0125e-02,  5.4810e-01],\n",
       "         [ 4.4330e+00, -4.0614e-02, -9.3000e-01,  ...,  5.8632e-01,\n",
       "           8.5199e-01,  3.2613e-01],\n",
       "         [ 4.0065e+00, -3.8860e-01,  1.3300e-01,  ...,  4.2493e-01,\n",
       "          -8.4257e-01, -9.4747e-01],\n",
       "         ...,\n",
       "         [ 4.0028e+00, -3.5933e-01, -7.4696e-01,  ..., -6.0526e-01,\n",
       "          -5.2416e-01, -1.7080e+00],\n",
       "         [ 1.9638e+00,  2.1324e+00,  1.3952e-01,  ...,  9.2989e-01,\n",
       "          -3.1165e-01,  7.0653e-02],\n",
       "         [ 4.3333e+00,  9.7720e-01, -1.0806e-01,  ...,  6.2465e-01,\n",
       "          -7.3475e-01,  4.4689e-01]],\n",
       "\n",
       "        [[ 3.7414e+00, -4.0093e-01, -1.0998e-01,  ...,  3.0867e-01,\n",
       "          -1.0084e+00, -1.6803e-01],\n",
       "         [ 1.5871e+00, -3.3424e-01, -1.1196e+00,  ...,  8.3043e-01,\n",
       "          -3.3293e-01, -5.2377e-01],\n",
       "         [ 2.8867e+00, -2.8988e-01,  6.8937e-02,  ...,  1.7027e-01,\n",
       "          -3.8680e-01, -1.4566e-01],\n",
       "         ...,\n",
       "         [ 3.1504e+00, -3.2355e-02,  1.6460e-01,  ...,  2.7232e-02,\n",
       "           6.7528e-01, -4.1252e-01],\n",
       "         [ 5.0428e+00,  1.0034e+00,  3.8388e-01,  ...,  8.8603e-01,\n",
       "          -1.1967e+00,  4.4349e-02],\n",
       "         [ 4.1755e+00,  5.0763e-01,  1.5665e-01,  ..., -3.6090e-01,\n",
       "          -8.1158e-01,  4.2539e-02]],\n",
       "\n",
       "        [[ 3.6083e+00, -5.0406e-01,  1.3725e-01,  ...,  3.1394e-01,\n",
       "          -5.3159e-01, -8.4095e-01],\n",
       "         [ 4.0363e+00,  3.5442e-01, -7.3723e-01,  ..., -9.8786e-01,\n",
       "          -6.2845e-01, -3.1002e-01],\n",
       "         [ 4.2668e+00,  9.6921e-01,  1.0070e-01,  ..., -9.9665e-01,\n",
       "          -4.5861e-01, -9.6754e-01],\n",
       "         ...,\n",
       "         [ 2.3440e+00, -6.6081e-01, -6.0697e-01,  ..., -6.1114e-01,\n",
       "          -8.6263e-01, -3.1293e-01],\n",
       "         [ 2.1452e+00,  6.2028e-01, -6.2463e-01,  ...,  3.1326e-02,\n",
       "          -6.2098e-01, -7.3979e-02],\n",
       "         [ 3.4539e+00,  4.3527e-01,  7.1768e-01,  ..., -7.2893e-01,\n",
       "           3.6676e-01,  4.0626e-01]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc(torch.rand(5, 10, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder consists of 6 consecutive encoder blocks, so can simply be implemented like the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    level = TensorLoggingLevels.enc_dec\n",
    "    def __init__(self, n_blocks=6, d_model=512,\n",
    "                 n_heads=8, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoders = nn.ModuleList([\n",
    "            EncoderBlock(d_model=d_model, d_feature=d_model // n_heads,\n",
    "                         d_ff=d_ff, dropout=dropout)\n",
    "            for _ in range(n_blocks)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x: torch.FloatTensor, mask=None):\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder is mostly the same as the encoder. There's just one additional multi-head attention block that takes the target sentence as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://i1.wp.com/mlexplained.com/wp-content/uploads/2017/12/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88-2017-12-29-19.14.47.png?w=287)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keys and values are the outputs of the encoder, and the queries are the outputs of the multi-head attention over the target entence embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    level = TensorLoggingLevels.enc_dec_block\n",
    "    def __init__(self, d_model=512, d_feature=64,\n",
    "                 d_ff=2048, n_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.masked_attn_head = MultiHeadAttention(d_model, d_feature, n_heads, dropout)\n",
    "        self.attn_head = MultiHeadAttention(d_model, d_feature, n_heads, dropout)\n",
    "        self.position_wise_feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "\n",
    "        self.layer_norm1 = LayerNorm(d_model)\n",
    "        self.layer_norm2 = LayerNorm(d_model)\n",
    "        self.layer_norm3 = LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_out, \n",
    "                src_mask=None, tgt_mask=None):\n",
    "        # Apply attention to inputs\n",
    "        att = self.masked_attn_head(x, x, x, mask=src_mask)\n",
    "        x = x + self.dropout(self.layer_norm1(att))\n",
    "        # Apply attention to the encoder outputs and outputs of the previous layer\n",
    "        att = self.attn_head(queries=x, keys=enc_out, values=enc_out, mask=tgt_mask)\n",
    "        x = x + self.dropout(self.layer_norm2(att))\n",
    "        # Apply position-wise feedforward network\n",
    "        pos = self.position_wise_feed_forward(x)\n",
    "        x = x + self.dropout(self.layer_norm2(pos))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EncoderBlock] Encoder block input size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] Input queries size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] output of single head size=torch.Size([5, 10, 64])\n",
      "[MultiHeadAttention] concatenated output size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] projected output size=torch.Size([5, 10, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 10, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 10, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] Input queries size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] output of single head size=torch.Size([5, 10, 64])\n",
      "[MultiHeadAttention] concatenated output size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] projected output size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] Input queries size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] output of single head size=torch.Size([5, 10, 64])\n",
      "[MultiHeadAttention] concatenated output size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] projected output size=torch.Size([5, 10, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.7539,  1.7319,  0.0315,  ...,  0.0723,  1.6455,  3.1135],\n",
       "         [ 1.5491,  0.5092,  0.0440,  ..., -0.4172,  1.7473,  2.5684],\n",
       "         [ 0.9116,  1.3977, -1.6380,  ..., -0.5238,  0.6878,  1.9830],\n",
       "         ...,\n",
       "         [ 1.1654,  1.5397, -1.0929,  ..., -0.4030,  2.1998,  1.5289],\n",
       "         [ 1.9782,  1.1124,  0.3097,  ..., -0.2545,  1.8794,  2.4676],\n",
       "         [ 1.2787,  1.9875, -0.9133,  ...,  0.7860,  0.9329,  2.9307]],\n",
       "\n",
       "        [[-0.2970,  1.4840, -0.8573,  ..., -0.6599,  1.4021,  1.9822],\n",
       "         [ 0.5374,  0.9221,  0.4522,  ..., -0.3725, -0.4283,  2.2946],\n",
       "         [-0.5751,  0.2213,  0.0646,  ..., -0.5866,  0.8773,  1.3984],\n",
       "         ...,\n",
       "         [ 0.6994,  1.1560, -0.2433,  ..., -0.7659,  2.5076,  3.0898],\n",
       "         [ 1.2502,  0.2776,  0.0700,  ..., -1.1239,  2.2884,  4.1344],\n",
       "         [ 0.9814,  0.2584, -0.8471,  ..., -1.1770,  1.8067,  2.7638]],\n",
       "\n",
       "        [[ 0.2110,  1.3540, -0.2460,  ..., -0.1773,  2.9904,  2.4827],\n",
       "         [ 0.5595,  2.2145, -0.0809,  ...,  0.3557,  3.6478,  2.8764],\n",
       "         [-0.5593,  0.9148, -1.2750,  ..., -0.1604,  2.5614,  2.1978],\n",
       "         ...,\n",
       "         [-0.3714,  1.2415, -0.4276,  ..., -0.1922,  2.2984,  0.6497],\n",
       "         [ 0.7306,  2.1059, -0.0945,  ...,  0.2825,  3.0444,  1.8527],\n",
       "         [ 0.3192,  1.7796, -0.3994,  ..., -0.1624,  2.0556,  2.1054]],\n",
       "\n",
       "        [[ 0.2925,  1.5971, -1.1854,  ..., -0.4209,  2.7074,  3.3913],\n",
       "         [ 0.6408,  1.6328, -2.4240,  ..., -0.5597,  2.3048,  3.5795],\n",
       "         [ 1.0939,  2.7546, -0.4246,  ..., -0.9170,  2.3278,  4.1368],\n",
       "         ...,\n",
       "         [ 0.6297,  0.5950, -0.0879,  ..., -0.1944,  0.7776,  0.5223],\n",
       "         [-0.0903,  2.2571, -1.2071,  ..., -0.4660,  1.4620,  0.3589],\n",
       "         [ 0.3469,  0.9941, -0.5804,  ...,  0.0765,  2.1133,  4.0442]],\n",
       "\n",
       "        [[ 0.2465,  0.5541, -0.3551,  ...,  0.9608,  1.6621,  3.2365],\n",
       "         [ 0.4394, -0.0092, -0.5763,  ...,  0.5018,  2.3055,  2.6984],\n",
       "         [ 0.6045,  0.3013, -0.0960,  ..., -0.1607,  1.6246,  3.4088],\n",
       "         ...,\n",
       "         [ 1.3267,  0.3277, -1.5987,  ..., -0.7834,  1.8397,  2.6718],\n",
       "         [ 1.1085,  0.6523, -0.8038,  ...,  0.5745,  1.3422,  1.8084],\n",
       "         [ 1.4197,  0.4619, -0.6926,  ..., -0.5779,  1.2310,  2.8757]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec = DecoderBlock()\n",
    "dec(torch.rand(5, 10, 512), enc(torch.rand(5, 10, 512)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the decoder is just a stack of the underlying block so is simple to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    level = TensorLoggingLevels.enc_dec\n",
    "    def __init__(self, n_blocks=6, d_model=512, d_feature=64,\n",
    "                 d_ff=2048, n_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.position_embedding = PositionalEmbedding(d_model)\n",
    "        self.decoders = nn.ModuleList([\n",
    "            DecoderBlock(d_model=d_model, d_feature=d_model // n_heads,\n",
    "                         d_ff=d_ff, dropout=dropout)\n",
    "            for _ in range(n_blocks)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x: torch.FloatTensor, \n",
    "                enc_out: torch.FloatTensor, \n",
    "                src_mask=None, tgt_mask=None):\n",
    "        for decoder in self.decoders:\n",
    "            x = decoder(x, enc_out, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention blocks are just simple matrix multiplications: therefore they don't have any notion of order! The Transformer explicitly adds positional information via the positional embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    level = 1\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.weight = nn.Parameter(pe, requires_grad=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.weight[:, :x.size(1), :] # (1, Seq, Feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordPositionEmbedding(nn.Module):\n",
    "    level = 1\n",
    "    def __init__(self, vocab_size, d_model=512):\n",
    "        super().__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model)\n",
    "        \n",
    "    def forward(self, x: torch.LongTensor, mask=None) -> torch.FloatTensor:\n",
    "        return self.word_embedding(x) + self.position_embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = WordPositionEmbedding(1000)\n",
    "encoder = TransformerEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] Input queries size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] output of single head size=torch.Size([5, 30, 64])\n",
      "[MultiHeadAttention] concatenated output size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] projected output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] Input queries size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] output of single head size=torch.Size([5, 30, 64])\n",
      "[MultiHeadAttention] concatenated output size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] projected output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] Input queries size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] output of single head size=torch.Size([5, 30, 64])\n",
      "[MultiHeadAttention] concatenated output size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] projected output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] Input queries size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] output of single head size=torch.Size([5, 30, 64])\n",
      "[MultiHeadAttention] concatenated output size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] projected output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] Input queries size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] output of single head size=torch.Size([5, 30, 64])\n",
      "[MultiHeadAttention] concatenated output size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] projected output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] Input queries size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] output of single head size=torch.Size([5, 30, 64])\n",
      "[MultiHeadAttention] concatenated output size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] projected output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.6179, -0.9129, -1.1146,  ..., -2.4037,  1.2557,  0.5186],\n",
       "         [ 0.0916,  2.5295, -2.1315,  ..., -0.4497,  4.3204,  2.8204],\n",
       "         [ 1.5603, -3.4203, -2.0865,  ..., -0.4914,  1.3665,  2.6843],\n",
       "         ...,\n",
       "         [ 2.5888, -1.3422, -4.9319,  ..., -0.1028,  0.8384,  0.3542],\n",
       "         [-5.0183, -4.6209, -0.9035,  ...,  0.4303, -0.1303,  4.3591],\n",
       "         [-1.7145, -1.2137,  2.0444,  ...,  0.1812,  1.1217,  3.8644]],\n",
       "\n",
       "        [[ 2.0313, -2.1645, -1.9350,  ..., -1.3981,  1.4702, -0.4281],\n",
       "         [-1.2277,  2.5563, -0.6967,  ...,  0.6443, -2.3782, -2.0074],\n",
       "         [-1.4581,  0.0839, -0.2799,  ..., -1.2875,  1.1970,  2.3062],\n",
       "         ...,\n",
       "         [-2.3831, -0.4709, -2.0701,  ...,  3.8697,  0.6059, -3.4711],\n",
       "         [ 0.4490,  0.5291,  2.7600,  ...,  1.3978,  1.0326, -1.8111],\n",
       "         [ 0.5248, -5.2180, -4.3385,  ...,  4.0048,  0.5965, -0.1954]],\n",
       "\n",
       "        [[ 1.1011,  3.0193, -6.9870,  ..., -2.1411,  1.3825,  0.8011],\n",
       "         [-1.0197, -2.8118, -5.5080,  ...,  3.4763,  2.2277,  0.0890],\n",
       "         [-1.3826, -1.6969, -1.4467,  ...,  3.8028, -0.9136, -0.3560],\n",
       "         ...,\n",
       "         [-1.2264,  1.4351, -4.2123,  ..., -0.6309, -1.4549, -1.9784],\n",
       "         [-7.1766, -0.6841, -2.6179,  ..., -0.2601, -0.0744, -0.3209],\n",
       "         [-2.1401, -0.1838, -6.1658,  ..., -1.3570, -2.0033,  4.9300]],\n",
       "\n",
       "        [[ 2.7114,  0.5631, -4.6310,  ..., -0.6070,  1.4394,  1.6799],\n",
       "         [ 3.8718, -2.7854, -0.9394,  ...,  0.3282,  3.2705,  3.3947],\n",
       "         [-0.9683, -2.5447,  2.2300,  ...,  0.3237, -3.3475,  1.2621],\n",
       "         ...,\n",
       "         [ 0.2158, -4.1396, -2.1032,  ..., -0.6964, -1.3234,  4.7453],\n",
       "         [-1.8302, -6.6640, -0.4434,  ...,  0.5833,  2.7051,  1.8170],\n",
       "         [ 2.0760, -0.9523, -4.4222,  ..., -1.8848,  1.8658,  1.4486]],\n",
       "\n",
       "        [[ 2.6665,  0.8288,  1.2940,  ...,  1.5792,  0.1903,  1.9555],\n",
       "         [ 4.2522,  0.7342,  0.9000,  ...,  1.1179,  1.3893,  2.4653],\n",
       "         [ 6.0333, -0.7581, -1.3763,  ...,  1.7336, -2.5313, -1.7228],\n",
       "         ...,\n",
       "         [ 0.8968, -1.2821, -0.7519,  ...,  4.2398, -3.4045,  0.7367],\n",
       "         [ 1.1741, -3.5128,  2.4998,  ...,  1.4319,  1.1795,  0.8753],\n",
       "         [ 2.7520, -3.7658, -1.0388,  ...,  3.6936, -2.6106,  3.7085]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(emb(torch.randint(1000, (5, 30))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put everything together now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://camo.githubusercontent.com/88e8f36ce61dedfd2491885b8df2f68c4d1f92f5/687474703a2f2f696d6775722e636f6d2f316b72463252362e706e67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll supress logging from the scaled dot product attention now\n",
    "logger.setLevel(TensorLoggingLevels.enc_dec_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = WordPositionEmbedding(1000)\n",
    "encoder = TransformerEncoder()\n",
    "decoder = TransformerDecoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.9257,  1.4525, -1.6411,  ..., -0.0952, -0.6878,  6.8743],\n",
       "         [-1.9338,  3.0697, -2.5299,  ...,  4.5336, -2.4466,  8.6706],\n",
       "         [-4.1225,  2.8537, -4.6281,  ..., -1.2991, -0.5202,  9.1048],\n",
       "         ...,\n",
       "         [ 1.0886,  0.7016, -4.6155,  ...,  6.5053,  1.7904,  7.6790],\n",
       "         [-3.7776, -0.6953, -9.0473,  ...,  7.3665,  1.2213,  3.6967],\n",
       "         [ 0.4485,  1.7003, -3.4540,  ...,  7.4460, -3.4167,  5.7489]],\n",
       "\n",
       "        [[-3.9350,  4.8898, -4.7293,  ...,  3.6016,  5.7632,  4.7220],\n",
       "         [ 1.5016,  1.2416, -7.1086,  ...,  6.4188, -0.8304,  5.8426],\n",
       "         [-3.1504,  0.9825, -6.5698,  ...,  9.4508, -1.6345,  2.9084],\n",
       "         ...,\n",
       "         [-5.7944,  1.2646, -4.3030,  ...,  7.2413, -1.0951,  1.7650],\n",
       "         [-3.4720, -4.2639, -6.6723,  ...,  5.8381,  3.3105,  7.4766],\n",
       "         [-2.5388,  0.0677, -9.8157,  ...,  6.1770, -0.3951,  5.8490]],\n",
       "\n",
       "        [[ 0.1910,  2.5887, -3.6397,  ...,  2.0766,  3.5951,  8.8904],\n",
       "         [ 1.0138,  3.6099, -2.3926,  ...,  5.7470,  3.2762,  8.8692],\n",
       "         [-1.3351,  0.4020, -2.5466,  ...,  4.5556,  0.4713,  9.6895],\n",
       "         ...,\n",
       "         [ 2.6186,  1.7712,  2.3781,  ...,  7.9307,  0.8701,  7.1573],\n",
       "         [-0.7652,  0.1678, -3.0550,  ...,  4.1666,  2.2341,  2.1027],\n",
       "         [-0.9989, -2.3889, -4.6011,  ...,  3.7121, -1.4445,  9.6689]],\n",
       "\n",
       "        [[-0.6880,  5.1757, -2.4814,  ...,  3.4376,  3.1818,  6.0955],\n",
       "         [ 1.6613,  0.6716, -2.2689,  ...,  6.8231,  1.8462,  4.1288],\n",
       "         [ 3.1004,  0.2224, -6.3301,  ...,  6.4839,  3.9485,  7.1276],\n",
       "         ...,\n",
       "         [ 3.6123,  4.8668, -5.4612,  ...,  2.1406, -2.9384,  3.8490],\n",
       "         [ 3.2818, -1.1043,  3.8861,  ...,  6.8822,  1.6453,  7.9141],\n",
       "         [ 1.6273,  0.9399, -0.2505,  ...,  4.9273,  5.1858,  5.2493]],\n",
       "\n",
       "        [[-1.8575, -1.1754, -0.5295,  ...,  8.3372,  0.6572,  2.9868],\n",
       "         [-0.3557, -4.4425, -4.5504,  ..., 10.6727, -2.2701,  5.7749],\n",
       "         [ 0.8695,  0.9001, -8.0555,  ...,  9.4198, -3.3246,  4.5401],\n",
       "         ...,\n",
       "         [-4.0743, -1.1492, -6.0522,  ...,  5.8506,  0.4282,  5.4175],\n",
       "         [-3.5244, -3.9848, -3.3920,  ...,  5.5266,  1.5918,  7.5220],\n",
       "         [-5.6467,  3.3867, -8.0712,  ...,  6.5992,  3.9642,  9.8302]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_ids = torch.randint(1000, (5, 30))\n",
    "tgt_ids = torch.randint(1000, (5, 30))\n",
    "x = encoder(emb(src_ids))\n",
    "decoder(emb(tgt_ids), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
