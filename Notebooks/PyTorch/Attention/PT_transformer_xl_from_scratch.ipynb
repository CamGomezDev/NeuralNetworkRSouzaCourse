{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll be implementing the Transformer-XL architecture from scratch.\n",
    "To run this notebook in full, make sure to use the `download_data.sh` script to download the Penn Treebank data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review of the Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start off with a quick overview of the Transformer architecture which will be the basis of the Transformer XL.\n",
    "\n",
    "Overall, the Transformer architecture is composed of multiple MultiHeadAttention layer stacked on top of each other, followed by feedforward layers, residual connections, and layer normalization layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://camo.githubusercontent.com/88e8f36ce61dedfd2491885b8df2f68c4d1f92f5/687474703a2f2f696d6775722e636f6d2f316b72463252362e706e67)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MultiHeadAttention layer is composed of multiple attention heads. Each attention head applies a linear transformation to its inputs and computes attention over its input values using keys and queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://i2.wp.com/mlexplained.com/wp-content/uploads/2017/12/multi_head_attention.png?zoom=2&resize=224%2C293)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach is incapable of handling position, so the Transformer adds embeddings representing the position of the input to the word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details, please refer to [this tutorial](https://github.com/keitakurita/Practical_NLP_in_PyTorch/blob/master/deep_dives/transformer_from_scratch.ipynb) I've written on just the Transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be building the Transformer XL by first implementing a single attention head, scaling it to compose the MultiHeadAttention layer for the Transformer XL, then building the DecoderBlock and stacking them to create the full Transformer XL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the Transformer XL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Single Attention Head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start off by implementing a single attention head in a MultiHeadAttention layer. To make things concrete, let's consider the first layer and assume we receive an input of word embeddings of shape `(seq=7, batch_size=3, embedding_dim=32)`. Note that the Transformer XL does not add positional embeddings to the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq, batch_size, embedding_dim = 7, 3, 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embs = torch.rand(seq, batch_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Transformer XL, we also feed the cached outputs of the model for the previous sequence. In this case, we would be feeding the word embeddings from the previous sequence as an additional input to our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://4.bp.blogspot.com/-Do42uKiMvKg/XFCns7oXi5I/AAAAAAAADuc/ZS-p1XHZUNo3K9wv6nRG5AmdEK7mJsrugCLcBGAs/s1600/xl-eval.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things concrete, let's imagine our previous sequence was of length `prev_seq=6`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_seq = 6\n",
    "memory = torch.rand(prev_seq, batch_size, embedding_dim) # hidden states from the previous sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each attention head takes keys, queries, and values as input. The processing goes like this:\n",
    "\n",
    "1. Apply a separate linear transformation to each of the keys, queries, and values.\n",
    "2. Compute attention scores for each of the values.\n",
    "3. For each query, compute an attention-weighted sum of the values.\n",
    "4. Apply a residual connection and layer normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start off with the linear transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_dim = 17 # this will be the internal dimension\n",
    "linear_k = nn.Linear(embedding_dim, inner_dim)\n",
    "linear_v = nn.Linear(embedding_dim, inner_dim)\n",
    "linear_q = nn.Linear(embedding_dim, inner_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The memory is concatenated across the sequence dimension and fed as keys/values. Be careful, as it's not concatenated with the queries. This is because each query represents one word we want to predict, so we can't modify the number of queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embs_w_memory = torch.cat([memory, word_embs], dim=0)\n",
    "k_tfmd = linear_k(word_embs_w_memory)\n",
    "v_tfmd = linear_v(word_embs_w_memory)\n",
    "q_tfmd = linear_q(word_embs) # No memory for the queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compute scaled dot product attention as per the usual Transformer. Scaled dot product attention computes the attention score as the dot product between the query and key vectors. To prevent the values from exploding as the dimensionality of the vectors increases, we divide the raw attention score by the sqrt of the embedding size.\n",
    "\n",
    "$$ \\textrm{Attention}(Q, K, V) = \\textrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V $$\n",
    "\n",
    "![image](https://i2.wp.com/mlexplained.com/wp-content/uploads/2017/12/scaled_dot_product_attention.png?zoom=2&w=750)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using einsum notation here to make the code easy to read: if you're not familiar with einsum, check out [this awesome tutorial](https://rockt.github.io/2018/04/30/einsum). In short, einsum denotes the shape of the inputs and outputs using one letter to represent each dimension. Below, the inputs are shaped `(i, b, d)` and `(j, b, d)` and the output is shaped `(i, j, b)` where the same letter represents the same size. Einsum is computed by taking the dot product across dimensions with the same character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_attn = torch.einsum(\"ibd,jbd->ijb\", q_tfmd, k_tfmd) / (embedding_dim ** 0.5) # scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we're not yet applying the softmax activation. This is because we need a couple more pieces to get the full attention score. The first of these is the relative positional embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative positional encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the key ideas in the Transformer XL is the idea of relative positional encodings. Instead of having a single embedding represent each **absolute** position, the Transformer XL computes an embedding that represents the **distance** between any two tokens. This is used to compute the attention between the two words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors use the following equation to compute the attention between a query vector $ q_i $ and key vector $k_j$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "A^{rel}_{i,j} = \n",
    "    \\underbrace{E_{x_i}^TW_q^TW_{k,E}E_{x_j}}_{(a)}\n",
    "    + \\underbrace{E_{x_i}^TW_q^TW_{k,R} \\color{green}R_\\color{green}{i-j} }_{(b)}\n",
    "    \\\\ \n",
    "    + \\underbrace{ \\color{red}u^\\color{red}T W_{k,E}E_{x_j}}_{(c)} \n",
    "    + \\underbrace{ \\color{red}v^\\color{red}T W_{k,R} \\color{green}R_\\color{green}{i-j}}_{(d)}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, $E_{x}$ is the embedding for $ x $ and $ W $ are all transformation matrices. The (a) term is the content-based attention terms that we already computed above. (b) and (d) are based on the relative positional embeddings and are dependent on the distance between $q_i$ and $k_j$. $u$ and $v$ are global bias terms that represent biases for certain content and certain positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move on to the detailed implementation of terms (b) to (d). We'll first add the content bias (term (c) in the above equation) since it is the most simple to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = torch.rand(17).expand_as(q_tfmd)\n",
    "content_attn = content_attn + torch.einsum(\"ibd,jbd->ijb\", u, k_tfmd) / (embedding_dim ** 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute the relative positional embeddings necessary for the positional attention terms. For the relative positional embeddings, the Transformer XL uses fixed sinusoidal embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12., 11., 10.,  9.,  8.,  7.,  6.,  5.,  4.,  3.,  2.,  1.,  0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_idxs = torch.arange(seq + prev_seq - 1, -1, -1.0, dtype=torch.float)\n",
    "pos_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAiv0lEQVR4nO3deZyU1Z3v8c+vqnoHurugAaGXwg2ihLVbWTQZjfGaoNGZaNQIamLGbJplkpubbebeO8mdcW5yk2gSTXipiUbERKPRSUyUxGSMytYsKpsBoRuataHpZml6rXP/eKqhaRqBrqe6tu/79apXVT31cM6P7cvDqfOcY845REQk/QSSXYCIiAyMAlxEJE0pwEVE0pQCXEQkTSnARUTSVGgwOxsxYoSLRCKD2aWISNpbsWLFXudcWd/jgxrgkUiE2trawexSRCTtmVl9f8c1hCIikqYU4CIiaUoBLiKSphTgIiJpSgEuIpKmThngZvawme0xszW9jn3HzDaY2Rtm9oyZlSS0ShEROcHpXIH/HLiqz7FFwETn3CTgb8DXfK5LRERO4ZQB7px7GWjqc+xF51xX7O0SoDwBtR3157f2cP9fNiWyCxGRtOPHGPjHgd+f7EMzu9PMas2strGxcUAdvLZpLz9YtJG2zu6B1igiknHiCnAz+wbQBSw42TnOufnOuWrnXHVZ2Ql3gp6WmkiYju4oa7a3DLBSEZHMM+AAN7PbgauBW1yCt/WZXlUKwPK6/YnsRkQkrQwowM3sKuArwIecc63+lnSi4UPyOKesiNq6plOfLCKSJU5nGuFCYDEw3swazOwO4EfAUGCRma02s58kuE5qImFq6/cTjWoPTxEROI3VCJ1zN/dz+KEE1PKOqiNhnli+jU2Nhzh/1NDB7l5EJOWkzZ2YNZGecXANo4iIQBoFeGW4kLKhedTqi0wRESCNAtzMqImU6gpcRCQmbQIcoLoqTMP+I+xsOZLsUkREki6tArwmEgbQMIqICGkW4O86ayiFuUHNBxcRIc0CPBQMMK2yVHdkioiQZgEOUB0pZf2uAxxo60x2KSIiSZV2AV4TCeMcrKzXVbiIZLe0C/ApFSUEA6YvMkUk66VdgBflhbhwzDDNBxeRrJd2AQ7efPDV25rp6IomuxQRkaRJywCviZTS3hVlzQ5t8CAi2SstA3x6bGErzQcXkWyWlgE+cmg+keGFmg8uIlktLQMcvPXBa+uaSPBubiIiKSttA7wmUsr+1k7ebjyc7FJERJIibQO8+ujCVhoHF5HslLYBfvaIIoYX5WocXESyVtoGuJlRHSmltl5X4CKSndI2wMFbF6V+Xyt7DrQluxQRkUGX1gF+dBxcC1uJSBZK6wC/cMww8nMCWhdFRLJSWgd4TjDA1IpSrUwoIlnplAFuZg+b2R4zW9PrWNjMFpnZxthzaWLLPLmaSClrd7RwqL0rWSWIiCTF6VyB/xy4qs+xrwJ/cs6dB/wp9j4pqiNhog5Wb21OVgkiIklxygB3zr0M9B1kvhZ4JPb6EeA6f8s6fVMrSwgYGgcXkawz0DHwUc65nbHXu4BRJzvRzO40s1ozq21sbBxgdyc3ND+Hd501TPPBRSTrxP0lpvNWkzrpilLOufnOuWrnXHVZWVm83fWrJhJm1dZmOru1wYOIZI+BBvhuMzsLIPa8x7+Szlx1pJTWjm7W7zyQzDJERAbVQAP8OeC22OvbgGf9KWdgqqu8G3q0LoqIZJPTmUa4EFgMjDezBjO7A7gHeL+ZbQSuiL1PmtHF+VSEC1i+RePgIpI9Qqc6wTl380k+ep/PtcSlpirMyxsbcc5hZskuR0Qk4dL6TszeqiNh9h7qoG5fa7JLEREZFBkT4DWxjY41H1xEskXGBPg5ZUMoKczRDj0ikjUyJsADAaO6SgtbiUj2yJgAB28cfPPew+w91J7sUkREEi6jArxnHFxX4SKSDTIqwCeOLSY3FNA4uIhkhYwK8LxQkCnlJSzXFmsikgUyKsABasaVsnZ7C60d2uBBRDJbxgV4dSRMV9SxeltzsksREUmojAvwaZWlmOmLTBHJfBkX4MUFOYwfNVR3ZIpIxsu4AAdvg4eV9fvp0gYPIpLBMjLAqyOlHO7oZsOug8kuRUQkYTIywGsi3gYPmg8uIpksIwN8TEkBY0sKNB9cRDJaRgY4eMMotXVNeHsui4hkngwO8DC7D7TTsP9IsksREUmIjA1wbfAgIpkuYwP8/JFDGZof0k71IpKxMjbAj23woCtwEclMGRvg4I2Db9xziP2HO5JdioiI7zI6wI/OB9d0QhHJQBkd4JPKi8kNaoMHEclMcQW4mX3RzNaa2RozW2hm+X4V5of8nCDvLi/WTBQRyUgDDnAzGwt8Dqh2zk0EgsBNfhXml+pIKW9ub6GtszvZpYiI+CreIZQQUGBmIaAQ2BF/Sf6qqQrT2e14XRs8iEiGGXCAO+e2A98FtgI7gRbn3It9zzOzO82s1sxqGxsbB17pAE2viu1Ury8yRSTDxDOEUgpcC4wDxgBFZja373nOufnOuWrnXHVZWdnAKx2g0qJczhs5ROPgIpJx4hlCuQLY4pxrdM51Ak8Ds/wpy1/VkTAr6vfTHdXCViKSOeIJ8K3ADDMrNDMD3ges96csf9VESjnY1sXfdmuDBxHJHPGMgS8FngJWAm/G2prvU12+0gYPIpKJ4pqF4pz7n865Cc65ic65ec65dr8K81N5aQGjh+VrYSsRySgZfSdmDzM7usGDiEimyIoAB28YZUdLG9ubtcGDiGSGrAnw6tgGD7oKF5FMkTUBPmH0MIbkhTQfXEQyRtYEeDBgTKsqpVZfZIpIhsiaAAeoqSrlrd0HaWntTHYpIiJxy6oAr46EcQ5WbtVVuIikv6wK8CkVJYQCpnFwEckIWRXgBblBJo4t1ji4iGSErApw8NZFWd3QTHuXNngQkfSWdQFeHQnT0RVlzfaWZJciIhKX7Avw2AYPy7ZoGEVE0lvWBfjwIXmcXVakOzJFJO1lXYCDt09mbf1+otrgQUTSWFYGeHWklJYjnazbeSDZpYiIDFhWBvjlE0aSGwzwq9ptyS5FRGTAsjLAhw/JY86ks3h65XYOtXcluxwRkQHJygAHmDezikPtXTyzsiHZpYiIDEjWBvjUihImjh3Go4vrcU5fZopI+snaADczbp0RYeOeQyzdoimFIpJ+sjbAAa6ZPIbighx+sbg+2aWIiJyxrA7wgtwgN0wv54W1u9h9oC3Z5YiInJGsDnCAuTOq6Io6Hl+6NdmliIickawP8MiIIt57fhkLl22lszua7HJERE5bXAFuZiVm9pSZbTCz9WY206/CBtOtM6vYc7CdF9fuTnYpIiKnLd4r8HuBPzjnJgCTgfXxlzT4/m78SMaWFPDo4rpklyIictoGHOBmVgy8B3gIwDnX4Zxr9qmu461+HJ69KyFNg7dj/dwZVSzd0sRbuw4mrB8RET/FcwU+DmgEfmZmq8zsQTMr6nuSmd1pZrVmVtvY2Diwng7uglW/gJ2vx1HuO7uxpoLcUIDHlmhKoYikh3gCPARMAx5wzk0FDgNf7XuSc26+c67aOVddVlY2sJ5q7oC8YfDKD+Io952Fi3K5etJZPL2ygYNtnQnrR0TEL/EEeAPQ4JxbGnv/FF6g+y+/GKo/Dut+A/veTkgXALfOjHC4o5tnVm1PWB8iIn4ZcIA753YB28xsfOzQ+4B1vlTVnxmfgUAOvPbDhHUxubyYd48t1vooIpIW4p2FcjewwMzeAKYA/xZ3RSczdBRMvQVWL/DGxBPAzJg3s4pNew6xZLPWRxGR1BZXgDvnVsfGtyc5565zziV2p+BZd0O0C5Y8kLAuPjR5DCWFOfxiSV3C+hAR8UN63YkZPhsu/HuofRjaWhLSRX5OkI9UV/DC2t3satH6KCKSutIrwAFmfwHaD8DyhxLWxS0XVxJ1jseXaX0UEUld6RfgZ02Cc6+AJfdD55GEdFE1XOujiEjqS78AB7jki3C40ftCM0FunVlF48F2XlibmC9MRUTilZ4BXjUbymvg1fugOzGbEr/3/JFUhAt4VJs9iEiKSs8AN/OuwpvrvZt7EiAYMOZeXMWyLU1s2HUgIX2IiMQjPQMc4PwPwIjx8Mr3IUE33dxQrfVRRCR1pW+ABwJwyRdg9xrY9MeEdBEuyuWaSWN4ZuV2rY8iIiknfQMcYOL1MKzcuwpPkFtnVnG4o5unV2p9FBFJLekd4KFcmHUX1L8KW5ee+vwBmFxRwuTyYn6xROujiEhqSe8AB5h2KxSE4dUfJKyLuTO89VEWb96XsD5ERM5U+gd4bhFc/Cl463nYk5gd3a7pWR9FUwpFJIWkf4ADXPSPkFMEr96bkObzc4LcWF3Bi+t2s7MlMXd/ioicqcwI8MIwTL8d3nwSmhOzfsncGVVEnWPhUq2PIiKpITMCHGDmZwGD136UkOYrwoVcNn4kC5dvo6NL66OISPJlToAXj4VJN8LKR+Hw3oR0MW+G1kcRkdSROQEOMPtz0NUGS3+akObfe34ZleFCfZkpIikhswK8bDxMmAPL5kP7Qd+bDwSMuTMqWVan9VFEJPkyK8DBW+SqrRlWPJKQ5m+YXkFeKKCrcBFJuswL8PJqiFwKi38EXe2+N19alMs1k8fwzKrtHND6KCKSRJkX4OBdhR/cCW/8KiHN3zqzitaObp5e0ZCQ9kVETkdmBvg5l8PoSd6NPdFu35ufVF7C5IoSrY8iIkmVmQHes+HDvo2w4XcJ6eLWGVW83XiYxW9rfRQRSY7MDHCAC66F0nEJ2/BhzqSzKC3M0ZZrIpI0cQe4mQXNbJWZ/daPgnwTCMLsz8OOlbDlZd+bz88J8pGaChat1/ooIpIcflyBfx5IzDKA8Zp8MwwZBa98LyHNz71Y66OISPLEFeBmVg7MAR70pxyf5eR7a6Rs/gtsX+l78xXhQi4fP5LHl2l9FBEZfPFegf8A+Apw0vQyszvNrNbMahsbG+PsbgCmfwzyihO24cO8mVXsPdTOH7Q+iogMsgEHuJldDexxzq14p/Occ/Odc9XOueqysrKBdjdw+cPgok/Auudg7ybfm3/PeWVUDS/kF4vrfG9bROSdxHMFPhv4kJnVAU8Al5vZY75U5beLPwWhPHjN/w0fAgFj7sVVLK/bz/qdWh9FRAbPgAPcOfc151y5cy4C3AS85Jyb61tlfhoyEqbOhdUL4cAO35u/obqcwtwg3/7dOrqjurFHRAZH5s4D72vW3eCisOR+35suKczlf11zIa9u2scDf/F/mEZEpD++BLhz7i/Ouav9aCthSiMw8R+g9mdwZL/vzd9QXc61U8bwvUV/Y9mWJt/bFxHpK3uuwAFmfwE6DsFy/2c9mhn/5+/fTWW4kM8tXEXT4Q7f+xAR6S27Anz0RDjvSljyE+ho9b35IXkhfvTRaTQd7uDLT75OVOPhIpJA2RXg4C1y1boXVi9ISPMTxxbzzavfxUsb9vDQK1sS0oeICGRjgFfOhIqL4dX7oDsxGzLMm1HFVReO5j/+sIFVW/0fbxcRgWwMcDO49MvQshVe/OcEdWH8x/WTGF2cz90LV9FyRDv3iIj/si/AAc6/EmZ8BpY+4M1KSYDighx+ePNUdrW08dVfv6GNH0TEd9kZ4ADv/xacewU8/+WELDcLMLWylK9cNZ7fr9nFY0u0briI+Ct7AzwYgusfhvA58Mt5sO/thHTziUvO5rLxZXzrt+tZu6MlIX2ISHbK3gAHyC+Gjz7hjYsvvAmONPveRSBg/L+PTKG0KIe7Hl/FofYu3/sQkeyU3QEOED4bbnwMmjbDUx+Dbv8DNlyUy303TaV+32G+8cybGg8XEV8owAEil8Cc78HbL8GL30hIFxefPZwvXnE+z67ewZO1DQnpQ0SyiwK8x/TbYMZnYelPoPbhhHTxmcvOZfa5w/mX59bwt90HE9KHiGQPBXhvV37Lu9X++f8Om//L9+aDAeP7N05hSF6Iux5fyZGObt/7EJHsoQDvLRCEDz8Ew8+FX92akJkpI4fm8/0bp7BxzyH+93+u9b19EckeCvC+8ofBzU+ABeDxGxMyM+XS88r4zN+dwxPLt/Hs6u2+ty8i2UEB3p/wOG9myv66hM1M+eIV51NdVcrXn36TLXsP+96+iGQ+BfjJRGbD1bGZKS983ffmQ8EA9908lZxQgM8uWElbp8bDReTMKMDfybRbYeZdsOynsPwh35sfU1LAd6+fzLqdB/j359f73r6IZDYF+Km8/18TOjPligtGcccl43hkcT1/WLPT9/ZFJHMpwE+lZ2bKiPMTNjPlf1w1gUnlxXzlqTfY1uT/TkEikpkU4KcjfxjcvNAL88c/4vumyLmhAD+6eRrOwd0LV9HZHfW1fRHJTArw03V0Zko9PHm77zNTKocXcs+HJ7F6WzPffeEtX9sWkcykAD8TVbPg6u/D5r/AC1/zvfk5k85i7oxKfvryZv68YY/v7YtIZlGAn6lp82IzU+bD8gd9b/6bcy5gwuih/NOvVrOz5Yjv7YtI5hhwgJtZhZn92czWmdlaM/u8n4WltPf/K5z33+D5r8Dbf/a16fycID++ZRrtXVHu+HmtvtQUkZOK5wq8C/iSc+4CYAbwWTO7wJ+yUlwgCB9+EMrGw5O3wd5NvjZ/TtkQfnzLNLbtb2XOfX9l0brdvrYvIplhwAHunNvpnFsZe30QWA+M9auwlNezZkogBAtv9H1mymXjR/K7uy+lcngh//hoLf/+/HrNThGR4/gyBm5mEWAqsLSfz+40s1ozq21sbPSju9RRWgU3Lug1M6XT1+Yrhxfy1KdmHf1i8+b5SzQuLiJHxR3gZjYE+DXwBefcgb6fO+fmO+eqnXPVZWVl8XaXeqpmwjX3ejNTFtwALf7utpOfE+Tb172be2+awrqdB5hz3yu8/LcM+4dQRAYkrgA3sxy88F7gnHvan5LS0NRbvBDftgzunwmrFoDP+15eO2Usz911CWVD8rjtZ8v43otv0R3V3poi2SyeWSgGPASsd859z7+S0tT02+HTr8Lod8Ozn/F2uT+4y9cuzh05hN98djbXTyvnvpc2Me+hpTQebPe1DxFJH/Fcgc8G5gGXm9nq2OODPtWVnsLj4LbfwlX3eAtf/fhieONJX6/GC3KDfOeGyfzf6yexcut+PnjfX1myeZ9v7YtI+jDn83/130l1dbWrra0dtP6Sau8m+M2noWEZTLgarv4BDPH3O4ANuw7wmQUrqdt7mC9dOZ5Pv/ccAgHztQ8RST4zW+Gcq+57XHdiJsqIc+Hjf/Bu+tm4CO6/GNY+42sXE0YP47m7LmHOpDF854W3+Pgjy9l/uMPXPkQkdSnAEykQhNmfh0++DCVV3lTDJz8GrU2+dTEkL8R9N03hW9dN5LVN+5hz319ZUe/vnHQRSU0K8MEwcgLcsQgu/2dY/5/e2PiG3/nWvJkxb0YVv/70LIJB48afLubBv25mMIfHRGTwKcAHSzAE7/ky3PkXGDoKnvgoPP1JX+/gfHd5Mb+9+1IunzCSb/9uPZ96bAUtR/y9uUhEUocCfLCNngifeAne+1VY85Q3b3zjIt+aLy7I4afzpvPNOe/iT+v3cM0PX2HN9hbf2heR1KEAT4ZQLlz2NfjEn6CgFBZcD8/eBW0n3Mg6IGbGJy49m19+ciad3VH+4f7XeGxJvYZURDKMAjyZxkzxhlQu+SdYvcC7GvdxedrpVaX87nOXMuvc4XzzN2u46/FVbNpz0Lf2RSS5NA88VTTUwjOfgn0bofoOb/ph3hBfmo5GHQ/819vc+8eNdHRHufS8Edw2M8JlE0YS1LxxkZR3snngCvBU0nkEXvo2LP4xlFTCB78D517hTUf0wd5D7TyxbCuPLdnKrgNtVIQLuHVGhI9UV1BcmONLHyLiPwV4Otm6xLuLs2kzDBsLk2+GKR+F4ef40nxnd5QX1+7mkdfqWFbXREFOkOumjuX2WRHGjx7qSx8i4h8FeLrpavfmiq9eAG+/BC4KlbO8lQ8vuM634ZW1O1p49LV6frN6O+1dUWacHeb2WeO44l0jCQX1FYlIKlCAp7MDO+D1hd4ytU1vQ04RXHgdTLkFqmaBxT+Ovf9wB08s38ZjS+rZ3nyEsSUFzJ1RxU01FZQW5cb/cxCRAVOAZwLnYNtSWPWYt65KxyEoHeddlU++GYrL4+6iqzvKH9fv4ZHX6li8eR95oQDXThnDbbMiXDim2IefhIicKQV4puk4DOue84ZY6v4KGJxzmXdVPuFqyMmPu4sNuw7wyGv1PLOqgbbOKDWRUm6fNY4rLxxFjoZXRAaNAjyTNW3xhlhWPw4t2yC/GCZe712Zj5kW9xBLS2snv6rdxqNL6tjWdITRw/KZO6OSmy6qZMSQPJ9+EiJyMgrwbBCNQt3L3lj5+uegqw3K3uUF+aQbYcjIuJrvjjpe2uANr7yyaS+5wQDTqkq4KBLmonHDmVpZQlFeyKefjIj0UIBnm7YWWPO0N8TSsBwCITjncqicAWOrYcxUyB824OY37TnIL5dvY8nmJtbuaCHqIBgwJo4ZxkXjwtREvIe+ABWJnwI8mzW+5QX5+t96s1gAMCgb74V5+XQYOx1GXuitmniGDrZ1snJrM8u27GP5lv2sbmimoysKwPmjhlATCXPROO9xVnGBjz8xkeygABdPaxPsWAnbV3q372+vhdbYnpqhAjhrMpRXe4E+drp3R+gZjqG3dXbzRkMLy+uaWLaliRX1+znU3gVARbjAC/RYqI8bUYT5MA1SJJMpwKV/zkFzfSzMV3iPna974+cARWWxMI9dqY+ZBgUlZ9RFV3eUDbsOsnRLE8u3NLG8rol9sa3fRgzJ46JxpUeHXCaMHqobiET6UIDL6evuhN1rvavzhlio733r2OfDzzt2hT78bCiu9Oag5xaeVvPOOd5uPMzyOi/Ql25pYnvzEcAbRx9bUkDV8EIqwoVUhQupDBdSOdx7HpqvNVsk+yjAJT5tLbBj1bEr9YZaOLzn+HMKh0NxBZRUeM99XxeGTzocs6P5CMvrmti4+xD1Ta1s3XeYrU2t7G89fkehcFFuv8FeNbyQUUPzCWh1RclACnDxl3NwcCfsr/fmnjdv9Z5bGqB5m/e6s/X4H5NTGAvz8v5DfuhZJ3yJeqCtk637Wtna5D3q97WyramV+qbD7Ghuozt67M9vbihARWlBLNCLqAgXMrakgHBRLqWFOZQW5VJSkKMhGkk7JwtwTdqVgTGDYWO8BzNP/Nw5b7/PE4J9q/d65+vQurdPm0HvKr6gBPJLoKCUYQUlTMwvYWKB956zS+BC7/POvNHs7sinrjWXuuZuL9hjYb+87tgXp30NzQ8RLsqlpDCXcGEOpYWx10U5sedcSmLHe17nhfxZ0lfET3EFuJldBdwLBIEHnXP3+FKVpD8zb8ikMOztPNSfjlY4sP1YyDdv80L9SLMX/od2QeMG7337ift65gDlscclofyjoc/QElxZMR05wzhIEUdcLoddDoe6cznQHeJAV4j9HSH2dwbZtz/I3p1B6tsCNHWGaHO5HCGXI+TRTg7gDckU5QYpKcyltCiHwtwQhblBCnKCFOQGKcwNUpgboiCn53WQgp5zcoMU5sQ+z+39eZDcYEAzcCQuAw5wMwsCPwbeDzQAy83sOefcOr+KkwyXWwgjzvMepxLt9sbh25q9cD/S3M/r2Pu2FqxlO3lta8lra/E2yoh2vlPr3uaC/awK0BXIpzOQR4fl0x7Npe1ADu3k0OmCtLsgHS5IezRAW9R73UGILheknRCHCNJJiC6OHe8kRGfseNRCWDCHQDAEgSAWCGGBIIFAkEAwCMGc2OsQwaD3fPR9qOd47BEKEQoGCQZDBEI5BMy8880IBAMEAkGCgZ5jAQLBIMFAgGAwgAWChILB2PFjn/UcCwYDBAPeXqsGXptmmEEgYASMY+9jnwXMO7/3c+9zjGPtWewcOXPxXIFfBGxyzm0GMLMngGsBBbj4LxA8dkU/EN1d0HXEC/Ojj1bv+YTjsc+62gh1thLqPEJB78+indDd4bXZ3RF734br7sR1d+C6Onsd78SiXQSiHSevLRp7pLCoMxzgsNjDew39H+/9Ogp09fM5wLFvMHreHzvu8ALe9fmsh8PATjx+4rn9/LgTGO6Ew6dqt+dYz9l9+zle65Xf5YKZH+in74GLJ8DHAtt6vW8ALu57kpndCdwJUFlZGUd3InEIhiA4FPISt+OQ0d9f+RjnvP9F9A3/7g5w3d46Nq7bO6fnuffr455PPNdFu+nu7vIeXd1EXRQXjRKNdhN1Dtfdc8wRjXbHPoviXBQXPXa+i7qjx5xzx55dFOec9/PAHXvtHO7o++jR9zh33PlH3/ec2/NrEnv2XrlY6kWP+3VzOMxxtN2jH8WO95x/NEjdsc/7/X3g6Am9P+jnvBN+8Ena68+Jx4cVlZzk3IFL+JeYzrn5wHzwZqEkuj+RlGQW+0ckBDn+LydgeH+ZNSshu8Qzn2o7UNHrfXnsmIiIDIJ4Anw5cJ6ZjTOzXOAm4Dl/yhIRkVMZ8P+4nHNdZnYX8ALeNMKHnXNrfatMRETeUVxDZs6554HnfapFRETOgO4pFhFJUwpwEZE0pQAXEUlTCnARkTQ1qMvJmlkjUD/AHz4C2HvKs5Ir1WtM9fog9WtM9fpANfoh1eqrcs6V9T04qAEeDzOr7W893FSS6jWmen2Q+jWmen2gGv2Q6vX10BCKiEiaUoCLiKSpdArw+cku4DSkeo2pXh+kfo2pXh+oRj+ken1AGo2Bi4jI8dLpClxERHpRgIuIpKm0CHAzu8rM3jKzTWb21WTX05uZVZjZn81snZmtNbPPJ7umkzGzoJmtMrPfJruWvsysxMyeMrMNZrbezPrZ6j65zOyLsd/jNWa20MzyU6Cmh81sj5mt6XUsbGaLzGxj7Lk0xer7Tuz3+Q0ze8bMSpJVX6yeE2rs9dmXzMyZ2Yhk1HYqKR/gvTZP/gBwAXCzmV2Q3KqO0wV8yTl3ATAD+GyK1dfb54H1yS7iJO4F/uCcmwBMJsXqNLOxwOeAaufcRLwllG9KblUA/By4qs+xrwJ/cs6dB/wp9j5Zfs6J9S0CJjrnJgF/A7422EX18XNOrBEzqwCuBLYOdkGnK+UDnF6bJzvnOoCezZNTgnNup3NuZez1QbzgGZvcqk5kZuXAHODBZNfSl5kVA+8BHgJwznU455qTWlT/QkCBmYWAQmBHkuvBOfcy0NTn8LXAI7HXjwDXDWZNvfVXn3PuRedcV+ztErzdvJLmJL+GAN8HvkK/m2GmhnQI8P42T065gAQwswgwFVia5FL68wO8P4ypuP/5OKAR+FlsiOdBMytKdlG9Oee2A9/FuxrbCbQ4515MblUnNco5tzP2ehcwKpnFnMLHgd8nu4i+zOxaYLtz7vVk1/JO0iHA04KZDQF+DXzBOXcg2fX0ZmZXA3uccyuSXctJhIBpwAPOuanAYZL73/4TxMaRr8X7x2YMUGRmc5Nb1ak513sb9tRiZt/AG4JckOxaejOzQuDrwL8ku5ZTSYcAT/nNk80sBy+8Fzjnnk52Pf2YDXzIzOrwhqAuN7PHklvScRqABudcz/9cnsIL9FRyBbDFOdfonOsEngZmJbmmk9ltZmcBxJ73JLmeE5jZ7cDVwC0u9W5GOQfvH+rXY39nyoGVZjY6qVX1Ix0CPKU3TzYzwxu7Xe+c+16y6+mPc+5rzrly51wE79fvJedcylw9Oud2AdvMbHzs0PuAdUksqT9bgRlmVhj7PX8fKfZFay/PAbfFXt8GPJvEWk5gZlfhDed9yDnXmux6+nLOvemcG+mci8T+zjQA02J/TlNKygd47MuOns2T1wO/SrHNk2cD8/CualfHHh9MdlFp6G5ggZm9AUwB/i255Rwv9r+Dp4CVwJt4f3eSfru1mS0EFgPjzazBzO4A7gHeb2Yb8f7ncE+K1fcjYCiwKPb35SfJqu8dakwLupVeRCRNpfwVuIiI9E8BLiKSphTgIiJpSgEuIpKmFOAiImlKAS4ikqYU4CIiaer/A87LQLG4rUdqAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "inv_freq = 1 / (10000 ** (torch.arange(0.0, embedding_dim, 2.0) / embedding_dim))\n",
    "sinusoid_inp = torch.einsum(\"i,j->ij\", pos_idxs, inv_freq)\n",
    "plt.plot(sinusoid_inp[0, :].detach().numpy());\n",
    "plt.plot(sinusoid_inp[6, :].detach().numpy());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 1, 32])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_positional_embeddings = torch.cat([sinusoid_inp.sin(), sinusoid_inp.cos()], dim=-1)[:,None,:]\n",
    "relative_positional_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can gather this into its own class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        inv_freq = 1 / (10000 ** (torch.arange(0.0, d, 2.0) / d))\n",
    "        # register buffer tells pytorch that this tensor is part of the modle\n",
    "        # this means that it will be saved in the state_dict and moved to the GPU\n",
    "        # along with the model\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        \n",
    "    def forward(self, positions: torch.LongTensor, # (seq, )\n",
    "               ):\n",
    "        # outer product\n",
    "        sinusoid_inp = torch.einsum(\"i,j->ij\", positions.float(), self.inv_freq)\n",
    "        pos_emb = torch.cat([sinusoid_inp.sin(), sinusoid_inp.cos()], dim=-1)\n",
    "        return pos_emb[:,None,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also apply transformations to the positional embeddings separate from the values/keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_p = nn.Linear(embedding_dim, inner_dim)\n",
    "pos_tfmd = linear_p(relative_positional_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we'll be adding the positional bias during attention computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 13, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = torch.rand(17) # positional bias\n",
    "pos_attn = torch.einsum(\"ibd,jd->ijb\", q_tfmd + v, pos_tfmd[:,0,:]) / (embedding_dim ** 0.5) # scale\n",
    "pos_attn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we compute a relative postional embedding for each key-query pair, a naive implementation of attention using relative positional embeddings would be $O(n^2)$ in terms of computational complexity. Luckily, the authors proposed a trick to reduce this to $O(n)$ time by computing the attention for one query then shifting the embeddings for different query positions.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_pad = torch.zeros((seq, 1, batch_size), dtype=torch.float)\n",
    "# this padding + shifting efficiently computes the attention for all \n",
    "pos_attn = (torch.cat([zero_pad, pos_attn], dim=1)\n",
    "                    .view(seq + prev_seq + 1, seq, batch_size)[1:]\n",
    "                    .view_as(pos_attn)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attention is computed as the sum of content and positional attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_attn = content_attn + pos_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we do language modeling, we need to prevent the model from being able to look at the word that it is supposed to be predicting. In the Transformer, we achieve this by setting the attention score to zero. This masks out words that we don't want the model to be able to see. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-5082ce5f8402>:5: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:650.)\n",
      "  raw_attn = raw_attn.masked_fill(mask, -float('inf'))\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(\n",
    "    torch.ones((seq, seq + prev_seq)),\n",
    "    diagonal=1 + prev_seq,\n",
    ").byte()[...,None]\n",
    "raw_attn = raw_attn.masked_fill(mask, -float('inf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute the outputs as the weighted sum of the value vectors using the attention scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 3, 17])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn = torch.softmax(raw_attn, dim=1)\n",
    "attn_weighted_sum = torch.einsum(\"ijb,jbd->ibd\", attn, v_tfmd)\n",
    "attn_weighted_sum.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we project the attention weighted sums back to their original dimension and apply a residual connection and layer normalization. We apply layer normalization after the residual connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 3, 32])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_out = nn.Linear(inner_dim, embedding_dim)\n",
    "layer_norm = nn.LayerNorm(embedding_dim)\n",
    "output = layer_norm(word_embs + linear_out(attn_weighted_sum))\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiHeadAttention (MHA): The core component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregating all the above and applying a couple of optimizations by grouping some computations together as well as adding dropout, we get the following MultiHeadAttention module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_input: int, d_inner: int, n_heads: int=4, \n",
    "                 dropout: float=0.1, dropouta: float=0.):\n",
    "        super().__init__()\n",
    "        self.d_input = d_input\n",
    "        self.d_inner = d_inner\n",
    "        self.n_heads = n_heads\n",
    "        # this layer applies the linear transformation required\n",
    "        # for the keys and values for all heads at once for efficiency\n",
    "        self.linear_kv = nn.Linear(\n",
    "            d_input, \n",
    "            (d_inner * n_heads * 2), # 2 is for keys and values\n",
    "            bias=False, # we don't apply bias, making this a simple matrix multiplication\n",
    "        )\n",
    "        # for queries (will not be concatenated with memorized states so separate)\n",
    "        self.linear_q = nn.Linear(\n",
    "            d_input, d_inner * n_heads,\n",
    "            bias=False\n",
    "        )\n",
    "        # for positional embeddings\n",
    "        self.linear_p = nn.Linear(\n",
    "            d_input, d_inner * n_heads,\n",
    "            bias=False\n",
    "        )\n",
    "        self.scale = 1 / (d_inner ** 0.5) # for scaled dot product attention\n",
    "        self.dropa = nn.Dropout(dropouta)\n",
    "        # we will use this to project back to the input dimension\n",
    "        self.lout = nn.Linear(self.d_inner * self.n_heads, self.d_input, bias=False)\n",
    "        self.norm = nn.LayerNorm(self.d_input)\n",
    "        self.dropo = nn.Dropout(dropout)\n",
    "        \n",
    "    def _rel_shift(self, x):\n",
    "        zero_pad = torch.zeros((x.size(0), 1, *x.size()[2:]),\n",
    "                               device=x.device, dtype=x.dtype)\n",
    "        return (torch.cat([zero_pad, x], dim=1)\n",
    "                    .view(x.size(1) + 1, x.size(0), *x.size()[2:])[1:]\n",
    "                    .view_as(x)) \n",
    "        \n",
    "    def forward(self, input_: torch.FloatTensor, # (cur_seq, b, d_in)\n",
    "                pos_embs: torch.FloatTensor, # (cur_seq + prev_seq, d_in)\n",
    "                memory: torch.FloatTensor, # (prev_seq, b, d_in)\n",
    "                u: torch.FloatTensor, # (H, d)\n",
    "                v: torch.FloatTensor, # (H, d)\n",
    "                mask: Optional[torch.FloatTensor]=None,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        pos_embs: we pass the positional embeddings in separately\n",
    "            because we need to handle relative positions\n",
    "        input shape: (seq, bs, self.d_input)\n",
    "        pos_embs shape: (seq + prev_seq, bs, self.d_input)\n",
    "        output shape: (seq, bs, self.d_input)\n",
    "        \"\"\"\n",
    "        cur_seq = input_.shape[0] #  sequence length of current segment\n",
    "        prev_seq = memory.shape[0] # sequence length of previous segment\n",
    "        H, d = self.n_heads, self.d_inner\n",
    "        input_with_memory = torch.cat([memory, input_], dim=0) # concatenate recurrent memory\n",
    "                                                               # across sequence dimension\n",
    "\n",
    "        # we will use the following symbols to represent the shape of the tensors\n",
    "        # cs: current sequence length, b: batch, H: number of heads\n",
    "        # d: inner dimension, ps: previous sequence length\n",
    "        # The key and value are now conditioned on the preceding context\n",
    "        k_tfmd, v_tfmd = \\\n",
    "            torch.chunk(self.linear_kv(input_with_memory), 2, dim=-1) # (cs + ps, b, H * d)\n",
    "        q_tfmd = self.linear_q(input_) # (cs, b, H * d)\n",
    "\n",
    "        # apply scaled dot product attention\n",
    "        # look at the following dimensions carefully, since this is the key operation\n",
    "        # in the Transformer/Transformer XL architecture\n",
    "        \n",
    "        _, bs, _ = q_tfmd.shape\n",
    "        assert bs == k_tfmd.shape[1]\n",
    "        # content-based attention term ((a) + (c) in the paper)\n",
    "        # this is the standard attention term in the original Transformer, except without positional embeddings\n",
    "        # which are handled separately in the Transformer XL (see below)\n",
    "        # here, i corresponds to the number of queries = number of current inputs/targets (seq-wise)\n",
    "        # j corresponds to the number of key/values = number of vectors that we can use to compute the \n",
    "        # vector for each query\n",
    "        content_attn = torch.einsum(\"ibhd,jbhd->ijbh\", (\n",
    "                (q_tfmd.view(cur_seq, bs, H, d) + # (a)\n",
    "                 u), # (c): u represents the global (independent of the query)\n",
    "                     # bias towards certain key/values = words\n",
    "                     # Note: maybe this could be a per-attention head parameter?\n",
    "                 k_tfmd.view(cur_seq + prev_seq, bs, H, d) # There is no positional information to be found here\n",
    "        )) # (cs, cs + ps, b, H)\n",
    "        \n",
    "        # position-based attention term ((b) + (d) in the paper)\n",
    "        # this attention is solely based on the position of the key/values\n",
    "        # (i.e. it does not take the content of the key/values into account)\n",
    "        p_tfmd = self.linear_p(pos_embs) # (cs + ps, b, H * d)\n",
    "        position_attn = torch.einsum(\"ibhd,jhd->ijbh\", (\n",
    "                (q_tfmd.view(cur_seq, bs, H, d) + # (b)\n",
    "                 v), # (d): v represents the global (independent of the query)\n",
    "                     # bias towards certain positions\n",
    "                 p_tfmd.view(cur_seq + prev_seq, H, d) # Notice there is not content information\n",
    "                                                        # regarding keys and values here!\n",
    "        )) # (cs, cs + ps, b, H)\n",
    "        \n",
    "        #  Compute positional attention efficiently\n",
    "        position_attn = self._rel_shift(position_attn)\n",
    "        \n",
    "        # the attention is the sum of content-based and position-based attention\n",
    "        attn = content_attn + position_attn\n",
    "\n",
    "        if mask is not None and mask.any().item():\n",
    "            attn = attn.masked_fill(\n",
    "                mask[...,None], -float('inf'))\n",
    "        attn = torch.softmax(attn * self.scale, # rescale to prevent values from exploding\n",
    "                             dim=1) # normalize across the value sequence dimension\n",
    "        attn = self.dropa(attn)\n",
    "        \n",
    "        attn_weighted_values = (torch.einsum(\"ijbh,jbhd->ibhd\",\n",
    "                                           (attn, # (cs, cs + ps, b, H)\n",
    "                                            v_tfmd.view(cur_seq + prev_seq, bs, H, d), # (cs + ps, b, H, d)\n",
    "                                           )) # (cs, b, H, d)\n",
    "                                .contiguous() # we need to change the memory layout to make `view` work\n",
    "                                .view(cur_seq, bs, H * d)) # (cs, b, H * d)\n",
    "\n",
    "        # Project back to input dimension and add residual connection\n",
    "        output = input_ + self.dropo(self.lout(attn_weighted_values))\n",
    "        output = self.norm(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it out to see if it runs successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention(32, 17, n_heads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt = torch.rand(7, 3, 32)\n",
    "pos = torch.rand(13, 32)\n",
    "mem = torch.rand(6, 3, 32)\n",
    "u, v = torch.rand(4, 17), torch.rand(4, 17)\n",
    "x1 = mha(inpt, pos, mem, u, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 3, 32])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3000, -1.3250,  0.7712, -1.5431, -2.0485,  0.0936,  0.0727, -0.9565,\n",
       "          0.1633, -0.1554,  0.3978,  1.2058, -0.2332,  0.1875, -1.4309,  0.7164,\n",
       "         -0.8837, -0.0584,  0.3661,  2.8744, -0.3735,  0.9966,  0.0616, -0.2500,\n",
       "         -0.3935, -0.2943, -0.0425, -0.0897,  2.0972, -0.7547,  0.2267, -0.6981],\n",
       "        [ 1.1258, -0.4527,  1.2436,  1.0746, -1.8253,  1.1411,  0.7147, -1.0583,\n",
       "         -0.1546,  0.4542, -0.7722,  0.0070, -1.0700, -0.2633, -1.3950,  1.1799,\n",
       "         -1.7211,  0.3068,  0.1408,  2.0054, -1.0284,  1.1384, -0.2103,  0.1778,\n",
       "         -0.6022,  0.5553,  0.6372, -0.0504,  1.5638, -1.3859, -0.9795, -0.4976],\n",
       "        [-1.0483,  0.6669, -0.9976, -0.9539, -0.2302, -0.0468, -0.3742, -0.3372,\n",
       "          1.8253, -0.5124, -0.2593,  0.2766, -0.7837, -1.4711,  0.4473, -1.0095,\n",
       "         -0.3196,  0.7523, -1.5405,  2.5527, -1.5389,  0.5726, -0.0483,  1.2332,\n",
       "          0.7084, -1.5856,  1.0134,  1.3779,  0.5555,  0.2900,  0.1859,  0.5991]],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct the decoder block, all we need in addition to the MultiHeadAttention layer is the Positionwise Feed Forward layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://i2.wp.com/mlexplained.com/wp-content/uploads/2017/12/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88-2017-12-29-19.14.41.png?w=273)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFF(nn.Module):\n",
    "    def __init__(self, d_input, d_inner, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_input = d_input\n",
    "        self.d_inner = d_inner\n",
    "        self.dropout = dropout\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_input, d_inner), nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_inner, d_input),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(d_input)\n",
    "\n",
    "    def forward(self, input_: torch.FloatTensor, # (cur_seq, bs, d_input)\n",
    "               ) -> torch.FloatTensor: # (cur_seq, bs, d_input)\n",
    "        ff_out = self.ff(input_)\n",
    "        output = self.layer_norm(input_ + ff_out)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can implement the decoder block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, n_heads, d_input, \n",
    "                 d_head_inner, d_ff_inner,\n",
    "                 dropout, dropouta=0.):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(d_input, d_head_inner, n_heads=n_heads, \n",
    "                                      dropout=dropout, dropouta=dropouta)\n",
    "        self.ff = PositionwiseFF(d_input, d_ff_inner, dropout)\n",
    "            \n",
    "    def forward(self, input_: torch.FloatTensor, # (cur_seq, bs, d_input)\n",
    "                pos_embs: torch.FloatTensor, # (cur_seq + prev_seq, d_input),\n",
    "                u: torch.FloatTensor, # (H, d_input), \n",
    "                v: torch.FloatTensor, # (H, d_input),\n",
    "                mask=None,\n",
    "                mems=None,\n",
    "               ):\n",
    "        return self.ff(self.mha(input_, pos_embs, mems, u, v, mask=mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The full Transformer XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with these components in place, we can build the full Transformer XL model.\n",
    "\n",
    "Aside from what we mentioned above, one common trick in language modeling that we haven't covered yet is tying the input embedding matrix $ E $ and output projection matrix $ P $. Remember, a language model predicts the next token in a sequence, so its output dimension is $\\mathbb{R}^{|V|}$  where $|V|$ is the vocab size. If we constrain the penultimate layer output to be the same dimension as the embeddings $d$, the embedding matrix $ E $ will be of shape $ \\mathbb{R}^{|V| \\times d}$ and the output projection matrix $ P $ will be of shape $ \\mathbb{R}^{d \\times |V|} $.\n",
    "\n",
    "In [this paper](https://arxiv.org/abs/1608.05859), the authors found that constraining the matrices such that $ P = E^T $ improved performance while greatly reducing the total parameter count (and thus memory usage!) of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of simply using the exact same weights, we scale the embeddings by the embedding dim. This trick is included in the codebase but not mentioned in the paper as far as I can tell. If you're aware of a paper where this trick was originally introduced, please let me know!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardWordEmbedding(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim,\n",
    "                div_val=1, sample_softmax=False):\n",
    "        super().__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.scale = embedding_dim ** 0.5\n",
    "\n",
    "    def forward(self, input_: torch.LongTensor):\n",
    "        return self.embedding(input_) * self.scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all we need to do is to put everything we have implemented above together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerXL(nn.Module):\n",
    "    def __init__(self, num_embeddings, n_layers, n_heads, \n",
    "                 d_model, d_head_inner, d_ff_inner,\n",
    "                 dropout=0.1, dropouta=0., \n",
    "                 seq_len: int=0, mem_len: int=0):\n",
    "        super().__init__()\n",
    "        self.n_layers,self.n_heads,self.d_model,self.d_head_inner,self.d_ff_inner = \\\n",
    "            n_layers,n_heads,d_model,d_head_inner,d_ff_inner\n",
    "        # Embedding layers\n",
    "        self.word_embs = StandardWordEmbedding(num_embeddings, d_model)\n",
    "        self.pos_embs = PositionalEmbedding(d_model)\n",
    "        # Core transformer\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.layers = nn.ModuleList([DecoderBlock(n_heads, d_model, d_head_inner=d_head_inner,\n",
    "                                                  d_ff_inner=d_ff_inner,\n",
    "                                                  dropout=dropout, dropouta=dropouta)\n",
    "                                     for _ in range(n_layers)])\n",
    "\n",
    "        # tie weights\n",
    "        self.output_projection = nn.Linear(d_model, num_embeddings)\n",
    "        self.output_projection.weight = self.word_embs.embedding.weight\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.seq_len, self.mem_len = seq_len, mem_len\n",
    "        \n",
    "        # u and v are global parameters: maybe changing these to per-head parameters\n",
    "        # might help performance?\n",
    "        self.u, self.v = (nn.Parameter(torch.Tensor(self.n_heads, self.d_head_inner)),\n",
    "                          nn.Parameter(torch.Tensor(self.n_heads, self.d_head_inner)))\n",
    "        \n",
    "    def init_memory(self, device=torch.device(\"cpu\")) -> torch.FloatTensor:\n",
    "        return [torch.empty(0, dtype=torch.float).to(device) for _ in range(self.n_layers+1)]\n",
    "    \n",
    "    def update_memory(self, \n",
    "            previous_memory: List[torch.FloatTensor], \n",
    "            hidden_states: List[torch.FloatTensor],\n",
    "        ):\n",
    "        assert len(hidden_states) == len(previous_memory)\n",
    "        mem_len, seq_len = previous_memory[0].size(0), hidden_states[0].size(0)\n",
    "\n",
    "        # For the updated memory, we use the most recent `self.mem_len`\n",
    "        # states, including the previous memory\n",
    "        # In other words, if `seq_len` < `self.mem_len` some of the previous memory\n",
    "        # will carry over to the next memory\n",
    "        with torch.no_grad():\n",
    "            new_memory = []\n",
    "            end_idx = mem_len + seq_len\n",
    "            beg_idx = max(0, end_idx - self.mem_len)\n",
    "            for m, h in zip(previous_memory, hidden_states):\n",
    "                cat = torch.cat([m, h], dim=0) # (mem_len + seq_len, bs, d)\n",
    "                new_memory.append(cat[beg_idx:end_idx].detach()) # (self.mem_len, bs, d)\n",
    "        return new_memory\n",
    "    \n",
    "    def reset_length(self, seq_len, ext_len, mem_len):\n",
    "        self.seq_len = seq_len\n",
    "        self.mem_len = mem_len\n",
    "    \n",
    "    def forward(self, idxs: torch.LongTensor, # (cs, bs)\n",
    "                target: torch.LongTensor, # (cs, bs)\n",
    "                memory: Optional[List[torch.FloatTensor]]=None,\n",
    "               ) -> Dict[str, torch.Tensor]:\n",
    "        if memory is None: \n",
    "            memory: List[torch.FloatTensor] = self.init_memory(idxs.device)\n",
    "        assert len(memory) == len(self.layers) + 1\n",
    "        cur_seq, bs = idxs.size()\n",
    "        prev_seq = memory[0].size(0)\n",
    "        \n",
    "        # Construct attention mask\n",
    "        dec_attn_mask = torch.triu(\n",
    "            torch.ones((cur_seq, cur_seq + prev_seq)),\n",
    "            diagonal=1 + prev_seq,\n",
    "        ).byte()[...,None].to(idxs.device)\n",
    "        \n",
    "        word_embs = self.drop(self.word_embs(idxs))\n",
    "        pos_idxs = torch.arange(cur_seq + prev_seq - 1, -1, -1.0, dtype=torch.float).to(word_embs.device)\n",
    "        pos_embs = self.drop(self.pos_embs(pos_idxs))\n",
    "        \n",
    "        # Main part of forward pass\n",
    "        hidden_states = [word_embs]\n",
    "        layer_out = word_embs\n",
    "        for mem, layer in zip(memory, self.layers):\n",
    "            layer_out = layer(layer_out, pos_embs, self.u, self.v, \n",
    "                              mask=dec_attn_mask, mems=mem)\n",
    "            hidden_states.append(layer_out)\n",
    "        \n",
    "        logits = self.output_projection(self.drop(layer_out))        \n",
    "        loss = self.loss_fn(logits.view(-1, logits.size(-1)), target.view(-1))\n",
    "        \n",
    "        # Update memory \n",
    "        # Ensure the memory is treated as a constant\n",
    "        # and we do not back propagate through them\n",
    "        new_memory = self.update_memory(memory, hidden_states)\n",
    "        return {\"loss\": loss, \"logits\": logits, \"memory\": new_memory}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TransformerXL(1000, 4, 3, 32, 17, 71, mem_len=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's feed some random inputs to confirm the model is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-78437edfdb27>:108: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:650.)\n",
      "  attn = attn.masked_fill(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': tensor(21.7364, grad_fn=<NllLossBackward>),\n",
       " 'logits': tensor([[[ -9.8747,  -1.6242,  -5.6594,  ...,   6.7810,  -4.4763,  -2.9676],\n",
       "          [ -4.4185,  -6.9766,  -6.2549,  ...,  -5.1306,  -5.1970,   1.4067],\n",
       "          [  3.5920,   5.5952,  -2.9383,  ...,  -9.8789,   3.0823,  -9.2266],\n",
       "          ...,\n",
       "          [ -6.2648, -10.4111,   2.3043,  ...,  -4.8364,   8.1018,  -4.2047],\n",
       "          [  7.2377,   1.4696,  -7.4642,  ...,  -5.6777,  12.5175, -10.0870],\n",
       "          [ -3.7430,   8.0056,   1.7758,  ...,   7.7770,   8.3731,   1.0725]],\n",
       " \n",
       "         [[  8.0647,   4.6946,   2.2836,  ...,   0.6863,  -1.8183,   3.0069],\n",
       "          [ -0.8347,  -4.2175,   8.7519,  ...,  -5.1518,   1.6615,  10.8120],\n",
       "          [ -0.8082,   8.0915,  -1.1121,  ...,  -2.1750,  11.3080, -16.3273],\n",
       "          ...,\n",
       "          [  5.2520, -11.7531,   6.0886,  ...,  -1.7315,  -0.1367,  -5.2649],\n",
       "          [ -5.2336,  -5.4635,   3.8159,  ..., -10.9708,   5.3890,  -0.3163],\n",
       "          [ -7.9101,  -1.1302,   1.0264,  ...,  -0.8357,  -5.6363,  15.4831]],\n",
       " \n",
       "         [[  4.7055,  -2.3658,  -8.8561,  ...,  -2.7900,   0.5246,  -4.1107],\n",
       "          [-12.2586,  -2.0488,  -6.3985,  ...,  -0.0648,  11.8137,  -1.9951],\n",
       "          [  0.4953,   0.0297,   3.7353,  ...,   1.6619,  -4.0050,   3.9570],\n",
       "          ...,\n",
       "          [ 14.3989,  -7.3235,  -1.9062,  ...,  -4.8618,  -0.5951,  -8.6956],\n",
       "          [  1.7854,  -2.7046,   2.2020,  ...,   2.8616,   3.4933,   1.0788],\n",
       "          [ -3.8049,  -1.1037,   1.6085,  ...,  -0.2471,   3.6418,  -0.2868]],\n",
       " \n",
       "         [[ -7.3950,   3.2395,  -4.1818,  ...,   3.7094,   0.5148,   5.6912],\n",
       "          [  0.4036,   0.6839,  -2.3887,  ...,   3.5228,  -2.2965,   4.2361],\n",
       "          [  1.1486,   2.4100, -10.6986,  ...,   2.2198,   6.4202,  -6.2732],\n",
       "          ...,\n",
       "          [  1.2885,  -0.9278,  -6.2265,  ..., -10.0118,   2.4804,  -6.3371],\n",
       "          [  4.6825,  -1.7708,  -4.9262,  ...,   4.0813,   3.4173,   2.0261],\n",
       "          [ -6.4158,  -5.7411,   4.0840,  ...,   1.2654,  -1.7726,  -2.1234]],\n",
       " \n",
       "         [[ -8.3928,   5.1028, -13.5679,  ...,   3.8398,   3.7026,   2.0931],\n",
       "          [ -5.3058,  -0.7187,  -0.7424,  ...,  -4.2376,  -0.1115,   0.9930],\n",
       "          [  2.6606,   7.9387, -16.1182,  ...,   3.6143,  -0.3141,  -7.8883],\n",
       "          ...,\n",
       "          [ -5.3441,  10.5750,  -2.9884,  ...,   0.8141,   6.7642,  -7.2618],\n",
       "          [  0.3471,   1.5172,  -5.6293,  ...,   2.1969,   5.6291,  -0.6534],\n",
       "          [ -0.1965,   5.0804,  -0.2557,  ...,   3.8227,  -4.7720,   1.0638]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " 'memory': [tensor([[[ -0.1324,  -3.4400,   4.2730,  ...,   3.1665,   4.0990,  -0.5421],\n",
       "           [ -5.8645,   4.3412,   3.7775,  ...,  -2.6642,  -0.9660,  -0.6118],\n",
       "           [  2.9270,   7.6475,   5.1538,  ...,   1.0678,   2.4962,  -6.9608],\n",
       "           ...,\n",
       "           [  0.0000,   2.2339,   5.1609,  ..., -12.0297,   1.9670,  -0.0000],\n",
       "           [ -0.0000,  -2.4664,   1.0648,  ...,  -5.0341,   3.9996, -10.5282],\n",
       "           [ -1.3806, -14.9792,  -5.0718,  ...,   0.5604,  -6.0117,   4.0025]],\n",
       "  \n",
       "          [[  5.4476,  -0.0000,  -7.0609,  ...,  -5.9037,   4.0832,   1.7755],\n",
       "           [ -5.5060,   0.3230,  -7.3612,  ...,  -1.4924,  -2.1940,   8.5145],\n",
       "           [ -0.0000,  -4.2562,   5.2767,  ...,  -6.4418,   7.2465,  -0.0979],\n",
       "           ...,\n",
       "           [  5.0522,  -2.2758,  -5.2514,  ...,   7.0532,   0.0000,   2.7400],\n",
       "           [ -0.0000,   7.4199, -11.2409,  ...,  -3.2419,   1.4770,  -9.3149],\n",
       "           [ -9.6997,  -4.8881,  -2.9278,  ...,   2.7698,  -4.6694,   0.0000]],\n",
       "  \n",
       "          [[  5.2016,   0.8620, -10.1837,  ...,   9.5934,   3.3128,  -2.8735],\n",
       "           [ 12.5437,  -6.4162,   4.1266,  ...,  -6.6651,   0.0000,  -5.1245],\n",
       "           [  8.3213,  -1.7226,   2.1603,  ...,   6.0111,  -0.8933,   9.8331],\n",
       "           ...,\n",
       "           [  6.5986,  -0.1800,   1.9059,  ...,   0.0000,  -0.3754,   7.0251],\n",
       "           [  1.8542,   3.3391,  12.2503,  ...,   0.2867,  -8.3986, -10.5883],\n",
       "           [  7.1353,  -8.4633,  -4.7444,  ...,   1.4663,  -2.8206,   4.3273]],\n",
       "  \n",
       "          [[ -3.1938,   5.1688,  -5.6119,  ...,  -3.7123,   0.5548,  11.1730],\n",
       "           [ -3.9830,   7.2829,   3.8665,  ...,   4.5229,   0.6729,   2.6308],\n",
       "           [ -1.6832, -15.3308,   4.2940,  ...,  -4.2814,   5.6054,  -2.7819],\n",
       "           ...,\n",
       "           [ -4.1085,   1.1635,   4.9425,  ...,   0.0000,   3.7867,  -5.6842],\n",
       "           [  3.3837,  -2.0566,   6.0913,  ...,  -2.7472,  -3.6713, -11.6737],\n",
       "           [  7.2658, -11.6394,   1.0401,  ...,  -1.1728,  -9.0947,  -5.6328]],\n",
       "  \n",
       "          [[ -1.2756,  -5.3791,  13.2355,  ...,   2.3028, -11.0544,   0.6984],\n",
       "           [ -6.7714,   1.6617,  -7.0273,  ...,   5.0576,  -5.8314,  -5.0287],\n",
       "           [  3.1894,  -3.1071,  12.1969,  ...,  -1.2228, -13.2576,  -6.1112],\n",
       "           ...,\n",
       "           [  0.1412,  -5.9752,  16.5444,  ...,  -0.0000,  -0.8522,  -8.1808],\n",
       "           [ -2.3615,   7.7154,   3.6832,  ...,  -0.2290,   0.0000,  -4.3403],\n",
       "           [ -0.2448, -12.0799,   3.3235,  ...,   0.9744,  -5.4740,   5.8186]]]),\n",
       "  tensor([[[ 0.1599, -0.4044,  1.0927,  ...,  0.9688,  0.4002,  0.0194],\n",
       "           [-1.1148,  1.3663,  0.9729,  ..., -0.0109, -0.2443,  0.0307],\n",
       "           [ 0.4028,  1.3603,  0.7654,  ...,  0.1080,  0.5235, -1.2540],\n",
       "           ...,\n",
       "           [ 0.6096,  0.6917,  1.2759,  ..., -1.7133,  0.4703,  0.2948],\n",
       "           [ 0.6646, -0.3424,  0.6157,  ..., -0.4809,  0.7471, -1.7362],\n",
       "           [ 0.3496, -2.3910, -0.2358,  ...,  0.6526, -0.4951,  0.8732]],\n",
       "  \n",
       "          [[ 1.1937,  0.0119, -1.3557,  ..., -0.5661,  0.9249,  0.5516],\n",
       "           [-0.8469, -0.3032, -1.5678,  ..., -0.0248, -0.3446,  1.5182],\n",
       "           [ 0.2485, -1.4091,  0.0458,  ..., -0.9966,  1.0682, -0.9117],\n",
       "           ...,\n",
       "           [ 1.0722, -0.4209, -0.4630,  ...,  1.7223,  0.7487,  0.6944],\n",
       "           [ 0.3867,  0.9771, -1.0679,  ..., -0.2548,  0.3497, -1.0605],\n",
       "           [-0.6941, -0.5711,  0.3822,  ...,  0.7915, -0.4513,  0.9406]],\n",
       "  \n",
       "          [[ 0.8553,  0.0936, -1.2880,  ...,  1.5608, -0.0934, -0.0479],\n",
       "           [ 2.3641, -1.5007,  0.8190,  ..., -0.6219, -0.0726, -0.2196],\n",
       "           [ 1.6141, -0.3200,  0.1239,  ...,  0.8699, -0.2296,  1.7729],\n",
       "           ...,\n",
       "           [ 1.0119,  0.1376,  0.5827,  ...,  0.0858,  0.7488,  1.3046],\n",
       "           [ 0.4411,  0.2277,  2.3446,  ...,  0.2490, -1.1184, -0.8715],\n",
       "           [ 1.7017, -2.1531, -0.9735,  ...,  0.0485, -0.7953,  0.2813]],\n",
       "  \n",
       "          [[-0.4506,  0.1764, -0.9329,  ..., -0.2568,  0.4973,  1.7618],\n",
       "           [-0.5718,  0.8548,  1.0269,  ...,  1.3742,  0.5199,  0.5990],\n",
       "           [-0.1131, -2.6075,  1.3269,  ..., -0.4091,  1.4131, -0.4757],\n",
       "           ...,\n",
       "           [-0.2089,  0.5123,  0.8162,  ...,  0.0986,  0.8294, -0.4676],\n",
       "           [ 0.8476, -0.5030,  1.8130,  ..., -0.1332, -0.5952, -1.6090],\n",
       "           [ 1.7042, -1.7201,  0.3214,  ..., -0.4244, -1.1859, -0.5233]],\n",
       "  \n",
       "          [[-0.4190, -1.3185,  2.2144,  ...,  0.4336, -2.3868,  0.0058],\n",
       "           [-1.0120,  0.3295, -1.4364,  ...,  1.1067, -1.6933, -0.6450],\n",
       "           [-0.0153,  0.0702,  1.7643,  ..., -0.1105, -1.8541, -0.9200],\n",
       "           ...,\n",
       "           [ 0.0269, -1.3836,  3.7938,  ..., -0.4851,  0.0047, -1.2547],\n",
       "           [ 0.0325,  1.2967,  0.8687,  ...,  0.7368, -0.2371,  0.2903],\n",
       "           [ 0.1389, -1.7016,  0.5785,  ...,  0.5029, -1.0800,  1.1645]]]),\n",
       "  tensor([[[-0.2169,  0.1988,  0.9589,  ...,  1.0911, -0.0857, -0.1376],\n",
       "           [-0.4876,  1.5744,  0.9652,  ...,  0.1982,  0.4056,  1.1999],\n",
       "           [ 0.2115,  1.4155,  0.3071,  ...,  0.4930,  0.3807, -0.8234],\n",
       "           ...,\n",
       "           [ 1.1498,  0.7050,  1.4497,  ..., -1.8980,  0.5536,  0.0649],\n",
       "           [ 0.7990, -0.3442,  0.9090,  ..., -0.5173,  0.5294, -1.7086],\n",
       "           [ 0.4772, -2.8424, -0.3842,  ...,  1.0666, -0.6939,  0.3435]],\n",
       "  \n",
       "          [[ 1.2822, -0.1356, -0.9063,  ..., -0.1152,  0.6077,  0.4941],\n",
       "           [ 0.2007, -0.5696, -1.1812,  ..., -0.0563,  0.2394,  2.3768],\n",
       "           [-0.5577, -0.9986, -0.0861,  ..., -0.6795,  1.5471, -0.3994],\n",
       "           ...,\n",
       "           [ 1.3779, -0.2064, -0.1432,  ...,  1.7496,  1.2185,  0.4814],\n",
       "           [ 0.7613,  0.4685, -0.7022,  ..., -0.1985,  0.5093, -1.2671],\n",
       "           [ 0.0643, -0.6256,  0.2122,  ...,  1.1877, -0.4935,  0.5483]],\n",
       "  \n",
       "          [[ 0.7253,  0.4063, -0.7894,  ...,  2.4186, -0.2739, -0.1651],\n",
       "           [ 2.5342, -1.1409,  1.0667,  ..., -0.4877, -0.1405, -0.3503],\n",
       "           [ 1.5002, -0.4650, -0.2301,  ...,  1.3161, -0.1173,  1.5413],\n",
       "           ...,\n",
       "           [ 1.3141,  0.1081,  0.8655,  ...,  0.0961,  1.2724,  1.3139],\n",
       "           [ 0.7045,  0.4111,  2.8688,  ...,  0.4405, -1.0094, -1.2496],\n",
       "           [ 1.6815, -1.9600, -0.9810,  ...,  0.1434, -0.9325, -0.0306]],\n",
       "  \n",
       "          [[-0.1753,  0.0505, -0.5273,  ..., -0.2727,  0.7614,  1.5176],\n",
       "           [-0.1387,  0.6695,  1.1611,  ...,  1.4989,  0.5825,  1.0931],\n",
       "           [-0.3199, -2.6319,  1.2386,  ..., -0.2008,  1.2786, -0.3888],\n",
       "           ...,\n",
       "           [-0.1549,  0.7958,  0.9172,  ...,  0.4416,  1.1668,  0.0675],\n",
       "           [ 0.8692, -0.3865,  2.1559,  ..., -0.2487, -0.7088, -1.8437],\n",
       "           [ 1.8458, -1.5240,  0.4167,  ..., -0.2821, -1.3721, -0.9306]],\n",
       "  \n",
       "          [[-0.3401, -1.0649,  2.4534,  ..., -0.0636, -2.1477, -0.2208],\n",
       "           [-0.6197,  0.1401, -0.8874,  ...,  1.3644, -0.9479, -0.1633],\n",
       "           [-0.6014,  0.1623,  1.6038,  ..., -0.3156, -1.3948, -0.6458],\n",
       "           ...,\n",
       "           [-0.1166, -1.1730,  3.9501,  ..., -0.3855,  0.0728, -1.2263],\n",
       "           [ 0.4650,  1.1303,  0.8484,  ...,  1.0282, -0.1200,  0.1532],\n",
       "           [ 0.3600, -1.7251,  0.8164,  ...,  0.6203, -1.2321,  1.0199]]]),\n",
       "  tensor([[[-0.0386,  0.4913,  0.5665,  ...,  0.7458, -0.1557,  0.5901],\n",
       "           [-0.8279,  2.0860,  1.3002,  ..., -0.1589,  0.3744,  2.3824],\n",
       "           [ 0.6192,  1.6960,  0.4188,  ...,  0.3631,  0.6137, -0.3062],\n",
       "           ...,\n",
       "           [ 1.4788,  1.2956,  1.6665,  ..., -1.6054,  0.1080,  0.8607],\n",
       "           [ 1.1666, -0.1449,  1.1204,  ..., -0.1063,  0.1000, -1.6903],\n",
       "           [ 0.4799, -2.6569, -0.2612,  ...,  1.2042, -0.4354,  0.0729]],\n",
       "  \n",
       "          [[ 1.2693,  0.6256, -0.7995,  ..., -0.2176,  0.3526,  0.7710],\n",
       "           [ 0.3552, -0.4751, -0.8258,  ..., -0.0971,  0.2195,  2.5734],\n",
       "           [-0.1813, -0.8375,  0.4050,  ..., -1.1903,  1.1251,  0.4161],\n",
       "           ...,\n",
       "           [ 1.8174,  0.5223,  0.2370,  ...,  2.0108,  0.9839,  0.9286],\n",
       "           [ 1.0115,  0.9353, -0.5081,  ...,  0.3920,  0.4256, -0.6978],\n",
       "           [-0.3469, -0.7907,  0.0937,  ...,  1.2690, -0.5320,  0.7889]],\n",
       "  \n",
       "          [[ 0.8367,  0.7732, -0.3120,  ...,  2.1814, -0.6955,  0.4645],\n",
       "           [ 2.7616, -0.8369,  1.2797,  ..., -0.3765,  0.0162, -0.2032],\n",
       "           [ 1.8149,  0.0957,  0.3635,  ...,  1.3036, -0.3192,  1.4570],\n",
       "           ...,\n",
       "           [ 1.4336,  1.3048,  1.3995,  ...,  0.1554,  0.6427,  1.6803],\n",
       "           [ 1.4681,  0.6321,  2.4005,  ...,  0.9013, -0.9519, -0.7108],\n",
       "           [ 1.4356, -1.6554, -0.7645,  ..., -0.1416, -0.9323,  0.0395]],\n",
       "  \n",
       "          [[-0.4305,  0.6428, -0.2591,  ..., -0.7234,  0.5774,  1.9017],\n",
       "           [-0.1955,  0.7428,  1.2674,  ...,  1.5695,  0.9080,  1.5656],\n",
       "           [-0.1688, -2.3864,  1.6052,  ..., -0.2434,  1.1462, -0.3455],\n",
       "           ...,\n",
       "           [ 0.1885,  0.6479,  1.0175,  ...,  0.3748,  0.9394,  0.4494],\n",
       "           [ 1.3579,  0.3578,  2.1922,  ...,  0.1823, -0.4616, -1.2075],\n",
       "           [ 1.9163, -1.1013,  0.3260,  ...,  0.0597, -1.3865, -1.1197]],\n",
       "  \n",
       "          [[-0.3968, -0.6181,  2.4818,  ..., -0.1578, -2.1290,  0.3907],\n",
       "           [-1.2435,  0.1058, -0.4725,  ...,  0.8125, -0.5813,  0.0214],\n",
       "           [-0.5611,  0.6763,  1.7926,  ..., -0.6713, -1.3816, -0.4201],\n",
       "           ...,\n",
       "           [ 0.0376, -1.0081,  3.7284,  ..., -0.3027,  0.2363, -1.3832],\n",
       "           [ 0.7271,  1.5261,  0.8390,  ...,  1.1679, -0.3128,  0.4451],\n",
       "           [ 0.5456, -1.2737,  0.8186,  ...,  0.6595, -1.3308,  1.3180]]]),\n",
       "  tensor([[[-0.8387,  0.0388,  0.4437,  ..., -0.1701, -0.6412,  0.5872],\n",
       "           [-0.9783,  2.0124,  1.0399,  ..., -0.4791,  0.1393,  2.6529],\n",
       "           [ 0.5781,  2.4194,  0.4868,  ...,  0.8815,  0.6425, -0.2582],\n",
       "           ...,\n",
       "           [ 1.6519,  0.7345,  0.8641,  ..., -1.4413, -0.2153,  0.4138],\n",
       "           [ 0.6295, -0.8196,  0.6925,  ...,  0.6176,  0.4106, -1.9725],\n",
       "           [-0.0983, -2.0252, -0.4812,  ...,  0.9708,  0.5329,  0.5590]],\n",
       "  \n",
       "          [[ 0.8188,  0.0972, -0.9309,  ..., -0.2523,  0.7139,  0.6341],\n",
       "           [ 0.2442, -0.1806, -0.7373,  ...,  0.1928,  0.6376,  2.4791],\n",
       "           [-0.5186, -0.1729,  0.3578,  ..., -1.4221,  1.2839,  0.6172],\n",
       "           ...,\n",
       "           [ 2.0536, -0.4586, -0.1536,  ...,  1.8473,  1.3208,  0.5988],\n",
       "           [ 0.7336,  0.7986, -0.8164,  ...,  0.8601,  0.0538, -0.9786],\n",
       "           [-0.7296, -0.5276,  0.4555,  ...,  0.9357, -0.4092,  0.9379]],\n",
       "  \n",
       "          [[ 0.2899,  0.3921, -0.9639,  ...,  1.7043, -0.6545,  0.5701],\n",
       "           [ 2.1103, -0.3820,  1.4260,  ..., -0.1306,  0.1594, -0.2578],\n",
       "           [ 1.5587, -0.1515,  0.4922,  ...,  1.4512,  0.1060,  1.7314],\n",
       "           ...,\n",
       "           [ 1.4499,  1.0728,  0.8707,  ...,  0.4135,  0.9234,  1.5421],\n",
       "           [ 1.7177, -0.4200,  2.0735,  ...,  1.5193, -0.3772, -1.0707],\n",
       "           [ 0.6159, -1.9184, -0.6783,  ..., -0.4371, -0.3145, -0.0992]],\n",
       "  \n",
       "          [[-0.9662,  0.5912, -0.4796,  ..., -0.5963,  0.5355,  2.2867],\n",
       "           [-0.7907,  0.7303,  1.4085,  ...,  1.4315,  1.2225,  1.4799],\n",
       "           [-0.5343, -1.9408,  1.8263,  ...,  0.1154,  1.5199, -0.5420],\n",
       "           ...,\n",
       "           [-0.0288,  0.1927,  0.7014,  ...,  0.4372,  1.0695, -0.0143],\n",
       "           [ 1.4514, -0.1818,  1.7944,  ...,  0.6985, -0.4315, -1.6318],\n",
       "           [ 1.3589, -1.1726,  0.3636,  ...,  0.2308, -0.8782, -1.0522]],\n",
       "  \n",
       "          [[-1.0494, -0.4984,  2.3949,  ..., -0.1495, -1.7034,  0.8409],\n",
       "           [-1.3094,  0.2956, -0.4878,  ...,  0.6527,  0.2230,  0.2483],\n",
       "           [-1.1013,  0.9472,  1.9026,  ..., -0.2214, -0.7736, -0.5690],\n",
       "           ...,\n",
       "           [-0.4500, -1.3989,  3.3160,  ..., -0.1047,  0.0735, -1.6474],\n",
       "           [ 0.7885,  0.8186,  0.6102,  ...,  2.0517,  0.1943,  0.0063],\n",
       "           [-0.2534, -1.5736,  1.0719,  ...,  0.3864, -1.2217,  1.4652]]])]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs = torch.randint(1000, (5, 9))\n",
    "tgts = torch.randint(1000, (5, 9))\n",
    "transformer(idxs, tgts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Transformer XL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's move on to actually training the Transformer XL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTING = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using the following configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "    \n",
    "    def set(self, key, val):\n",
    "        self[key] = val\n",
    "        setattr(self, key, val)\n",
    "        \n",
    "    def update(self, dct):\n",
    "        for k, v in dct.items():\n",
    "            self.set(k, v)\n",
    "\n",
    "# We will use prime numbers to ensure our implementation\n",
    "# is actually correct\n",
    "config = Config(\n",
    "    seed=101,\n",
    "    debug=False,\n",
    "    warmup_step=0,\n",
    "    # Check default params\n",
    "    min_lr=0., \n",
    "    dropouta=0.,\n",
    "    clip=0.25,\n",
    "    log_interval=200,\n",
    "    eval_interval=100,\n",
    ")\n",
    "\n",
    "if TESTING:\n",
    "    config.update(dict(\n",
    "        debug=True,\n",
    "        lr=0.00025,\n",
    "        bs=8,\n",
    "        epochs=2,\n",
    "        max_step=10000, # shorten for testing\n",
    "        n_layers=4,\n",
    "        n_heads=3,\n",
    "        d_model=32,\n",
    "        d_head_inner=17,\n",
    "        d_ff_inner=71,\n",
    "        dropout=0.1,\n",
    "        train_bptt=33,\n",
    "        eval_bptt=41,\n",
    "        mem_len=41,\n",
    "        eval_mem_len=63,\n",
    "    ))\n",
    "else:\n",
    "    config.update(dict(\n",
    "        lr=0.0025,\n",
    "        bs=22,\n",
    "        epochs=2,\n",
    "        max_step=400000,\n",
    "        n_layers=12,\n",
    "        n_heads=8,\n",
    "        d_model=512,\n",
    "        d_head_inner=64,\n",
    "        d_ff_inner=2048,\n",
    "        dropout=0.1,\n",
    "        train_bptt=512,\n",
    "        eval_bptt=128,\n",
    "        mem_len=512,\n",
    "        eval_mem_len=2100,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loading for the Transformer XL is similar to data loading for an RNN-based language model but is quite different from standard data loading, so we'll go over it in detail.\n",
    "\n",
    "Suppose we chunked the input into sequences of 4 words to feed into the model. Remember that the Transformer XL is stateful, meaning the computations of each minibatch are carried over to the next minibatch. For a minibatch size of 1, handling this is simple. We just chunk the input and feed it into the model like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://i0.wp.com/mlexplained.com/wp-content/uploads/2019/06/Screen-Shot-2019-07-03-at-8.53.22-PM.png?w=1554&ssl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what happens if the `batch_size` is 2? We can't split the sentence like this, otherwise, we would be breaking the dependencies between segments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://mlexplained.com/wp-content/uploads/2019/07/Screen-Shot-2019-07-03-at-8.56.15-PM-e1562212986605.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct way to handle the corpus with a `batch_size`of 2 is to feed it like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://mlexplained.com/wp-content/uploads/2019/07/Screen-Shot-2019-07-03-at-9.05.04-PM-1-e1562213341253.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalizing this, we first divide the corpus into `batch_size` length segments, then feed each segment piece by piece into the model. Let's go through an example. Suppose `batch_size` is 4 and our entire corpus looks like this:\n",
    "\n",
    "    pytorch is an amazing deep learning framework that makes nlp really easy\n",
    "\n",
    "We want to make sure that the previous batch contains the previous segment at the same position.\n",
    "\n",
    "In other words, assuming we fed the model one word at a time, we want to iterate over this sentence like this\n",
    "\n",
    "    Batch 1: pytorch   amazing   framework nlp\n",
    "    Batch 2: is        deep      that      really\n",
    "    Batch 3: an        learning  makes     easy\n",
    "\n",
    "Notice that you can reconstruct the original sentence by reading from top to bottom, left to right\n",
    "instead of left to right, top to bottom.\n",
    "\n",
    "In reality, we feed the model with a sequence of words for each batch. The length of this sequence is commonly referred to the bptt (back propagation through time) length, since this is the maximum length the gradients propagate through in the sequence direction. With a longer bptt length of 2 for example, the\n",
    "minibatch would be of shape (batch_size, bptt) and would look like\n",
    "\n",
    "    Batch 1: pytorch   amazing   framework nlp\n",
    "             is        deep      that      really\n",
    "    Batch 2: an        learning  makes     easy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement this in a dataloader like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "import math\n",
    "\n",
    "class LMDataLoader(data.DataLoader):\n",
    "    def __init__(self, data: torch.LongTensor, batch_size: int, bptt: int,\n",
    "                 device=torch.device(\"cpu\")):\n",
    "        self.batch_size = batch_size\n",
    "        self.bptt = bptt\n",
    "        self.n_steps = data.size(0) // batch_size\n",
    "        \n",
    "        # we reshape the data here so that we can index\n",
    "        # efficiently into it while training\n",
    "        self.data = (data[:self.n_steps * batch_size] # trim off any elements that don't fit cleanly\n",
    "                     .view(batch_size, self.n_steps) # \n",
    "                     .transpose(0, 1) # \n",
    "                     .contiguous().to(device) # put on device as contiguous tensor\n",
    "                     )\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch_start_idx in range(0, self.data.size(0) - 1, self.bptt):\n",
    "            batch_end_idx = min(batch_start_idx + self.bptt, self.data.size(0) - 1)\n",
    "            # TODO: What is `self.ext_len` in the original code?\n",
    "            batch_data = self.data[batch_start_idx:batch_end_idx]\n",
    "            target = self.data[batch_start_idx+1:batch_end_idx+1]\n",
    "            # we generate the sequence length as well for loss calculation later\n",
    "            yield batch_data, target, batch_end_idx - batch_start_idx\n",
    "    \n",
    "    def __len__(self):\n",
    "        return math.ceil(self.data.size(0) / self.bptt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = torch.arange(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 16\n",
    "BPTT = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_corpus[:BPTT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = LMDataLoader(test_corpus, BS, BPTT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1, *_ = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 16])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,  62, 124, 186, 248, 310, 372, 434, 496, 558, 620, 682, 744, 806,\n",
       "         868, 930],\n",
       "        [  1,  63, 125, 187, 249, 311, 373, 435, 497, 559, 621, 683, 745, 807,\n",
       "         869, 931],\n",
       "        [  2,  64, 126, 188, 250, 312, 374, 436, 498, 560, 622, 684, 746, 808,\n",
       "         870, 932],\n",
       "        [  3,  65, 127, 189, 251, 313, 375, 437, 499, 561, 623, 685, 747, 809,\n",
       "         871, 933],\n",
       "        [  4,  66, 128, 190, 252, 314, 376, 438, 500, 562, 624, 686, 748, 810,\n",
       "         872, 934],\n",
       "        [  5,  67, 129, 191, 253, 315, 377, 439, 501, 563, 625, 687, 749, 811,\n",
       "         873, 935],\n",
       "        [  6,  68, 130, 192, 254, 316, 378, 440, 502, 564, 626, 688, 750, 812,\n",
       "         874, 936],\n",
       "        [  7,  69, 131, 193, 255, 317, 379, 441, 503, 565, 627, 689, 751, 813,\n",
       "         875, 937],\n",
       "        [  8,  70, 132, 194, 256, 318, 380, 442, 504, 566, 628, 690, 752, 814,\n",
       "         876, 938],\n",
       "        [  9,  71, 133, 195, 257, 319, 381, 443, 505, 567, 629, 691, 753, 815,\n",
       "         877, 939]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1, b2, sl = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the actual data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using the penn treebank dataset to benchmark our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "DATASET = \"penn\"\n",
    "DATA_DIR = Path(\"../data\") / DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using a utility vocabulary class borrowed directly from the Transformer XL repo to numericalize our inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vocabulary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-9eb8d270502c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../utils\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvocabulary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vocabulary'"
     ]
    }
   ],
   "source": [
    "import sys; sys.path.append(\"../utils\")\n",
    "from vocabulary import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(special=[\"<eos>\"], lower_case=True)\n",
    "\n",
    "vocab.count_file(DATA_DIR / \"train.txt\")\n",
    "vocab.count_file(DATA_DIR / \"valid.txt\")\n",
    "vocab.count_file(DATA_DIR / \"test.txt\")\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.build_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = vocab.encode_file(DATA_DIR / \"train.txt\", ordered=True, add_eos=True)\n",
    "valid_dataset = vocab.encode_file(DATA_DIR / \"valid.txt\", ordered=True, add_eos=True)\n",
    "test_dataset = vocab.encode_file(DATA_DIR / \"test.txt\", ordered=True, add_eos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can prepare the data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = LMDataLoader(train_dataset, config.bs, config.train_bptt, device=device)\n",
    "valid_iter = LMDataLoader(valid_dataset, config.bs, config.eval_bptt, device=device)\n",
    "test_iter = LMDataLoader(test_dataset, config.bs, config.eval_bptt, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We borrow the following initialization from the Transformer XL repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weight(weight):\n",
    "    nn.init.normal_(weight, 0.0, 0.02)\n",
    "\n",
    "def init_bias(bias):\n",
    "    nn.init.constant_(bias, 0.0)\n",
    "    \n",
    "# Borrowed from the transformer XL repo\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        if hasattr(m, 'weight') and m.weight is not None:\n",
    "            init_weight(m.weight)\n",
    "        if hasattr(m, 'bias') and m.bias is not None:\n",
    "            init_bias(m.bias)\n",
    "    elif classname.find('Embedding') != -1:\n",
    "        if hasattr(m, 'weight'):\n",
    "            init_weight(m.weight)\n",
    "    elif classname.find('LayerNorm') != -1:\n",
    "        if hasattr(m, 'weight'):\n",
    "            nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "        if hasattr(m, 'bias') and m.bias is not None:\n",
    "            init_bias(m.bias)\n",
    "    else:\n",
    "        if hasattr(m, 'u'):\n",
    "            init_weight(m.u)\n",
    "        if hasattr(m, 'v'):\n",
    "            init_weight(m.v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No fancy initialization here. Since we have multiple Layer Normalization layers, we can get away with initializing everything using a simple normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop is fairly standard. You can use any framework you like here including [ignite](https://github.com/pytorch/ignite), [allennlp](https://github.com/allenai/allennlp), and [fastai](https://github.com/fastai/fastai). We'll be writing our own loop to simplify things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "loss_change = []\n",
    "val_loss_change = []\n",
    "\n",
    "def train_epoch(\n",
    "    epoch: int,\n",
    "    model: nn.Module, train_loader: data.DataLoader, \n",
    "    val_loader: data.DataLoader,\n",
    "    optimizer: optim.Optimizer,\n",
    "    scheduler,\n",
    "    train_step_start=0.,\n",
    " ):\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    mems = None\n",
    "    train_step = train_step_start\n",
    "    train_loss = 0\n",
    "    log_start_time = time.time()\n",
    "    best_val_loss = float(\"inf\")\n",
    "    \n",
    "    pbar = tqdm(train_loader, total=min(config.max_step - train_step_start, len(train_loader)))\n",
    "    for batch_idx, (data, target, seq_len) in enumerate(pbar):\n",
    "        model.zero_grad()\n",
    "        out_dict = model(data, target, memory=mems)\n",
    "        loss, mems = out_dict[\"loss\"], out_dict[\"memory\"]\n",
    "\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        loss_change.append(loss.item())\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # step-wise learning rate annealing\n",
    "        train_step += 1\n",
    "        # linear warmup stage\n",
    "        if train_step < config.warmup_step:\n",
    "            curr_lr = config.lr * train_step / config.warmup_step\n",
    "            optimizer.param_groups[0]['lr'] = curr_lr\n",
    "        else:\n",
    "            scheduler.step(train_step)\n",
    "            \n",
    "        if train_step % config.log_interval == 0:\n",
    "            cur_loss = train_loss / config.log_interval\n",
    "            elapsed = time.time() - log_start_time\n",
    "            log_str = '| epoch {:3d} step {:>8d} | lr {:.3g} ' \\\n",
    "                      '| loss {:5.2f}'.format(\n",
    "                epoch, train_step, optimizer.param_groups[0]['lr'], cur_loss)\n",
    "            log_str += ' | ppl {:9.3f}'.format(math.exp(cur_loss))\n",
    "            pbar.set_description(log_str)\n",
    "            train_loss = 0\n",
    "            log_start_time = time.time()\n",
    "\n",
    "        if train_step % config.eval_interval == 0:\n",
    "            val_loss = evaluate(model, val_loader)\n",
    "            val_loss_change.append(val_loss)\n",
    "            eval_start_time = time.time()\n",
    "\n",
    "        if train_step == config.max_step:\n",
    "            return train_step\n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, valid_loader):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
    "    total_steps = min(config.max_step, len(train_loader) * config.epochs)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                    total_steps, eta_min=config.min_lr)\n",
    "    train_step_start = 0\n",
    "    for epoch in range(config.epochs):\n",
    "        if train_step_start >= config.max_step:\n",
    "            break\n",
    "        train_step_start = train_epoch(\n",
    "            epoch,\n",
    "            model,\n",
    "            train_iter,\n",
    "            valid_iter,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            train_step_start,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language models are normally evaluated by perplexity. Perplexity is the exponential of the cross entropy loss. It is also equivalent to the reciprocal of the likelihood. If the language model assigns a probability of 0.1 to each word in each input sentence on average, it would receive a perplexity of 100.\n",
    "\n",
    "Intuitively, perplexity represents how many tries it would take for the model to guess the correct word. A perplexity of 100 would signify that the model would need 100 tries to guess each word in the input sequence correctly.\n",
    "\n",
    "Keeping this in mind, we can write the evaluation code like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, val_loader: data.DataLoader):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    model.reset_length(config.eval_bptt,\n",
    "        0, config.eval_mem_len+config.train_bptt-config.eval_bptt)\n",
    "\n",
    "    # Evaluation\n",
    "    total_len, total_loss = 0, 0.\n",
    "    with torch.no_grad():\n",
    "        mems = None\n",
    "        for i, (data, target, seq_len) in enumerate(val_loader):\n",
    "            out_dict = model(data, target, memory=mems)\n",
    "            loss, mems = out_dict[\"loss\"], out_dict[\"memory\"]\n",
    "            total_loss += seq_len * loss.float().item()\n",
    "            total_len += seq_len\n",
    "\n",
    "    # Switch back to the training mode\n",
    "    model.reset_length(config.train_bptt, 0, config.mem_len)\n",
    "    model.train()\n",
    "    return total_loss / total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_final(model, val_loader):\n",
    "    model.eval()\n",
    "    total_len, total_loss = 0, 0.\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model.reset_length(config.eval_bptt, 0, config.eval_mem_len + config.train_bptt - config.eval_bptt)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        mems = None\n",
    "        for i, (data, target, seq_len) in enumerate(val_loader):\n",
    "            out_dict = model(data, target, memory=mems)\n",
    "            loss, mems = out_dict[\"loss\"], out_dict[\"memory\"]\n",
    "            total_loss += seq_len * loss.item()\n",
    "            total_len += seq_len\n",
    "        total_time = time.time() - start_time\n",
    "    \n",
    "    model.reset_length(config.train_bptt, 0, config.mem_len)\n",
    "    loss_val = total_loss / total_len\n",
    "    return {\"loss\": loss_val, \"ppl\": math.exp(loss_val)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all we have to do is initialize the model and start training it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_xl = TransformerXL(\n",
    "    num_embeddings=len(vocab), n_layers=config.n_layers,\n",
    "    n_heads=config.n_heads, d_model=config.d_model,\n",
    "    d_head_inner=config.d_head_inner, \n",
    "    d_ff_inner=config.d_ff_inner,\n",
    "    dropout=config.dropout,\n",
    "    dropouta=config.dropouta,\n",
    "    seq_len=config.train_bptt,\n",
    "    mem_len=config.mem_len,\n",
    ")\n",
    "if torch.cuda.is_available(): transformer_xl.cuda()\n",
    "transformer_xl.apply(weights_init);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    transformer_xl,\n",
    "    train_iter,\n",
    "    valid_iter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_final(transformer_xl, valid_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe the loss change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(val_loss_change)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall the loss is decreasing and everything looks nice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
