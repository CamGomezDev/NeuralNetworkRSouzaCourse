{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "440a7701-0beb-4861-9971-b0619fdf603a",
   "metadata": {},
   "source": [
    "\n",
    "How to Choose Loss Functions When Training Deep Learning Neural Networks\n",
    "By Jason Brownlee on January 30, 2019 in Deep Learning Performance\n",
    "\n",
    "Deep learning neural networks are trained using the stochastic gradient descent optimization algorithm.\n",
    "\n",
    "As part of the optimization algorithm, the error for the current state of the model must be estimated repeatedly. This requires the choice of an error function, conventionally called a loss function, that can be used to estimate the loss of the model so that the weights can be updated to reduce the loss on the next evaluation.\n",
    "\n",
    "Neural network models learn a mapping from inputs to outputs from examples and the choice of loss function must match the framing of the specific predictive modeling problem, such as classification or regression. Further, the configuration of the output layer must also be appropriate for the chosen loss function.\n",
    "\n",
    "In this tutorial, you will discover how to choose a loss function for your deep learning neural network for a given predictive modeling problem.\n",
    "\n",
    "After completing this tutorial, you will know:\n",
    "\n",
    "    How to configure a model for mean squared error and variants for regression problems.\n",
    "    How to configure a model for cross-entropy and hinge loss functions for binary classification.\n",
    "    How to configure a model for cross-entropy and KL divergence loss functions for multi-class classification.\n",
    "\n",
    "Kick-start your project with my new book Better Deep Learning, including step-by-step tutorials and the Python source code files for all examples.\n",
    "\n",
    "Let’s get started.\n",
    "\n",
    "    Update Oct/2019: Updated for Keras 2.3 and TensorFlow 2.0.\n",
    "    Update Jan/2020: Updated for changes in scikit-learn v0.22 API\n",
    "\n",
    "How to Choose Loss Functions When Training Deep Learning Neural Networks\n",
    "\n",
    "How to Choose Loss Functions When Training Deep Learning Neural Networks\n",
    "Photo by GlacierNPS, some rights reserved.\n",
    "Tutorial Overview\n",
    "\n",
    "This tutorial is divided into three parts; they are:\n",
    "\n",
    "    Regression Loss Functions\n",
    "        Mean Squared Error Loss\n",
    "        Mean Squared Logarithmic Error Loss\n",
    "        Mean Absolute Error Loss\n",
    "    Binary Classification Loss Functions\n",
    "        Binary Cross-Entropy\n",
    "        Hinge Loss\n",
    "        Squared Hinge Loss\n",
    "    Multi-Class Classification Loss Functions\n",
    "        Multi-Class Cross-Entropy Loss\n",
    "        Sparse Multiclass Cross-Entropy Loss\n",
    "        Kullback Leibler Divergence Loss\n",
    "\n",
    "We will focus on how to choose and implement different loss functions.\n",
    "\n",
    "For more theory on loss functions, see the post:\n",
    "\n",
    "    Loss and Loss Functions for Training Deep Learning Neural Networks\n",
    "\n",
    "Regression Loss Functions\n",
    "\n",
    "A regression predictive modeling problem involves predicting a real-valued quantity.\n",
    "\n",
    "In this section, we will investigate loss functions that are appropriate for regression predictive modeling problems.\n",
    "\n",
    "As the context for this investigation, we will use a standard regression problem generator provided by the scikit-learn library in the make_regression() function. This function will generate examples from a simple regression problem with a given number of input variables, statistical noise, and other properties.\n",
    "\n",
    "We will use this function to define a problem that has 20 input features; 10 of the features will be meaningful and 10 will not be relevant. A total of 1,000 examples will be randomly generated. The pseudorandom number generator will be fixed to ensure that we get the same 1,000 examples each time the code is run.\n",
    "# generate regression dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=1)\n",
    "1\n",
    "2\n",
    "\t\n",
    "# generate regression dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=1)\n",
    "\n",
    "Neural networks generally perform better when the real-valued input and output variables are to be scaled to a sensible range. For this problem, each of the input variables and the target variable have a Gaussian distribution; therefore, standardizing the data in this case is desirable.\n",
    "\n",
    "We can achieve this using the StandardScaler transformer class also from the scikit-learn library. On a real problem, we would prepare the scaler on the training dataset and apply it to the train and test sets, but for simplicity, we will scale all of the data together before splitting into train and test sets.\n",
    "# standardize dataset\n",
    "X = StandardScaler().fit_transform(X)\n",
    "y = StandardScaler().fit_transform(y.reshape(len(y),1))[:,0]\n",
    "1\n",
    "2\n",
    "3\n",
    "\t\n",
    "# standardize dataset\n",
    "X = StandardScaler().fit_transform(X)\n",
    "y = StandardScaler().fit_transform(y.reshape(len(y),1))[:,0]\n",
    "\n",
    "Once scaled, the data will be split evenly into train and test sets.\n",
    "# split into train and test\n",
    "n_train = 500\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "\t\n",
    "# split into train and test\n",
    "n_train = 500\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "\n",
    "A small Multilayer Perceptron (MLP) model will be defined to address this problem and provide the basis for exploring different loss functions.\n",
    "\n",
    "The model will expect 20 features as input as defined by the problem. The model will have one hidden layer with 25 nodes and will use the rectified linear activation function (ReLU). The output layer will have 1 node, given the one real-value to be predicted, and will use the linear activation function.\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=20, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "\t\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=20, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "The model will be fit with stochastic gradient descent with a learning rate of 0.01 and a momentum of 0.9, both sensible default values.\n",
    "\n",
    "Training will be performed for 100 epochs and the test set will be evaluated at the end of each epoch so that we can plot learning curves at the end of the run.\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='...', optimizer=opt)\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=100, verbose=0)\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "\t\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='...', optimizer=opt)\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=100, verbose=0)\n",
    "\n",
    "Now that we have the basis of a problem and model, we can take a look evaluating three common loss functions that are appropriate for a regression predictive modeling problem.\n",
    "\n",
    "Although an MLP is used in these examples, the same loss functions can be used when training CNN and RNN models for regression.\n",
    "Want Better Results with Deep Learning?\n",
    "\n",
    "Take my free 7-day email crash course now (with sample code).\n",
    "\n",
    "Click to sign-up and also get a free PDF Ebook version of the course.\n",
    "\n",
    "Download Your FREE Mini-Course\n",
    "Mean Squared Error Loss\n",
    "\n",
    "The Mean Squared Error, or MSE, loss is the default loss to use for regression problems.\n",
    "\n",
    "Mathematically, it is the preferred loss function under the inference framework of maximum likelihood if the distribution of the target variable is Gaussian. It is the loss function to be evaluated first and only changed if you have a good reason.\n",
    "\n",
    "Mean squared error is calculated as the average of the squared differences between the predicted and actual values. The result is always positive regardless of the sign of the predicted and actual values and a perfect value is 0.0. The squaring means that larger mistakes result in more error than smaller mistakes, meaning that the model is punished for making larger mistakes.\n",
    "\n",
    "The mean squared error loss function can be used in Keras by specifying ‘mse‘ or ‘mean_squared_error‘ as the loss function when compiling the model.\n",
    "model.compile(loss='mean_squared_error')\n",
    "1\n",
    "\t\n",
    "model.compile(loss='mean_squared_error')\n",
    "\n",
    "It is recommended that the output layer has one node for the target variable and the linear activation function is used.\n",
    "model.add(Dense(1, activation='linear'))\n",
    "1\n",
    "\t\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "A complete example of demonstrating an MLP on the described regression problem is listed below.\n",
    "# mlp for regression with mse loss function\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from matplotlib import pyplot\n",
    "# generate regression dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=1)\n",
    "# standardize dataset\n",
    "X = StandardScaler().fit_transform(X)\n",
    "y = StandardScaler().fit_transform(y.reshape(len(y),1))[:,0]\n",
    "# split into train and test\n",
    "n_train = 500\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=20, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='mean_squared_error', optimizer=opt)\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=100, verbose=0)\n",
    "# evaluate the model\n",
    "train_mse = model.evaluate(trainX, trainy, verbose=0)\n",
    "test_mse = model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))\n",
    "# plot loss during training\n",
    "pyplot.title('Loss / Mean Squared Error')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "5\n",
    "6\n",
    "7\n",
    "8\n",
    "9\n",
    "10\n",
    "11\n",
    "12\n",
    "13\n",
    "14\n",
    "15\n",
    "16\n",
    "17\n",
    "18\n",
    "19\n",
    "20\n",
    "21\n",
    "22\n",
    "23\n",
    "24\n",
    "25\n",
    "26\n",
    "27\n",
    "28\n",
    "29\n",
    "30\n",
    "31\n",
    "32\n",
    "33\n",
    "34\n",
    "\t\n",
    "# mlp for regression with mse loss function\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from matplotlib import pyplot\n",
    "# generate regression dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=1)\n",
    "# standardize dataset\n",
    "X = StandardScaler().fit_transform(X)\n",
    "y = StandardScaler().fit_transform(y.reshape(len(y),1))[:,0]\n",
    "# split into train and test\n",
    "n_train = 500\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=20, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='mean_squared_error', optimizer=opt)\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=100, verbose=0)\n",
    "# evaluate the model\n",
    "train_mse = model.evaluate(trainX, trainy, verbose=0)\n",
    "test_mse = model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))\n",
    "# plot loss during training\n",
    "pyplot.title('Loss / Mean Squared Error')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "\n",
    "Running the example first prints the mean squared error for the model on the train and test datasets.\n",
    "\n",
    "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
    "\n",
    "In this case, we can see that the model learned the problem achieving zero error, at least to three decimal places.\n",
    "Train: 0.000, Test: 0.001\n",
    "1\n",
    "\t\n",
    "Train: 0.000, Test: 0.001\n",
    "\n",
    "A line plot is also created showing the mean squared error loss over the training epochs for both the train (blue) and test (orange) sets.\n",
    "\n",
    "We can see that the model converged reasonably quickly and both train and test performance remained equivalent. The performance and convergence behavior of the model suggest that mean squared error is a good match for a neural network learning this problem.\n",
    "Line plot of Mean Squared Error Loss over Training Epochs When Optimizing the Mean Squared Error Loss Function\n",
    "\n",
    "Line plot of Mean Squared Error Loss over Training Epochs When Optimizing the Mean Squared Error Loss Function\n",
    "Mean Squared Logarithmic Error Loss\n",
    "\n",
    "There may be regression problems in which the target value has a spread of values and when predicting a large value, you may not want to punish a model as heavily as mean squared error.\n",
    "\n",
    "Instead, you can first calculate the natural logarithm of each of the predicted values, then calculate the mean squared error. This is called the Mean Squared Logarithmic Error loss, or MSLE for short.\n",
    "\n",
    "It has the effect of relaxing the punishing effect of large differences in large predicted values.\n",
    "\n",
    "As a loss measure, it may be more appropriate when the model is predicting unscaled quantities directly. Nevertheless, we can demonstrate this loss function using our simple regression problem.\n",
    "\n",
    "The model can be updated to use the ‘mean_squared_logarithmic_error‘ loss function and keep the same configuration for the output layer. We will also track the mean squared error as a metric when fitting the model so that we can use it as a measure of performance and plot the learning curve.\n",
    "model.compile(loss='mean_squared_logarithmic_error', optimizer=opt, metrics=['mse'])\n",
    "1\n",
    "\t\n",
    "model.compile(loss='mean_squared_logarithmic_error', optimizer=opt, metrics=['mse'])\n",
    "\n",
    "The complete example of using the MSLE loss function is listed below.\n",
    "# mlp for regression with msle loss function\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from matplotlib import pyplot\n",
    "# generate regression dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=1)\n",
    "# standardize dataset\n",
    "X = StandardScaler().fit_transform(X)\n",
    "y = StandardScaler().fit_transform(y.reshape(len(y),1))[:,0]\n",
    "# split into train and test\n",
    "n_train = 500\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=20, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='mean_squared_logarithmic_error', optimizer=opt, metrics=['mse'])\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=100, verbose=0)\n",
    "# evaluate the model\n",
    "_, train_mse = model.evaluate(trainX, trainy, verbose=0)\n",
    "_, test_mse = model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))\n",
    "# plot loss during training\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Loss')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "# plot mse during training\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Mean Squared Error')\n",
    "pyplot.plot(history.history['mean_squared_error'], label='train')\n",
    "pyplot.plot(history.history['val_mean_squared_error'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "5\n",
    "6\n",
    "7\n",
    "8\n",
    "9\n",
    "10\n",
    "11\n",
    "12\n",
    "13\n",
    "14\n",
    "15\n",
    "16\n",
    "17\n",
    "18\n",
    "19\n",
    "20\n",
    "21\n",
    "22\n",
    "23\n",
    "24\n",
    "25\n",
    "26\n",
    "27\n",
    "28\n",
    "29\n",
    "30\n",
    "31\n",
    "32\n",
    "33\n",
    "34\n",
    "35\n",
    "36\n",
    "37\n",
    "38\n",
    "39\n",
    "40\n",
    "41\n",
    "\t\n",
    "# mlp for regression with msle loss function\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from matplotlib import pyplot\n",
    "# generate regression dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=1)\n",
    "# standardize dataset\n",
    "X = StandardScaler().fit_transform(X)\n",
    "y = StandardScaler().fit_transform(y.reshape(len(y),1))[:,0]\n",
    "# split into train and test\n",
    "n_train = 500\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=20, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='mean_squared_logarithmic_error', optimizer=opt, metrics=['mse'])\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=100, verbose=0)\n",
    "# evaluate the model\n",
    "_, train_mse = model.evaluate(trainX, trainy, verbose=0)\n",
    "_, test_mse = model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))\n",
    "# plot loss during training\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Loss')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "# plot mse during training\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Mean Squared Error')\n",
    "pyplot.plot(history.history['mean_squared_error'], label='train')\n",
    "pyplot.plot(history.history['val_mean_squared_error'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "\n",
    "Running the example first prints the mean squared error for the model on the train and test dataset.\n",
    "\n",
    "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
    "\n",
    "In this case, we can see that the model resulted in slightly worse MSE on both the training and test dataset. It may not be a good fit for this problem as the distribution of the target variable is a standard Gaussian.\n",
    "Train: 0.165, Test: 0.184\n",
    "1\n",
    "\t\n",
    "Train: 0.165, Test: 0.184\n",
    "\n",
    "A line plot is also created showing the mean squared logarithmic error loss over the training epochs for both the train (blue) and test (orange) sets (top), and a similar plot for the mean squared error (bottom).\n",
    "\n",
    "We can see that the MSLE converged well over the 100 epochs algorithm; it appears that the MSE may be showing signs of overfitting the problem, dropping fast and starting to rise from epoch 20 onwards.\n",
    "Line Plots of Mean Squared Logistic Error Loss and Mean Squared Error Over Training Epochs\n",
    "\n",
    "Line Plots of Mean Squared Logarithmic Error Loss and Mean Squared Error Over Training Epochs\n",
    "Mean Absolute Error Loss\n",
    "\n",
    "On some regression problems, the distribution of the target variable may be mostly Gaussian, but may have outliers, e.g. large or small values far from the mean value.\n",
    "\n",
    "The Mean Absolute Error, or MAE, loss is an appropriate loss function in this case as it is more robust to outliers. It is calculated as the average of the absolute difference between the actual and predicted values.\n",
    "\n",
    "The model can be updated to use the ‘mean_absolute_error‘ loss function and keep the same configuration for the output layer.\n",
    "model.compile(loss='mean_absolute_error', optimizer=opt, metrics=['mse'])\n",
    "1\n",
    "\t\n",
    "model.compile(loss='mean_absolute_error', optimizer=opt, metrics=['mse'])\n",
    "\n",
    "The complete example using the mean absolute error as the loss function on the regression test problem is listed below.\n",
    "# mlp for regression with mae loss function\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from matplotlib import pyplot\n",
    "# generate regression dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=1)\n",
    "# standardize dataset\n",
    "X = StandardScaler().fit_transform(X)\n",
    "y = StandardScaler().fit_transform(y.reshape(len(y),1))[:,0]\n",
    "# split into train and test\n",
    "n_train = 500\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=20, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='mean_absolute_error', optimizer=opt, metrics=['mse'])\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=100, verbose=0)\n",
    "# evaluate the model\n",
    "_, train_mse = model.evaluate(trainX, trainy, verbose=0)\n",
    "_, test_mse = model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))\n",
    "# plot loss during training\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Loss')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "# plot mse during training\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Mean Squared Error')\n",
    "pyplot.plot(history.history['mean_squared_error'], label='train')\n",
    "pyplot.plot(history.history['val_mean_squared_error'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "5\n",
    "6\n",
    "7\n",
    "8\n",
    "9\n",
    "10\n",
    "11\n",
    "12\n",
    "13\n",
    "14\n",
    "15\n",
    "16\n",
    "17\n",
    "18\n",
    "19\n",
    "20\n",
    "21\n",
    "22\n",
    "23\n",
    "24\n",
    "25\n",
    "26\n",
    "27\n",
    "28\n",
    "29\n",
    "30\n",
    "31\n",
    "32\n",
    "33\n",
    "34\n",
    "35\n",
    "36\n",
    "37\n",
    "38\n",
    "39\n",
    "40\n",
    "41\n",
    "\t\n",
    "# mlp for regression with mae loss function\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from matplotlib import pyplot\n",
    "# generate regression dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=1)\n",
    "# standardize dataset\n",
    "X = StandardScaler().fit_transform(X)\n",
    "y = StandardScaler().fit_transform(y.reshape(len(y),1))[:,0]\n",
    "# split into train and test\n",
    "n_train = 500\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=20, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='mean_absolute_error', optimizer=opt, metrics=['mse'])\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=100, verbose=0)\n",
    "# evaluate the model\n",
    "_, train_mse = model.evaluate(trainX, trainy, verbose=0)\n",
    "_, test_mse = model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))\n",
    "# plot loss during training\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Loss')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "# plot mse during training\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Mean Squared Error')\n",
    "pyplot.plot(history.history['mean_squared_error'], label='train')\n",
    "pyplot.plot(history.history['val_mean_squared_error'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "\n",
    "Running the example first prints the mean squared error for the model on the train and test dataset.\n",
    "\n",
    "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
    "\n",
    "In this case, we can see that the model learned the problem, achieving a near zero error, at least to three decimal places.\n",
    "Train: 0.002, Test: 0.002\n",
    "1\n",
    "\t\n",
    "Train: 0.002, Test: 0.002\n",
    "\n",
    "A line plot is also created showing the mean absolute error loss over the training epochs for both the train (blue) and test (orange) sets (top), and a similar plot for the mean squared error (bottom).\n",
    "\n",
    "In this case, we can see that MAE does converge but shows a bumpy course, although the dynamics of MSE don’t appear greatly affected. We know that the target variable is a standard Gaussian with no large outliers, so MAE would not be a good fit in this case.\n",
    "\n",
    "It might be more appropriate on this problem if we did not scale the target variable first.\n",
    "Line plots of Mean Absolute Error Loss and Mean Squared Error over Training Epochs\n",
    "\n",
    "Line plots of Mean Absolute Error Loss and Mean Squared Error over Training Epochs\n",
    "Binary Classification Loss Functions\n",
    "\n",
    "Binary classification are those predictive modeling problems where examples are assigned one of two labels.\n",
    "\n",
    "The problem is often framed as predicting a value of 0 or 1 for the first or second class and is often implemented as predicting the probability of the example belonging to class value 1.\n",
    "\n",
    "In this section, we will investigate loss functions that are appropriate for binary classification predictive modeling problems.\n",
    "\n",
    "We will generate examples from the circles test problem in scikit-learn as the basis for this investigation. The circles problem involves samples drawn from two concentric circles on a two-dimensional plane, where points on the outer circle belong to class 0 and points for the inner circle belong to class 1. Statistical noise is added to the samples to add ambiguity and make the problem more challenging to learn.\n",
    "\n",
    "We will generate 1,000 examples and add 10% statistical noise. The pseudorandom number generator will be seeded with the same value to ensure that we always get the same 1,000 examples.\n",
    "# generate circles\n",
    "X, y = make_circles(n_samples=1000, noise=0.1, random_state=1)\n",
    "1\n",
    "2\n",
    "\t\n",
    "# generate circles\n",
    "X, y = make_circles(n_samples=1000, noise=0.1, random_state=1)\n",
    "\n",
    "We can create a scatter plot of the dataset to get an idea of the problem we are modeling. The complete example is listed below.\n",
    "# scatter plot of the circles dataset with points colored by class\n",
    "from sklearn.datasets import make_circles\n",
    "from numpy import where\n",
    "from matplotlib import pyplot\n",
    "# generate circles\n",
    "X, y = make_circles(n_samples=1000, noise=0.1, random_state=1)\n",
    "# select indices of points with each class label\n",
    "for i in range(2):\n",
    "\tsamples_ix = where(y == i)\n",
    "\tpyplot.scatter(X[samples_ix, 0], X[samples_ix, 1], label=str(i))\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "5\n",
    "6\n",
    "7\n",
    "8\n",
    "9\n",
    "10\n",
    "11\n",
    "12\n",
    "\t\n",
    "# scatter plot of the circles dataset with points colored by class\n",
    "from sklearn.datasets import make_circles\n",
    "from numpy import where\n",
    "from matplotlib import pyplot\n",
    "# generate circles\n",
    "X, y = make_circles(n_samples=1000, noise=0.1, random_state=1)\n",
    "# select indices of points with each class label\n",
    "for i in range(2):\n",
    "\tsamples_ix = where(y == i)\n",
    "\tpyplot.scatter(X[samples_ix, 0], X[samples_ix, 1], label=str(i))\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "\n",
    "Running the example creates a scatter plot of the examples, where the input variables define the location of the point and the class value defines the color, with class 0 blue and class 1 orange.\n",
    "Scatter Plot of Dataset for the Circles Binary Classification Problem\n",
    "\n",
    "Scatter Plot of Dataset for the Circles Binary Classification Problem\n",
    "\n",
    "The points are already reasonably scaled around 0, almost in [-1,1]. We won’t rescale them in this case.\n",
    "\n",
    "The dataset is split evenly for train and test sets.\n",
    "# split into train and test\n",
    "n_train = 500\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "\t\n",
    "# split into train and test\n",
    "n_train = 500\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "\n",
    "A simple MLP model can be defined to address this problem that expects two inputs for the two features in the dataset, a hidden layer with 50 nodes, a rectified linear activation function and an output layer that will need to be configured for the choice of loss function.\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(1, activation='...'))\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "\t\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(1, activation='...'))\n",
    "\n",
    "The model will be fit using stochastic gradient descent with the sensible default learning rate of 0.01 and momentum of 0.9.\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='...', optimizer=opt, metrics=['accuracy'])\n",
    "1\n",
    "2\n",
    "\t\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='...', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "We will fit the model for 200 training epochs and evaluate the performance of the model against the loss and accuracy at the end of each epoch so that we can plot learning curves.\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=200, verbose=0)\n",
    "1\n",
    "2\n",
    "\t\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=200, verbose=0)\n",
    "\n",
    "Now that we have the basis of a problem and model, we can take a look evaluating three common loss functions that are appropriate for a binary classification predictive modeling problem.\n",
    "\n",
    "Although an MLP is used in these examples, the same loss functions can be used when training CNN and RNN models for binary classification.\n",
    "Binary Cross-Entropy Loss\n",
    "\n",
    "Cross-entropy is the default loss function to use for binary classification problems.\n",
    "\n",
    "It is intended for use with binary classification where the target values are in the set {0, 1}.\n",
    "\n",
    "Mathematically, it is the preferred loss function under the inference framework of maximum likelihood. It is the loss function to be evaluated first and only changed if you have a good reason.\n",
    "\n",
    "Cross-entropy will calculate a score that summarizes the average difference between the actual and predicted probability distributions for predicting class 1. The score is minimized and a perfect cross-entropy value is 0.\n",
    "\n",
    "Cross-entropy can be specified as the loss function in Keras by specifying ‘binary_crossentropy‘ when compiling the model.\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "1\n",
    "\t\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "The function requires that the output layer is configured with a single node and a ‘sigmoid‘ activation in order to predict the probability for class 1.\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "1\n",
    "\t\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "The complete example of an MLP with cross-entropy loss for the two circles binary classification problem is listed below.\n",
    "# mlp for the circles problem with cross entropy loss\n",
    "from sklearn.datasets import make_circles\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from matplotlib import pyplot\n",
    "# generate 2d classification dataset\n",
    "X, y = make_circles(n_samples=1000, noise=0.1, random_state=1)\n",
    "# split into train and test\n",
    "n_train = 500\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=200, verbose=0)\n",
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "# plot loss during training\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Loss')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "# plot accuracy during training\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Accuracy')\n",
    "pyplot.plot(history.history['accuracy'], label='train')\n",
    "pyplot.plot(history.history['val_accuracy'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "5\n",
    "6\n",
    "7\n",
    "8\n",
    "9\n",
    "10\n",
    "11\n",
    "12\n",
    "13\n",
    "14\n",
    "15\n",
    "16\n",
    "17\n",
    "18\n",
    "19\n",
    "20\n",
    "21\n",
    "22\n",
    "23\n",
    "24\n",
    "25\n",
    "26\n",
    "27\n",
    "28\n",
    "29\n",
    "30\n",
    "31\n",
    "32\n",
    "33\n",
    "34\n",
    "35\n",
    "36\n",
    "37\n",
    "\t\n",
    "# mlp for the circles problem with cross entropy loss\n",
    "from sklearn.datasets import make_circles\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from matplotlib import pyplot\n",
    "# generate 2d classification dataset\n",
    "X, y = make_circles(n_samples=1000, noise=0.1, random_state=1)\n",
    "# split into train and test\n",
    "n_train = 500\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=200, verbose=0)\n",
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "# plot loss during training\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Loss')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "# plot accuracy during training\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Accuracy')\n",
    "pyplot.plot(history.history['accuracy'], label='train')\n",
    "pyplot.plot(history.history['val_accuracy'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "\n",
    "Running the example first prints the classification accuracy for the model on the train and test dataset.\n",
    "\n",
    "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
    "\n",
    "In this case, we can see that the model learned the problem reasonably well, achieving about 83% accuracy on the training dataset and about 85% on the test dataset. The scores are reasonably close, suggesting the model is probably not over or underfit.\n",
    "Train: 0.836, Test: 0.852\n",
    "1\n",
    "\t\n",
    "Train: 0.836, Test: 0.852\n",
    "\n",
    "A figure is also created showing two line plots, the top with the cross-entropy loss over epochs for the train (blue) and test (orange) dataset, and the bottom plot showing classification accuracy over epochs.\n",
    "\n",
    "The plot shows that the training process converged well. The plot for loss is smooth, given the continuous nature of the error between the probability distributions, whereas the line plot for accuracy shows bumps, given examples in the train and test set can ultimately only be predicted as correct or incorrect, providing less granular feedback on performance.\n",
    "Line Plots of Cross Entropy Loss and Classification Accuracy over Training Epochs on the Two Circles Binary Classification Problem\n",
    "\n",
    "Line Plots of Cross Entropy Loss and Classification Accuracy over Training Epochs on the Two Circles Binary Classification Problem\n",
    "Hinge Loss\n",
    "\n",
    "An alternative to cross-entropy for binary classification problems is the hinge loss function, primarily developed for use with Support Vector Machine (SVM) models.\n",
    "\n",
    "It is intended for use with binary classification where the target values are in the set {-1, 1}.\n",
    "\n",
    "The hinge loss function encourages examples to have the correct sign, assigning more error when there is a difference in the sign between the actual and predicted class values.\n",
    "\n",
    "Reports of performance with the hinge loss are mixed, sometimes resulting in better performance than cross-entropy on binary classification problems.\n",
    "\n",
    "Firstly, the target variable must be modified to have values in the set {-1, 1}.\n",
    "# change y from {0,1} to {-1,1}\n",
    "y[where(y == 0)] = -1\n",
    "1\n",
    "2\n",
    "\t\n",
    "# change y from {0,1} to {-1,1}\n",
    "y[where(y == 0)] = -1\n",
    "\n",
    "The hinge loss function can then be specified as the ‘hinge‘ in the compile function.\n",
    "model.compile(loss='hinge', optimizer=opt, metrics=['accuracy'])\n",
    "1\n",
    "\t\n",
    "model.compile(loss='hinge', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "Finally, the output layer of the network must be configured to have a single node with a hyperbolic tangent activation function capable of outputting a single value in the range [-1, 1].\n",
    "model.add(Dense(1, activation='tanh'))\n",
    "1\n",
    "\t\n",
    "model.add(Dense(1, activation='tanh'))\n",
    "\n",
    "The complete example of an MLP with a hinge loss function for the two circles binary classification problem is listed below.\n",
    "# mlp for the circles problem with hinge loss\n",
    "from sklearn.datasets import make_circles\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from matplotlib import pyplot\n",
    "from numpy import where\n",
    "# generate 2d classification dataset\n",
    "X, y = make_circles(n_samples=1000, noise=0.1, random_state=1)\n",
    "# change y from {0,1} to {-1,1}\n",
    "y[where(y == 0)] = -1\n",
    "# split into train and test\n",
    "n_train = 500\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(1, activation='tanh'))\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='hinge', optimizer=opt, metrics=['accuracy'])\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=200, verbose=0)\n",
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "# plot loss during training\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Loss')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "# plot accuracy during training\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Accuracy')\n",
    "pyplot.plot(history.history['accuracy'], label='train')\n",
    "pyplot.plot(history.history['val_accuracy'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "5\n",
    "6\n",
    "7\n",
    "8\n",
    "9\n",
    "10\n",
    "11\n",
    "12\n",
    "13\n",
    "14\n",
    "15\n",
    "16\n",
    "17\n",
    "18\n",
    "19\n",
    "20\n",
    "21\n",
    "22\n",
    "23\n",
    "24\n",
    "25\n",
    "26\n",
    "27\n",
    "28\n",
    "29\n",
    "30\n",
    "31\n",
    "32\n",
    "33\n",
    "34\n",
    "35\n",
    "36\n",
    "37\n",
    "38\n",
    "39\n",
    "40\n",
    "\t\n",
    "# mlp for the circles problem with hinge loss\n",
    "from sklearn.datasets import make_circles\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from matplotlib import pyplot\n",
    "from numpy import where\n",
    "# generate 2d classification dataset\n",
    "X, y = make_circles(n_samples=1000, noise=0.1, random_state=1)\n",
    "# change y from {0,1} to {-1,1}\n",
    "y[where(y == 0)] = -1\n",
    "# split into train and test\n",
    "n_train = 500\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(1, activation='tanh'))\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='hinge', optimizer=opt, metrics=['accuracy'])\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=200, verbose=0)\n",
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "# plot loss during training\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Loss')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "# plot accuracy during training\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Accuracy')\n",
    "pyplot.plot(history.history['accuracy'], label='train')\n",
    "pyplot.plot(history.history['val_accuracy'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "\n",
    "Running the example first prints the classification accuracy for the model on the train and test dataset.\n",
    "\n",
    "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
    "\n",
    "In this case, we can see slightly worse performance than using cross-entropy, with the chosen model configuration with less than 80% accuracy on the train and test sets.\n",
    "Train: 0.792, Test: 0.740\n",
    "1\n",
    "\t\n",
    "Train: 0.792, Test: 0.740\n",
    "\n",
    "A figure is also created showing two line plots, the top with the hinge loss over epochs for the train (blue) and test (orange) dataset, and the bottom plot showing classification accuracy over epochs.\n",
    "\n",
    "The plot of hinge loss shows that the model has converged and has reasonable loss on both datasets. The plot of classification accuracy also shows signs of convergence, albeit at a lower level of skill than may be desirable on this problem.\n",
    "Line Plots of Hinge Loss and Classification Accuracy over Training Epochs on the Two Circles Binary Classification Problem\n",
    "\n",
    "Line Plots of Hinge Loss and Classification Accuracy over Training Epochs on the Two Circles Binary Classification Problem\n",
    "Squared Hinge Loss\n",
    "\n",
    "The hinge loss function has many extensions, often the subject of investigation with SVM models.\n",
    "\n",
    "A popular extension is called the squared hinge loss that simply calculates the square of the score hinge loss. It has the effect of smoothing the surface of the error function and making it numerically easier to work with.\n",
    "\n",
    "If using a hinge loss does result in better performance on a given binary classification problem, is likely that a squared hinge loss may be appropriate.\n",
    "\n",
    "As with using the hinge loss function, the target variable must be modified to have values in the set {-1, 1}.\n",
    "# change y from {0,1} to {-1,1}\n",
    "y[where(y == 0)] = -1\n",
    "1\n",
    "2\n",
    "\t\n",
    "# change y from {0,1} to {-1,1}\n",
    "y[where(y == 0)] = -1\n",
    "\n",
    "The squared hinge loss can be specified as ‘squared_hinge‘ in the compile() function when defining the model.\n",
    "model.compile(loss='squared_hinge', optimizer=opt, metrics=['accuracy'])\n",
    "1\n",
    "\t\n",
    "model.compile(loss='squared_hinge', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "And finally, the output layer must use a single node with a hyperbolic tangent activation function capable of outputting continuous values in the range [-1, 1].\n",
    "model.add(Dense(1, activation='tanh'))\n",
    "1\n",
    "\t\n",
    "model.add(Dense(1, activation='tanh'))\n",
    "\n",
    "The complete example of an MLP with the squared hinge loss function on the two circles binary classification problem is listed below.\n",
    "# mlp for the circles problem with squared hinge loss\n",
    "from sklearn.datasets import make_circles\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from matplotlib import pyplot\n",
    "from numpy import where\n",
    "# generate 2d classification dataset\n",
    "X, y = make_circles(n_samples=1000, noise=0.1, random_state=1)\n",
    "# change y from {0,1} to {-1,1}\n",
    "y[where(y == 0)] = -1\n",
    "# split into train and test\n",
    "n_train = 500\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(1, activation='tanh'))\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='squared_hinge', optimizer=opt, metrics=['accuracy'])\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=200, verbose=0)\n",
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "# plot loss during training\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Loss')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "# plot accuracy during training\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Accuracy')\n",
    "pyplot.plot(history.history['accuracy'], label='train')\n",
    "pyplot.plot(history.history['val_accuracy'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "5\n",
    "6\n",
    "7\n",
    "8\n",
    "9\n",
    "10\n",
    "11\n",
    "12\n",
    "13\n",
    "14\n",
    "15\n",
    "16\n",
    "17\n",
    "18\n",
    "19\n",
    "20\n",
    "21\n",
    "22\n",
    "23\n",
    "24\n",
    "25\n",
    "26\n",
    "27\n",
    "28\n",
    "29\n",
    "30\n",
    "31\n",
    "32\n",
    "33\n",
    "34\n",
    "35\n",
    "36\n",
    "37\n",
    "38\n",
    "39\n",
    "40\n",
    "\t\n",
    "# mlp for the circles problem with squared hinge loss\n",
    "from sklearn.datasets import make_circles\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from matplotlib import pyplot\n",
    "from numpy import where\n",
    "# generate 2d classification dataset\n",
    "X, y = make_circles(n_samples=1000, noise=0.1, random_state=1)\n",
    "# change y from {0,1} to {-1,1}\n",
    "y[where(y == 0)] = -1\n",
    "# split into train and test\n",
    "n_train = 500\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(1, activation='tanh'))\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='squared_hinge', optimizer=opt, metrics=['accuracy'])\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=200, verbose=0)\n",
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "# plot loss during training\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Loss')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "# plot accuracy during training\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Accuracy')\n",
    "pyplot.plot(history.history['accuracy'], label='train')\n",
    "pyplot.plot(history.history['val_accuracy'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "\n",
    "Running the example first prints the classification accuracy for the model on the train and test datasets.\n",
    "\n",
    "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
    "\n",
    "In this case, we can see that for this problem and the chosen model configuration, the hinge squared loss may not be appropriate, resulting in classification accuracy of less than 70% on the train and test sets.\n",
    "Train: 0.682, Test: 0.646\n",
    "1\n",
    "\t\n",
    "Train: 0.682, Test: 0.646\n",
    "\n",
    "A figure is also created showing two line plots, the top with the squared hinge loss over epochs for the train (blue) and test (orange) dataset, and the bottom plot showing classification accuracy over epochs.\n",
    "\n",
    "The plot of loss shows that indeed, the model converged, but the shape of the error surface is not as smooth as other loss functions where small changes to the weights are causing large changes in loss.\n",
    "Line Plots of Squared Hinge Loss and Classification Accuracy over Training Epochs on the Two Circles Binary Classification Problem\n",
    "\n",
    "Line Plots of Squared Hinge Loss and Classification Accuracy over Training Epochs on the Two Circles Binary Classification Problem\n",
    "Multi-Class Classification Loss Functions\n",
    "\n",
    "Multi-Class classification are those predictive modeling problems where examples are assigned one of more than two classes.\n",
    "\n",
    "The problem is often framed as predicting an integer value, where each class is assigned a unique integer value from 0 to (num_classes – 1). The problem is often implemented as predicting the probability of the example belonging to each known class.\n",
    "\n",
    "In this section, we will investigate loss functions that are appropriate for multi-class classification predictive modeling problems.\n",
    "\n",
    "We will use the blobs problem as the basis for the investigation. The make_blobs() function provided by the scikit-learn provides a way to generate examples given a specified number of classes and input features. We will use this function to generate 1,000 examples for a 3-class classification problem with 2 input variables. The pseudorandom number generator will be seeded consistently so that the same 1,000 examples are generated each time the code is run.\n",
    "# generate dataset\n",
    "X, y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "1\n",
    "2\n",
    "\t\n",
    "# generate dataset\n",
    "X, y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "\n",
    "The two input variables can be taken as x and y coordinates for points on a two-dimensional plane.\n",
    "\n",
    "The example below creates a scatter plot of the entire dataset coloring points by their class membership.\n",
    "# scatter plot of blobs dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "from numpy import where\n",
    "from matplotlib import pyplot\n",
    "# generate dataset\n",
    "X, y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "# select indices of points with each class label\n",
    "for i in range(3):\n",
    "\tsamples_ix = where(y == i)\n",
    "\tpyplot.scatter(X[samples_ix, 0], X[samples_ix, 1])\n",
    "pyplot.show()\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "5\n",
    "6\n",
    "7\n",
    "8\n",
    "9\n",
    "10\n",
    "11\n",
    "\t\n",
    "# scatter plot of blobs dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "from numpy import where\n",
    "from matplotlib import pyplot\n",
    "# generate dataset\n",
    "X, y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "# select indices of points with each class label\n",
    "for i in range(3):\n",
    "\tsamples_ix = where(y == i)\n",
    "\tpyplot.scatter(X[samples_ix, 0], X[samples_ix, 1])\n",
    "pyplot.show()\n",
    "\n",
    "Running the example creates a scatter plot showing the 1,000 examples in the dataset with examples belonging to the 0, 1, and 2 classes colors blue, orange, and green respectively.\n",
    "Scatter Plot of Examples Generated from the Blobs Multi-Class Classification Problem\n",
    "\n",
    "Scatter Plot of Examples Generated from the Blobs Multi-Class Classification Problem\n",
    "\n",
    "The input features are Gaussian and could benefit from standardization; nevertheless, we will keep the values unscaled in this example for brevity.\n",
    "\n",
    "The dataset will be split evenly between train and test sets.\n",
    "# split into train and test\n",
    "n_train = 500\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "\t\n",
    "# split into train and test\n",
    "n_train = 500\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "\n",
    "A small MLP model will be used as the basis for exploring loss functions.\n",
    "\n",
    "The model expects two input variables, has 50 nodes in the hidden layer and the rectified linear activation function, and an output layer that must be customized based on the selection of the loss function.\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(..., activation='...'))\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "\t\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(..., activation='...'))\n",
    "\n",
    "The model is fit using stochastic gradient descent with a sensible default learning rate of 0.01 and a momentum of 0.9.\n",
    "# compile model\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='...', optimizer=opt, metrics=['accuracy'])\n",
    "1\n",
    "2\n",
    "3\n",
    "\t\n",
    "# compile model\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='...', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "The model will be fit for 100 epochs on the training dataset and the test dataset will be used as a validation dataset, allowing us to evaluate both loss and classification accuracy on the train and test sets at the end of each training epoch and draw learning curves.\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=100, verbose=0)\n",
    "1\n",
    "2\n",
    "\t\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=100, verbose=0)\n",
    "\n",
    "Now that we have the basis of a problem and model, we can take a look evaluating three common loss functions that are appropriate for a multi-class classification predictive modeling problem.\n",
    "\n",
    "Although an MLP is used in these examples, the same loss functions can be used when training CNN and RNN models for multi-class classification.\n",
    "Multi-Class Cross-Entropy Loss\n",
    "\n",
    "Cross-entropy is the default loss function to use for multi-class classification problems.\n",
    "\n",
    "In this case, it is intended for use with multi-class classification where the target values are in the set {0, 1, 3, …, n}, where each class is assigned a unique integer value.\n",
    "\n",
    "Mathematically, it is the preferred loss function under the inference framework of maximum likelihood. It is the loss function to be evaluated first and only changed if you have a good reason.\n",
    "\n",
    "Cross-entropy will calculate a score that summarizes the average difference between the actual and predicted probability distributions for all classes in the problem. The score is minimized and a perfect cross-entropy value is 0.\n",
    "\n",
    "Cross-entropy can be specified as the loss function in Keras by specifying ‘categorical_crossentropy‘ when compiling the model.\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "1\n",
    "\t\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "The function requires that the output layer is configured with an n nodes (one for each class), in this case three nodes, and a ‘softmax‘ activation in order to predict the probability for each class.\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "1\n",
    "\t\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "In turn, this means that the target variable must be one hot encoded.\n",
    "\n",
    "This is to ensure that each example has an expected probability of 1.0 for the actual class value and an expected probability of 0.0 for all other class values. This can be achieved using the to_categorical() Keras function.\n",
    "# one hot encode output variable\n",
    "y = to_categorical(y)\n",
    "1\n",
    "2\n",
    "\t\n",
    "# one hot encode output variable\n",
    "y = to_categorical(y)\n",
    "\n",
    "The complete example of an MLP with cross-entropy loss for the multi-class blobs classification problem is listed below.\n",
    "# mlp for the blobs multi-class classification problem with cross-entropy loss\n",
    "from sklearn.datasets import make_blobs\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "# one hot encode output variable\n",
    "y = to_categorical(y)\n",
    "# split into train and test\n",
    "n_train = 500\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "# compile model\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=100, verbose=0)\n",
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "# plot loss during training\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Loss')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "# plot accuracy during training\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Accuracy')\n",
    "pyplot.plot(history.history['accuracy'], label='train')\n",
    "pyplot.plot(history.history['val_accuracy'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "5\n",
    "6\n",
    "7\n",
    "8\n",
    "9\n",
    "10\n",
    "11\n",
    "12\n",
    "13\n",
    "14\n",
    "15\n",
    "16\n",
    "17\n",
    "18\n",
    "19\n",
    "20\n",
    "21\n",
    "22\n",
    "23\n",
    "24\n",
    "25\n",
    "26\n",
    "27\n",
    "28\n",
    "29\n",
    "30\n",
    "31\n",
    "32\n",
    "33\n",
    "34\n",
    "35\n",
    "36\n",
    "37\n",
    "38\n",
    "39\n",
    "40\n",
    "41\n",
    "\t\n",
    "# mlp for the blobs multi-class classification problem with cross-entropy loss\n",
    "from sklearn.datasets import make_blobs\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "# one hot encode output variable\n",
    "y = to_categorical(y)\n",
    "# split into train and test\n",
    "n_train = 500\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "# compile model\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=100, verbose=0)\n",
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "# plot loss during training\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Loss')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "# plot accuracy during training\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Accuracy')\n",
    "pyplot.plot(history.history['accuracy'], label='train')\n",
    "pyplot.plot(history.history['val_accuracy'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "\n",
    "Running the example first prints the classification accuracy for the model on the train and test dataset.\n",
    "\n",
    "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
    "\n",
    "In this case, we can see the model performed well, achieving a classification accuracy of about 84% on the training dataset and about 82% on the test dataset.\n",
    "Train: 0.840, Test: 0.822\n",
    "1\n",
    "\t\n",
    "Train: 0.840, Test: 0.822\n",
    "\n",
    "A figure is also created showing two line plots, the top with the cross-entropy loss over epochs for the train (blue) and test (orange) dataset, and the bottom plot showing classification accuracy over epochs.\n",
    "\n",
    "In this case, the plot shows the model seems to have converged. The line plots for both cross-entropy and accuracy both show good convergence behavior, although somewhat bumpy. The model may be well configured given no sign of over or under fitting. The learning rate or batch size may be tuned to even out the smoothness of the convergence in this case.\n",
    "Line Plots of Cross Entropy Loss and Classification Accuracy over Training Epochs on the Blobs Multi-Class Classification Problem\n",
    "\n",
    "Line Plots of Cross Entropy Loss and Classification Accuracy over Training Epochs on the Blobs Multi-Class Classification Problem\n",
    "Sparse Multiclass Cross-Entropy Loss\n",
    "\n",
    "A possible cause of frustration when using cross-entropy with classification problems with a large number of labels is the one hot encoding process.\n",
    "\n",
    "For example, predicting words in a vocabulary may have tens or hundreds of thousands of categories, one for each label. This can mean that the target element of each training example may require a one hot encoded vector with tens or hundreds of thousands of zero values, requiring significant memory.\n",
    "\n",
    "Sparse cross-entropy addresses this by performing the same cross-entropy calculation of error, without requiring that the target variable be one hot encoded prior to training.\n",
    "\n",
    "Sparse cross-entropy can be used in keras for multi-class classification by using ‘sparse_categorical_crossentropy‘ when calling the compile() function.\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "1\n",
    "\t\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "The function requires that the output layer is configured with an n nodes (one for each class), in this case three nodes, and a ‘softmax‘ activation in order to predict the probability for each class.\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "1\n",
    "\t\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "No one hot encoding of the target variable is required, a benefit of this loss function.\n",
    "\n",
    "The complete example of training an MLP with sparse cross-entropy on the blobs multi-class classification problem is listed below.\n",
    "# mlp for the blobs multi-class classification problem with sparse cross-entropy loss\n",
    "from sklearn.datasets import make_blobs\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from matplotlib import pyplot\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "# split into train and test\n",
    "n_train = 500\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "# compile model\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=100, verbose=0)\n",
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "# plot loss during training\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Loss')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "# plot accuracy during training\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Accuracy')\n",
    "pyplot.plot(history.history['accuracy'], label='train')\n",
    "pyplot.plot(history.history['val_accuracy'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "5\n",
    "6\n",
    "7\n",
    "8\n",
    "9\n",
    "10\n",
    "11\n",
    "12\n",
    "13\n",
    "14\n",
    "15\n",
    "16\n",
    "17\n",
    "18\n",
    "19\n",
    "20\n",
    "21\n",
    "22\n",
    "23\n",
    "24\n",
    "25\n",
    "26\n",
    "27\n",
    "28\n",
    "29\n",
    "30\n",
    "31\n",
    "32\n",
    "33\n",
    "34\n",
    "35\n",
    "36\n",
    "37\n",
    "38\n",
    "\t\n",
    "# mlp for the blobs multi-class classification problem with sparse cross-entropy loss\n",
    "from sklearn.datasets import make_blobs\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from matplotlib import pyplot\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "# split into train and test\n",
    "n_train = 500\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "# compile model\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=100, verbose=0)\n",
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "# plot loss during training\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Loss')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "# plot accuracy during training\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Accuracy')\n",
    "pyplot.plot(history.history['accuracy'], label='train')\n",
    "pyplot.plot(history.history['val_accuracy'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "\n",
    "Running the example first prints the classification accuracy for the model on the train and test dataset.\n",
    "\n",
    "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
    "\n",
    "In this case, we can see the model achieves good performance on the problem. In fact, if you repeat the experiment many times, the average performance of sparse and non-sparse cross-entropy should be comparable.\n",
    "Train: 0.832, Test: 0.818\n",
    "1\n",
    "\t\n",
    "Train: 0.832, Test: 0.818\n",
    "\n",
    "A figure is also created showing two line plots, the top with the sparse cross-entropy loss over epochs for the train (blue) and test (orange) dataset, and the bottom plot showing classification accuracy over epochs.\n",
    "\n",
    "In this case, the plot shows good convergence of the model over training with regard to loss and classification accuracy.\n",
    "Line Plots of Sparse Cross Entropy Loss and Classification Accuracy over Training Epochs on the Blobs Multi-Class Classification Problem\n",
    "\n",
    "Line Plots of Sparse Cross Entropy Loss and Classification Accuracy over Training Epochs on the Blobs Multi-Class Classification Problem\n",
    "Kullback Leibler Divergence Loss\n",
    "\n",
    "Kullback Leibler Divergence, or KL Divergence for short, is a measure of how one probability distribution differs from a baseline distribution.\n",
    "\n",
    "A KL divergence loss of 0 suggests the distributions are identical. In practice, the behavior of KL Divergence is very similar to cross-entropy. It calculates how much information is lost (in terms of bits) if the predicted probability distribution is used to approximate the desired target probability distribution.\n",
    "\n",
    "As such, the KL divergence loss function is more commonly used when using models that learn to approximate a more complex function than simply multi-class classification, such as in the case of an autoencoder used for learning a dense feature representation under a model that must reconstruct the original input. In this case, KL divergence loss would be preferred. Nevertheless, it can be used for multi-class classification, in which case it is functionally equivalent to multi-class cross-entropy.\n",
    "\n",
    "KL divergence loss can be used in Keras by specifying ‘kullback_leibler_divergence‘ in the compile() function.\n",
    "model.compile(loss='kullback_leibler_divergence', optimizer=opt, metrics=['accuracy'])\n",
    "1\n",
    "\t\n",
    "model.compile(loss='kullback_leibler_divergence', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "As with cross-entropy, the output layer is configured with an n nodes (one for each class), in this case three nodes, and a ‘softmax‘ activation in order to predict the probability for each class.\n",
    "\n",
    "Also, as with categorical cross-entropy, we must one hot encode the target variable to have an expected probability of 1.0 for the class value and 0.0 for all other class values.\n",
    "# one hot encode output variable\n",
    "y = to_categorical(y)\n",
    "1\n",
    "2\n",
    "\t\n",
    "# one hot encode output variable\n",
    "y = to_categorical(y)\n",
    "\n",
    "The complete example of training an MLP with KL divergence loss for the blobs multi-class classification problem is listed below.\n",
    "# mlp for the blobs multi-class classification problem with kl divergence loss\n",
    "from sklearn.datasets import make_blobs\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "# one hot encode output variable\n",
    "y = to_categorical(y)\n",
    "# split into train and test\n",
    "n_train = 500\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "# compile model\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='kullback_leibler_divergence', optimizer=opt, metrics=['accuracy'])\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=100, verbose=0)\n",
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "# plot loss during training\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Loss')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "# plot accuracy during training\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Accuracy')\n",
    "pyplot.plot(history.history['accuracy'], label='train')\n",
    "pyplot.plot(history.history['val_accuracy'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "5\n",
    "6\n",
    "7\n",
    "8\n",
    "9\n",
    "10\n",
    "11\n",
    "12\n",
    "13\n",
    "14\n",
    "15\n",
    "16\n",
    "17\n",
    "18\n",
    "19\n",
    "20\n",
    "21\n",
    "22\n",
    "23\n",
    "24\n",
    "25\n",
    "26\n",
    "27\n",
    "28\n",
    "29\n",
    "30\n",
    "31\n",
    "32\n",
    "33\n",
    "34\n",
    "35\n",
    "36\n",
    "37\n",
    "38\n",
    "39\n",
    "40\n",
    "41\n",
    "\t\n",
    "# mlp for the blobs multi-class classification problem with kl divergence loss\n",
    "from sklearn.datasets import make_blobs\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "# one hot encode output variable\n",
    "y = to_categorical(y)\n",
    "# split into train and test\n",
    "n_train = 500\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "# compile model\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='kullback_leibler_divergence', optimizer=opt, metrics=['accuracy'])\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=100, verbose=0)\n",
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "# plot loss during training\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Loss')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "# plot accuracy during training\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Accuracy')\n",
    "pyplot.plot(history.history['accuracy'], label='train')\n",
    "pyplot.plot(history.history['val_accuracy'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "\n",
    "Running the example first prints the classification accuracy for the model on the train and test dataset.\n",
    "\n",
    "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
    "\n",
    "In this case, we see performance that is similar to those results seen with cross-entropy loss, in this case about 82% accuracy on the train and test dataset.\n",
    "Train: 0.822, Test: 0.822\n",
    "1\n",
    "\t\n",
    "Train: 0.822, Test: 0.822\n",
    "\n",
    "A figure is also created showing two line plots, the top with the KL divergence loss over epochs for the train (blue) and test (orange) dataset, and the bottom plot showing classification accuracy over epochs.\n",
    "\n",
    "In this case, the plot shows good convergence behavior for both loss and classification accuracy. It is very likely that an evaluation of cross-entropy would result in nearly identical behavior given the similarities in the measure.\n",
    "Line Plots of KL Divergence Loss and Classification Accuracy over Training Epochs on the Blobs Multi-Class Classification Problem"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
