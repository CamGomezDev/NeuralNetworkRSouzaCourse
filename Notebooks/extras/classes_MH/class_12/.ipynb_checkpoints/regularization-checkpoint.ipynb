{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "convenient-prairie",
   "metadata": {},
   "source": [
    "## Modelado de datos con Redes Neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-limit",
   "metadata": {},
   "source": [
    "## Overfitting and Regularization \n",
    "\n",
    "* (there is lot of examples of data to traing a Machine Learning but there are still few good sites that explain ***intuitively*** the complexity behind ML elements)\n",
    "\n",
    "* remember to keep yourself question: ***\"why?\"*** \n",
    "\n",
    "* proper visualizations in Python are under construction\n",
    "\n",
    "* last classes before NN (next week we start Tensorflow - rather  Thursday than Monday)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-culture",
   "metadata": {},
   "source": [
    "Simple case linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-reminder",
   "metadata": {},
   "source": [
    "Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-forge",
   "metadata": {},
   "source": [
    "$$J(\\theta_0,\\theta_1,\\theta_2,,\\theta_3...) = \\frac{1}{m} \\sum_{i=1}^{m}[ h_{\\theta}(x_i) - y_i]^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-hollow",
   "metadata": {},
   "source": [
    "Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-england",
   "metadata": {},
   "source": [
    "$$\\theta_0:=\\theta_0 - \\alpha \\frac{d}{d\\theta_0}J(\\theta_0,\\theta_1)$$\n",
    "$$\\theta_1:=\\theta_1 - \\alpha \\frac{d}{d\\theta_1}J(\\theta_0,\\theta_1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-china",
   "metadata": {},
   "source": [
    "$$\\theta_0:=\\theta_0 - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}[ h_{\\theta}(x^i) - y^i]$$\n",
    "$$\\theta_1:=\\theta_1 - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}([ h_{\\theta}(x^i) - y^i]*x^i)$$\n",
    "\n",
    "for all the $\\theta$s in general:\n",
    "\n",
    "$$\\theta_j:=\\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}([ h_{\\theta}(x^i) - y^i]*x^i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-pledge",
   "metadata": {},
   "source": [
    "$h_{\\theta}(x)=\\theta^T X$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-german",
   "metadata": {},
   "source": [
    "$h_{\\theta}(x):$\n",
    "\n",
    "* simple linear regression: \n",
    "\n",
    "$$h_{\\theta}(x)=\\theta_0+\\theta_1x_1$$\n",
    "\n",
    "\n",
    "* multiple linear regression: \n",
    "\n",
    "$$h_{\\theta}(x)=\\theta_0+\\theta_1x_1 +\\theta_2 x_2 +\\theta_3x_3 +\\theta_4x_4 $$\n",
    "\n",
    "\n",
    "* polynomial linear regression (\"linear\"): \n",
    "\n",
    "$$h_{\\theta}(x)=\\theta_0+\\theta_1x_1 +\\theta_2 x_2^2 +\\theta_3x_3^3 +\\theta_4x_4^4 + \\cdots \\theta_nx_n^n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-attribute",
   "metadata": {},
   "source": [
    "to some example data (below) we can fit anything (any polynomial, we just need sufficient number of points)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-subsection",
   "metadata": {},
   "source": [
    "<img src=\"imgs/fitting_polynomials.png\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-train",
   "metadata": {},
   "source": [
    "Reminder from Monday"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-celebration",
   "metadata": {},
   "source": [
    "#### a) underfit\n",
    "* if we fit the data with a polynomial that is too simple for the real behaviour\n",
    "\n",
    "$$h_{\\theta}(x)=\\theta_0+\\theta_1x_1$$\n",
    "(straight line)\n",
    "\n",
    "<img src=\"imgs/underfit.png\" width=\"200\" />\n",
    "\n",
    "* this is called underfit\n",
    "\n",
    "***high bias ***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "least-reliance",
   "metadata": {},
   "source": [
    "#### b) just right\n",
    "* if we fit the data with a right polynomial \n",
    "$$h_{\\theta}(x)=\\theta_0+\\theta_1x_1 +\\theta_2x_2^2 $$\n",
    "\n",
    "<img src=\"imgs/just_right.png\" width=\"200\" />\n",
    "\n",
    "***just right ***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-buying",
   "metadata": {},
   "source": [
    "#### c) overfit\n",
    "* * if we fit the data with a polynomial too complex for the real behaviour\n",
    "\n",
    "$$h_{\\theta}(x)=\\theta_0+\\theta_1x_1 +\\theta_2 x_2^2 +\\theta_3x_3^3 +\\theta_4x_4^4 $$\n",
    "\n",
    "<img src=\"imgs/overfit.png\" width=\"200\" />\n",
    "* overfitting\n",
    "\n",
    "\n",
    "***high variance***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continental-november",
   "metadata": {},
   "source": [
    "* overfitting is much more common than underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-conviction",
   "metadata": {},
   "source": [
    "### How to understand bias and variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-catering",
   "metadata": {},
   "source": [
    "<img src=\"imgs/bulls_eye.png\" width=\"300\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-injury",
   "metadata": {},
   "source": [
    "1. (Top Left figure) Very Accurate Model — therefore the error of our model will be low, meaning a low bias and low variance as shown in the first figure.\n",
    "\n",
    "\n",
    "2. (Top Right figure) As variance increases, the spread of our data point increases which result in less accurate prediction.\n",
    "\n",
    "\n",
    "3. (Bottom Left figure) As Bias increases the error between our predicted value and the observed values increases. A high bias assumes a strong assumption or strong restrictions on the model.\n",
    "\n",
    "4. Not happening"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "administrative-chapel",
   "metadata": {},
   "source": [
    "#### In other words\n",
    "\n",
    "1. (Top Left figure) Just right\n",
    "\n",
    "\n",
    "2. (Top Right figure) Every new point that we will try to evaluate using our model (hypothesis with assigned $\\theta$s) will be bad but differently bad (large spread)\n",
    "\n",
    "3. (Bottom Left figure) Every new point that we will try to evaluate using our model (hypothesis with assigned $\\theta$s) will be bad but equally bad (small spread)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-square",
   "metadata": {},
   "source": [
    "### How to deal with overfitting\n",
    "\n",
    "1. Reduce number of features (eg in the case of the CO2 emission of the car or price houses)\n",
    "* Manually select which features to keep\n",
    "* Model selection algorithm (later).\n",
    "  * PCA - Principal Component Analysis\n",
    "  * Correlation Matrix\n",
    "  * Autoencoders\n",
    "\n",
    "2. Regularization.\n",
    "* Keep all the features, but reduce magnitude/values of parameters ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-static",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-court",
   "metadata": {},
   "source": [
    "$$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}[ h_{\\theta}(x_i) - y_i]^2 + \\lambda \\sum_{k=1}^{p} \\theta_k^2$$\n",
    "\n",
    "$$h_{\\theta}(x)=\\theta_0+\\theta_1x_1 +\\theta_2 x_2^2 +\\theta_3x_3^3 +\\theta_4x_4^4 $$\n",
    "where:\n",
    "\n",
    "$\\lambda$ is a regularization weight - a tuning parameter that decides how much we want to penalize the flexibility of our model. It can be tuned using cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turkish-spoke",
   "metadata": {},
   "source": [
    "# why?\n",
    "Lets imagine we want to fit a simple model using complex polynomial ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-athletics",
   "metadata": {},
   "source": [
    "$$h_{\\theta}(x)=\\theta_0+\\theta_1x_1 +\\theta_2 x_1^2 +\\theta_3x_1^3 +\\theta_4x_1^4$$\n",
    "\n",
    "\n",
    "and we dont want to overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abroad-flash",
   "metadata": {},
   "source": [
    "<img src=\"imgs/fitting_polynomials.png\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-covering",
   "metadata": {},
   "source": [
    "and lets imagine that the gradient descent minimized the cost function and found the coefficients $\\theta$s to be: \n",
    "\n",
    "$$h_{\\theta}(x)=1+10x_1 +100 x_2^2 +1000x_3^3 +10000x_4^4$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-frost",
   "metadata": {},
   "source": [
    "to prevent from overfitting we could make all the coefficients smaller. why? \n",
    "\n",
    "#### Lets look at a more particular case : four points that we want to fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "democratic-conservation",
   "metadata": {},
   "source": [
    "<img src=\"imgs/green_blue_curve_overfit.png\" width=\"400\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boxed-assurance",
   "metadata": {},
   "source": [
    "Look at this very real fact! \n",
    "\n",
    "* If we have those four points and we fit it with this far too complex polynomial:\n",
    "\n",
    "$$h_{\\theta}(x)=30 - 31x -5x^2 +7x^3 -x^4$$\n",
    "  we get a green line.\n",
    "\n",
    "\n",
    "\n",
    "* If we now divide all the coefficients by 5:\n",
    "\n",
    "$$h_{\\theta}(x)=\\frac{30}{5} - \\frac{31}{5}x -\\frac{5}{5}x^2 +\\frac{7}{5}x^3 -\\frac{x}{5}^4$$\n",
    "\n",
    "  we get a blue line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-airport",
   "metadata": {},
   "source": [
    "# why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-theory",
   "metadata": {},
   "source": [
    "Remember what does the gradient descent do:\n",
    "\n",
    "Gradient descent is always trying to minimize the cost function by finding OPTIMAL values of $\\theta$\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}[ (\\theta_0+\\theta_1x_1 +\\theta_2 x_1^2...) - y_i]^2 $$\n",
    "\n",
    "\n",
    "If we add some extra term to the cost function\n",
    "\n",
    "$$J(\\theta) = \\cdots + \\lambda \\sum_{k=1}^{p} \\theta_k^2$$\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}[ h_{\\theta}(x_i) - y_i]^2 + \\lambda \\sum_{k=1}^{p} \\theta_k^2$$\n",
    "\n",
    "we make the cost function increase!\n",
    "\n",
    "So, in order to compensate the increase of the $J(\\theta)$ gradient descent will have to decrease the values of $\\theta$ by choosing smaller $\\theta$s in order to minimize the total cost. As we have seen above making the $\\theta$s smaller reduces the overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-model",
   "metadata": {},
   "source": [
    "what is the purpose of $\\lambda$?\n",
    "\n",
    "$\\lambda$ is a parameter that we choose to decide how much we want to decrease the values of $\\theta$s or speaking the language of machine learning: to penalize the flexibility of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-niagara",
   "metadata": {},
   "source": [
    "# Lets see the effect of $\\lambda$ on the fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subsequent-bones",
   "metadata": {},
   "source": [
    "<img src=\"imgs/alpha.png\" width=\"1000\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-custody",
   "metadata": {},
   "source": [
    "1. $\\lambda$ = 0 - The objective becomes same as simple linear regression. We’ll get the same coefficients as simple linear regression.\n",
    "\n",
    "2. $\\lambda$ = $\\infty$: The coefficients will be zero. Why? Because of infinite weight on the square of coefficients.\n",
    "\n",
    "3. 0 < $\\lambda$ < $\\infty$: The magnitude of $\\lambda$ will decide the weight given to different parts of the objective. The coefficients will be somewhere between 0 and ones for simple linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-center",
   "metadata": {},
   "source": [
    "### but the conclusion $\\lambda$ helps to fight the overfitting but be careful because if $\\lambda$ is too large it can make to model to underfit!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-finder",
   "metadata": {},
   "source": [
    "# Different regularization terms:\n",
    "\n",
    "There are two kinds of regularization:\n",
    "\n",
    "* L1 Regularization called Lasso\n",
    "* L2 Regularization called Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceramic-detective",
   "metadata": {},
   "source": [
    "#### L2 Regularization called Ridge\n",
    "\n",
    "is a “squared magnitude” of coefficient as penalty term to the loss function. Here the highlighted part represents L2 regularization element. Is exactly this particular term that we added to the cost function \n",
    "\n",
    "$$J(\\theta) = \\cdots + \\lambda \\sum_{k=1}^{p} \\theta_k^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-knowing",
   "metadata": {},
   "source": [
    "#### L1 Regularization called LASSO\n",
    "\n",
    "LASSO full form is \"Least Absolute Shrinkage Selector Operator\". It is quite similar to Ridge regression. LASSO adds “absolute value of magnitude” of coefficient as penalty term to the loss function. This way:\n",
    "\n",
    "$$J(\\theta) = \\cdots + \\lambda \\sum_{k=1}^{p} |\\theta_k|$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-beverage",
   "metadata": {},
   "source": [
    "# Difference between L1 and L2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-beijing",
   "metadata": {},
   "source": [
    "Two of the very powerful techniques that use the concept of L1 and L2 regularization are Lasso regression and Ridge regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-encyclopedia",
   "metadata": {},
   "source": [
    "To understand how L1 helps in feature selection, you should consider it in comparison with L2.\n",
    "\n",
    "L1’s penalty: $\\sum |\\theta_j|$\n",
    "\n",
    "L2’s penalty:  $\\sum (\\theta_j)^2$\n",
    "\n",
    "Observation: L1 penalizes weights equally regardless of the magnitude of those weights. L2 penalizes bigger weights more than smaller weights.\n",
    "\n",
    "For example, suppose $\\theta_3$=100\n",
    "and $\\theta_4$=10\n",
    "\n",
    ".\n",
    "\n",
    "* By reducing $\\theta_3$ by 1, L1’s penalty is reduced by 1. By reducing $\\theta_4$ by 1, L1’s penalty is also reduced by 1.\n",
    "\n",
    "\n",
    "* By reducing $\\theta_3$\n",
    "by 1, L2’s penalty is reduced by 199. By reducing $\\theta_4$ by 1, L2’s penalty is reduced by only 19. Thus, L2 tends to prefer reducing $\\theta_3$ over $\\theta_4$\n",
    "\n",
    "    .\n",
    "\n",
    "In general, when a weight $\\theta_j$\n",
    "\n",
    "has already been small in magnitude, L2 does not care to reduce it to zero, L2 would rather reduce big weights than eliminate small weights to 0. The result is that the weights are reduced, but almost never reduced to 0, i.e. almost never be completely eliminated, meaning no feature selection.\n",
    "\n",
    "\n",
    "On the other hand, L1 cares about reducing big weights and small weights equally. For L1, the less informative features get reduced. Some features may get completely eliminated by L1, thus we have feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-force",
   "metadata": {},
   "source": [
    "### In other words\n",
    "\n",
    "L1 will shrink some coefficients to exactly zero (practically excluding them from the model), making it behave as a variable selection method.\n",
    "\n",
    "In contrast, because L2 minimizes the sum of the squares of the coefficients, it will affect larger ones much more than it will shrink smaller ones, so coefficients close to zero will barely be shrunk further. Therefore, with L2 regularization, we end up with a model that has a lot of coefficients close to, but not exactly zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legislative-madrid",
   "metadata": {},
   "source": [
    "### So is L1 better than L2 regularization?\n",
    "\n",
    "Not necessarily.\n",
    "\n",
    "LASSO (L1 regularization) is better when we want to select variables from a larger subset, for instance for exploratory analysis or when we want a simple interpretable model. It will also perform better (have a higher prediction accuracy) than ridge regression in situations where a small number of independent variables are good predictors of the outcome and the rest are not that important.\n",
    "\n",
    "Ridge regression (L2 regularization) performs better than LASSO when we have a large number of variables (or even all of them) each contributing a little bit in predicting the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-arlington",
   "metadata": {},
   "source": [
    "# When to use which ?\n",
    "\n",
    "### Use Ridge L2\n",
    "***The basic principle of Ridge is to remove high variance***.\n",
    "\n",
    "    * When you need to reduce variance\n",
    "\n",
    "\n",
    "\n",
    "### Use Lasso L1\n",
    "***The basic principle of Ridge is to remove low contributing features.***\n",
    "\n",
    "    * When you need to reduce dimensions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "median-austria",
   "metadata": {},
   "source": [
    "# Bottom line...\n",
    "## If you are in doubt use L2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-relationship",
   "metadata": {},
   "source": [
    "### Regularization in TensorFlow:\n",
    "* as simple as this:\n",
    "\n",
    "from tensorflow.keras import regularizers    \n",
    "\n",
    "`model = Sequential([Dense(512, kernel_regularizer = regularizers.l2(wd)...`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judicial-colony",
   "metadata": {},
   "source": [
    "# Homework 5 (logistic regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-visitor",
   "metadata": {},
   "source": [
    "Use scikit-learn to perform digit classification using \n",
    "data from Homework 2 (mnist_test.csv)\n",
    "\n",
    "*we will compare it with Neural Network later"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
